{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nutzer\\AppData\\Local\\Temp\\ipykernel_29140\\1828077501.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my', 'does', 'no', 'me', 'be', 'being', \"shan't\", 've', 'against', 'doing', 'having', 'his', 'ourselves', \"you'll\", 'only', 'been', 'him', 'such', 'which', 'nor', 'than', 'ain', 'it', \"it's\", 'just', 'is', \"should've\", 'haven', 'now', \"needn't\", \"mightn't\", 'its', 'between', \"hasn't\", 'where', 'theirs', 'can', 'should', \"weren't\", 'm', 'and', 'what', 'same', 'don', 'himself', \"wasn't\", 're', 'again', 'as', 'more', 'aren', \"won't\", 'that', 'when', \"you're\", 'on', 'y', 'then', 'above', 'to', \"shouldn't\", 'very', 'itself', 'with', 'she', 'an', \"hadn't\", 'this', \"mustn't\", 'they', 'each', 'not', 'from', 'ours', \"that'll\", 'out', 'some', 'these', 'too', \"isn't\", 'do', 'few', 'down', \"didn't\", 'those', 'into', \"you'd\", 'under', 'had', 'for', 'at', 'below', 'if', 'i', 'hers', 'were', 'but', 'by', \"aren't\", 'how', 'why', 'yours', 'both', 'their', 'he', 'll', 'd', 'once', 'or', 'other', 'weren', 'myself', 'over', 'will', 'the', 'in', 'a', 'any', 'hasn', 'our', 'about', 'didn', 'doesn', 'we', 's', 'until', 'of', \"don't\", 'here', 'did', \"haven't\", \"couldn't\", 'while', 'o', 'own', \"doesn't\", 'wouldn', 'through', 'are', 'her', 't', 'mustn', 'yourself', 'shouldn', 'won', 'after', 'further', 'have', 'herself', 'was', 'most', 'whom', \"you've\", 'who', 'you', 'mightn', 'all', 'couldn', 'before', 'ma', 'your', 'shan', 'because', 'there', \"wouldn't\", 'has', 'themselves', 'yourselves', 'isn', 'am', 'wasn', 'off', 'so', \"she's\", 'hadn', 'them', 'up', 'during', 'needn'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nutzer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nutzer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "fdist = FreqDist(english_stopwords)\n",
    "top_stopwords = set(word.lower() for word, _ in fdist.most_common(1000)) \n",
    "print(top_stopwords)\n",
    "\n",
    "test_text = '''\n",
    "Do you have a problem that you are struggling to solve? Why don't you ask your friends for advice? When people ask for advice on solving a problem, often times they speak to more than one person. This is because different views are better for figuring out a tough problem, many opinions are better than one, and other people may have experienced a problem like yours and may be able to help you in making better decisions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Text Image Recognition\n",
    "\n",
    "Is there some similarity to Malware detection, where it's common to translate malware code to bytes and then images, which are then fed into NN/DL networks? Let's test a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3992\n",
      "3998\n"
     ]
    }
   ],
   "source": [
    "ai = json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\4k\\\\generator_1_daryl149_llama-2-7b-chat-hf_4000_outputs.json\", encoding='utf8'))\n",
    "#ai = ai + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\4k\\\\generator_1_gpt2_4000_outputs.json\", encoding='utf8'))\n",
    "#ai = ai + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_1_gpt2_100_outputs.json\", encoding='utf8'))\n",
    "\n",
    "human = json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\4k\\\\generator_0_daryl149_llama-2-7b-chat-hf_4000_outputs.json\", encoding='utf8'))\n",
    "#human = human + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\4k\\\\generator_0_gpt2_4000_outputs.json\", encoding='utf8'))\n",
    "#human = human + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_0_gpt2_100_outputs.json\", encoding='utf8'))\n",
    "print(len(ai))\n",
    "print(len(human))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis as Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_image(detector_outputs):\n",
    "    for output in detector_outputs:\n",
    "        meta = output['metadata']\n",
    "        height = meta['sample_rate']\n",
    "        width = meta['sample_sequence_length']\n",
    "        results = output['ensemble_results'][0]['sample_results']\n",
    "        \n",
    "        image = Image.new('RGBA', (100, 20), color=(0,0,0,255))\n",
    "        pixels = image.load()\n",
    "\n",
    "        x = 1\n",
    "        offset_x = x\n",
    "        y = 1\n",
    "\n",
    "        for result in results:\n",
    "            steps = result['model_outputs']['steps']\n",
    "            steps.sort(key= lambda x: x['step'])\n",
    "\n",
    "            for step in steps:                \n",
    "                r = 255\n",
    "                g = 255\n",
    "                b = 255\n",
    "                \n",
    "                # Let's try to ignore the stopwords as they are probably useless\n",
    "                vis = int(step['target_prob'] * 255)\n",
    "                if step['target'].lower() in english_stopwords:\n",
    "                    vis = 0\n",
    "\n",
    "                all_probs = step['top_k_probs']\n",
    "                all_probs.sort(reverse=True)\n",
    "                other_probs = all_probs[1:]\n",
    "                \n",
    "                # mid\n",
    "                pixels[x, y] = (r, g, b, vis)\n",
    "                # right\n",
    "                pixels[x+1, y] = (r, g, b, int(other_probs[0] * 255))\n",
    "                # left\n",
    "                pixels[x-1, y] = (r, g, b, int(other_probs[1] * 255))\n",
    "                # down\n",
    "                pixels[x, y-1] = (r, g, b, int(other_probs[2] * 255))\n",
    "                # up\n",
    "                pixels[x, y+1] = (r, g, b, int(other_probs[3] * 255))\n",
    "                \n",
    "                x += 3\n",
    "            \n",
    "            offset_x = int((offset_x % 2 == 1))\n",
    "            x = offset_x\n",
    "            y += 3\n",
    "        display(image)\n",
    "\n",
    "def draw_image(width, height):\n",
    "    image = Image.new('RGBA', (width, height))\n",
    "    pixels = image.load()\n",
    "    # iterate over all pixels\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            r = random.randint(0, 255)\n",
    "            g = random.randint(0, 255)\n",
    "            b = random.randint(0, 255)\n",
    "            a = random.randint(0, 1)\n",
    "            pixels[x, y] = (r, g, b, int(a * 255))  # Set the pixel RGBA values\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_to_image(human[:2])\n",
    "output_to_image(ai[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Let's try some CNN detection on the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D CNN\n",
    "\n",
    "Let's start by creating 2d arrays with x channels per \"pixel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(detector_outputs, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for output in detector_outputs:\n",
    "        results = output['ensemble_results'][0]['sample_results']\n",
    "        rows = []\n",
    "\n",
    "        for result in results:\n",
    "            steps = result['model_outputs']['steps']\n",
    "            steps.sort(key= lambda x: x['step'])\n",
    "            columns = []\n",
    "            for step in steps:\n",
    "                prob = step['target_prob']\n",
    "                # Common target stepwords, we mask out\n",
    "                #if step['target'].lower() in english_stopwords:\n",
    "                #    prob = 0\n",
    "                channels = [prob]\n",
    "                # Again, mask out common stopwords\n",
    "                top_k_tokens = step['top_k_tokens']\n",
    "                top_k_probs = step['top_k_probs']\n",
    "                for i in range(0 , len(top_k_tokens)):\n",
    "                    token = top_k_tokens[i]\n",
    "                    #if token.lower() in english_stopwords:\n",
    "                    #    top_k_probs[i] = 0\n",
    "                top_k_probs.sort()\n",
    "                channels = channels + top_k_probs\n",
    "                columns.append(channels)\n",
    "                \n",
    "            rows.append(columns)\n",
    "          \n",
    "        data.append(rows)\n",
    "        labels.append(label)  \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7990\n",
      "torch.Size([7990, 11, 6, 32])\n",
      "tensor([[[[1.4948e-02, 9.9464e-01, 3.6222e-01,  ..., 8.7772e-01,\n",
      "           1.6577e-01, 9.9810e-01],\n",
      "          [3.7504e-01, 9.9988e-01, 3.6402e-01,  ..., 9.7442e-01,\n",
      "           4.6888e-01, 1.7561e-01],\n",
      "          [7.9430e-01, 8.9913e-01, 9.2838e-01,  ..., 8.2640e-01,\n",
      "           5.4406e-02, 5.1559e-01],\n",
      "          [5.3736e-02, 1.0000e+00, 1.0000e+00,  ..., 1.3157e-01,\n",
      "           9.9854e-01, 3.3312e-01],\n",
      "          [8.9472e-01, 3.1400e-01, 8.0724e-03,  ..., 9.3399e-01,\n",
      "           1.1258e-04, 9.9999e-01],\n",
      "          [1.2509e-01, 9.9996e-01, 1.8191e-01,  ..., 5.0755e-01,\n",
      "           2.1886e-04, 8.9712e-01]],\n",
      "\n",
      "         [[4.4322e-04, 3.6408e-09, 5.6110e-04,  ..., 9.8575e-06,\n",
      "           8.6356e-04, 1.8386e-07],\n",
      "          [3.0243e-05, 8.5786e-09, 6.1478e-04,  ..., 7.8075e-07,\n",
      "           1.0294e-04, 1.4030e-04],\n",
      "          [3.6288e-05, 1.1921e-04, 1.4194e-06,  ..., 1.2344e-06,\n",
      "           5.3669e-05, 8.2722e-05],\n",
      "          [9.8556e-04, 3.8597e-11, 9.1448e-09,  ..., 1.8885e-03,\n",
      "           7.8861e-07, 3.3022e-05],\n",
      "          [3.4786e-09, 7.2326e-05, 4.1510e-07,  ..., 8.7365e-07,\n",
      "           3.6531e-04, 1.3180e-08],\n",
      "          [1.4686e-05, 6.4524e-09, 1.6709e-03,  ..., 1.0796e-04,\n",
      "           6.1528e-06, 1.3005e-05]],\n",
      "\n",
      "         [[1.2711e-03, 1.7276e-06, 1.5118e-03,  ..., 3.3387e-05,\n",
      "           1.0602e-03, 2.8059e-07],\n",
      "          [2.2301e-04, 7.3816e-07, 5.0125e-03,  ..., 4.3898e-05,\n",
      "           1.4280e-03, 7.3310e-04],\n",
      "          [1.2171e-04, 2.4706e-04, 3.9932e-06,  ..., 1.7477e-06,\n",
      "           4.5547e-04, 2.1003e-04],\n",
      "          [1.9253e-03, 2.9091e-10, 2.0017e-08,  ..., 1.8677e-02,\n",
      "           3.9467e-06, 7.4044e-05],\n",
      "          [9.3571e-07, 2.1592e-04, 2.5461e-05,  ..., 4.0841e-05,\n",
      "           4.5403e-04, 2.8191e-08],\n",
      "          [3.7256e-05, 8.2770e-08, 1.1328e-02,  ..., 5.9526e-04,\n",
      "           3.3813e-04, 7.4238e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.6603e-02, 8.4117e-05, 9.5188e-02,  ..., 2.1709e-02,\n",
      "           1.2446e-01, 1.7627e-04],\n",
      "          [1.1890e-02, 4.1640e-06, 2.0370e-01,  ..., 7.4566e-04,\n",
      "           8.1383e-02, 4.4334e-02],\n",
      "          [2.5907e-02, 3.3454e-02, 1.5002e-04,  ..., 2.5886e-03,\n",
      "           5.4406e-02, 9.0924e-03],\n",
      "          [1.0978e-01, 3.0247e-07, 4.1782e-07,  ..., 1.3157e-01,\n",
      "           3.3171e-04, 1.0526e-01],\n",
      "          [1.2599e-02, 2.3713e-02, 1.3400e-01,  ..., 1.4868e-02,\n",
      "           8.3586e-02, 9.4828e-07],\n",
      "          [1.2509e-01, 7.0458e-06, 1.2373e-01,  ..., 4.1020e-02,\n",
      "           9.3654e-02, 2.4928e-02]],\n",
      "\n",
      "         [[4.7149e-02, 5.0042e-03, 3.5306e-01,  ..., 2.4267e-02,\n",
      "           1.6577e-01, 9.5363e-04],\n",
      "          [3.7504e-01, 1.0091e-04, 2.6158e-01,  ..., 2.2086e-02,\n",
      "           1.3078e-01, 1.7561e-01],\n",
      "          [1.6875e-01, 5.4718e-02, 7.1137e-02,  ..., 1.7043e-01,\n",
      "           8.3840e-02, 4.6430e-01],\n",
      "          [1.3484e-01, 2.5234e-06, 8.6524e-07,  ..., 1.4949e-01,\n",
      "           9.0790e-04, 3.3312e-01],\n",
      "          [8.5607e-02, 3.1400e-01, 1.7870e-01,  ..., 3.2052e-02,\n",
      "           1.1298e-01, 2.6535e-06],\n",
      "          [2.9265e-01, 8.2000e-06, 1.8191e-01,  ..., 3.6570e-01,\n",
      "           3.3773e-01, 3.2066e-02]],\n",
      "\n",
      "         [[7.9258e-01, 9.9464e-01, 3.6222e-01,  ..., 8.7772e-01,\n",
      "           6.2084e-01, 9.9810e-01],\n",
      "          [5.9479e-01, 9.9988e-01, 3.6402e-01,  ..., 9.7442e-01,\n",
      "           4.6888e-01, 7.2779e-01],\n",
      "          [7.9430e-01, 8.9913e-01, 9.2838e-01,  ..., 8.2640e-01,\n",
      "           7.3822e-01, 5.1559e-01],\n",
      "          [5.6317e-01, 1.0000e+00, 1.0000e+00,  ..., 2.2018e-01,\n",
      "           9.9854e-01, 5.2323e-01],\n",
      "          [8.9472e-01, 6.2406e-01, 6.7311e-01,  ..., 9.3399e-01,\n",
      "           7.8377e-01, 9.9999e-01],\n",
      "          [5.4360e-01, 9.9996e-01, 3.2415e-01,  ..., 5.0755e-01,\n",
      "           4.1783e-01, 8.9712e-01]]]])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "data_ai, labels_ai = create_dataset(ai, 1)\n",
    "data_hu, labels_hu = create_dataset(human, 0)\n",
    "data = np.concatenate((data_ai, data_hu), axis=0)\n",
    "labels = np.concatenate((labels_hu, labels_ai))\n",
    "print(len(labels))\n",
    "\n",
    "# Convert numpy arrays to tensors\n",
    "# We need: [batch_size, channels, height, width]\n",
    "# data_tensor = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1).unsqueeze(-2)\n",
    "data_tensor = torch.tensor(data, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "print(data_tensor.shape)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "print(data_tensor[:1])\n",
    "print(labels_tensor[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape torch.Size([6392, 11, 6, 32])\n",
      "x_test shape torch.Size([1598, 11, 6, 32])\n",
      "y_train shape torch.Size([6392])\n",
      "y_test shape torch.Size([1598])\n",
      "tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
    "print(\"x_train shape\",X_train.shape)\n",
    "print(\"x_test shape\",X_test.shape)\n",
    "print(\"y_train shape\",y_train.shape)\n",
    "print(\"y_test shape\",y_test.shape)\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(11, 32, kernel_size=(2, 8), stride=(1, 1), padding=(0, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Dropout(p=0.25, inplace=False)\n",
      "  (4): Conv2d(32, 64, kernel_size=(2, 6), stride=(1, 1), padding=(0, 1))\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Dropout(p=0.25, inplace=False)\n",
      "  (8): Flatten(start_dim=1, end_dim=-1)\n",
      "  (9): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.4, inplace=False)\n",
      "  (12): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (13): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=11, out_channels=32, kernel_size=(2,8), padding=(0, 1)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(1,2), stride=(1, 2)),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2,6), padding=(0,1)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(1,2), stride=(1,2)),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=20, out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.4),\n",
    "\n",
    "    nn.Linear(in_features=64, out_features=1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.12359, Training Accuracy: 0.95733\n",
      "Epoch 1, Test Loss: 0.13873, Test Accuracy: 0.95152\n",
      "\n",
      "Epoch 2, Training Loss: 0.12239, Training Accuracy: 0.95643\n",
      "Epoch 2, Test Loss: 0.15014, Test Accuracy: 0.94673\n",
      "\n",
      "Epoch 3, Training Loss: 0.11389, Training Accuracy: 0.96046\n",
      "Epoch 3, Test Loss: 0.16598, Test Accuracy: 0.94277\n",
      "\n",
      "Epoch 4, Training Loss: 0.12029, Training Accuracy: 0.95771\n",
      "Epoch 4, Test Loss: 0.14173, Test Accuracy: 0.94749\n",
      "\n",
      "Epoch 5, Training Loss: 0.12096, Training Accuracy: 0.95743\n",
      "Epoch 5, Test Loss: 0.16195, Test Accuracy: 0.93839\n",
      "\n",
      "Epoch 6, Training Loss: 0.12100, Training Accuracy: 0.95752\n",
      "Epoch 6, Test Loss: 0.17828, Test Accuracy: 0.94070\n",
      "\n",
      "Epoch 7, Training Loss: 0.11745, Training Accuracy: 0.95869\n",
      "Epoch 7, Test Loss: 0.15414, Test Accuracy: 0.94559\n",
      "\n",
      "Epoch 8, Training Loss: 0.12172, Training Accuracy: 0.95782\n",
      "Epoch 8, Test Loss: 0.15771, Test Accuracy: 0.94469\n",
      "\n",
      "Epoch 9, Training Loss: 0.11504, Training Accuracy: 0.95927\n",
      "Epoch 9, Test Loss: 0.17093, Test Accuracy: 0.94573\n",
      "\n",
      "Epoch 10, Training Loss: 0.11514, Training Accuracy: 0.95974\n",
      "Epoch 10, Test Loss: 0.15233, Test Accuracy: 0.94467\n",
      "\n",
      "Epoch 11, Training Loss: 0.11576, Training Accuracy: 0.95988\n",
      "Epoch 11, Test Loss: 0.15246, Test Accuracy: 0.94288\n",
      "\n",
      "Epoch 12, Training Loss: 0.11541, Training Accuracy: 0.96022\n",
      "Epoch 12, Test Loss: 0.16189, Test Accuracy: 0.93662\n",
      "\n",
      "Epoch 13, Training Loss: 0.11759, Training Accuracy: 0.95941\n",
      "Epoch 13, Test Loss: 0.16069, Test Accuracy: 0.94178\n",
      "\n",
      "Epoch 14, Training Loss: 0.11543, Training Accuracy: 0.95937\n",
      "Epoch 14, Test Loss: 0.15855, Test Accuracy: 0.94007\n",
      "\n",
      "Epoch 15, Training Loss: 0.11965, Training Accuracy: 0.95905\n",
      "Epoch 15, Test Loss: 0.14481, Test Accuracy: 0.94712\n",
      "\n",
      "Epoch 16, Training Loss: 0.11803, Training Accuracy: 0.95967\n",
      "Epoch 16, Test Loss: 0.14767, Test Accuracy: 0.94821\n",
      "\n",
      "Epoch 17, Training Loss: 0.12075, Training Accuracy: 0.95726\n",
      "Epoch 17, Test Loss: 0.15266, Test Accuracy: 0.94745\n",
      "\n",
      "Epoch 18, Training Loss: 0.11539, Training Accuracy: 0.95973\n",
      "Epoch 18, Test Loss: 0.18613, Test Accuracy: 0.93802\n",
      "\n",
      "Epoch 19, Training Loss: 0.11751, Training Accuracy: 0.96032\n",
      "Epoch 19, Test Loss: 0.14538, Test Accuracy: 0.94801\n",
      "\n",
      "Epoch 20, Training Loss: 0.11699, Training Accuracy: 0.95927\n",
      "Epoch 20, Test Loss: 0.16014, Test Accuracy: 0.94620\n",
      "\n",
      "Epoch 21, Training Loss: 0.11526, Training Accuracy: 0.95988\n",
      "Epoch 21, Test Loss: 0.17262, Test Accuracy: 0.93762\n",
      "\n",
      "Epoch 22, Training Loss: 0.11236, Training Accuracy: 0.96039\n",
      "Epoch 22, Test Loss: 0.16481, Test Accuracy: 0.93953\n",
      "\n",
      "Epoch 23, Training Loss: 0.11457, Training Accuracy: 0.95993\n",
      "Epoch 23, Test Loss: 0.16855, Test Accuracy: 0.93837\n",
      "\n",
      "Epoch 24, Training Loss: 0.12255, Training Accuracy: 0.95752\n",
      "Epoch 24, Test Loss: 0.15569, Test Accuracy: 0.94364\n",
      "\n",
      "Epoch 25, Training Loss: 0.11258, Training Accuracy: 0.96083\n",
      "Epoch 25, Test Loss: 0.17075, Test Accuracy: 0.94255\n",
      "\n",
      "Epoch 26, Training Loss: 0.11665, Training Accuracy: 0.95927\n",
      "Epoch 26, Test Loss: 0.17850, Test Accuracy: 0.93499\n",
      "\n",
      "Epoch 27, Training Loss: 0.11377, Training Accuracy: 0.95897\n",
      "Epoch 27, Test Loss: 0.19074, Test Accuracy: 0.93714\n",
      "\n",
      "Epoch 28, Training Loss: 0.11703, Training Accuracy: 0.95876\n",
      "Epoch 28, Test Loss: 0.16580, Test Accuracy: 0.94106\n",
      "\n",
      "Epoch 29, Training Loss: 0.11372, Training Accuracy: 0.95948\n",
      "Epoch 29, Test Loss: 0.16093, Test Accuracy: 0.94484\n",
      "\n",
      "Epoch 30, Training Loss: 0.11320, Training Accuracy: 0.96055\n",
      "Epoch 30, Test Loss: 0.17501, Test Accuracy: 0.93830\n",
      "\n",
      "Epoch 31, Training Loss: 0.11492, Training Accuracy: 0.96007\n",
      "Epoch 31, Test Loss: 0.18258, Test Accuracy: 0.93720\n",
      "\n",
      "Epoch 32, Training Loss: 0.11226, Training Accuracy: 0.96030\n",
      "Epoch 32, Test Loss: 0.16287, Test Accuracy: 0.94414\n",
      "\n",
      "Epoch 33, Training Loss: 0.11231, Training Accuracy: 0.96090\n",
      "Epoch 33, Test Loss: 0.16976, Test Accuracy: 0.93772\n",
      "\n",
      "Epoch 34, Training Loss: 0.11184, Training Accuracy: 0.96093\n",
      "Epoch 34, Test Loss: 0.17161, Test Accuracy: 0.94420\n",
      "\n",
      "Epoch 35, Training Loss: 0.11334, Training Accuracy: 0.96012\n",
      "Epoch 35, Test Loss: 0.15066, Test Accuracy: 0.94502\n",
      "\n",
      "Epoch 36, Training Loss: 0.11409, Training Accuracy: 0.96014\n",
      "Epoch 36, Test Loss: 0.15816, Test Accuracy: 0.94417\n",
      "\n",
      "Epoch 37, Training Loss: 0.10939, Training Accuracy: 0.96236\n",
      "Epoch 37, Test Loss: 0.15728, Test Accuracy: 0.94699\n",
      "\n",
      "Epoch 38, Training Loss: 0.11229, Training Accuracy: 0.96097\n",
      "Epoch 38, Test Loss: 0.14933, Test Accuracy: 0.94379\n",
      "\n",
      "Epoch 39, Training Loss: 0.10948, Training Accuracy: 0.96219\n",
      "Epoch 39, Test Loss: 0.17396, Test Accuracy: 0.94741\n",
      "\n",
      "Epoch 40, Training Loss: 0.11622, Training Accuracy: 0.95847\n",
      "Epoch 40, Test Loss: 0.18816, Test Accuracy: 0.93349\n",
      "\n",
      "Epoch 41, Training Loss: 0.10885, Training Accuracy: 0.96226\n",
      "Epoch 41, Test Loss: 0.15929, Test Accuracy: 0.94564\n",
      "\n",
      "Epoch 42, Training Loss: 0.10969, Training Accuracy: 0.96248\n",
      "Epoch 42, Test Loss: 0.16284, Test Accuracy: 0.94322\n",
      "\n",
      "Epoch 43, Training Loss: 0.11503, Training Accuracy: 0.96119\n",
      "Epoch 43, Test Loss: 0.16684, Test Accuracy: 0.93785\n",
      "\n",
      "Epoch 44, Training Loss: 0.11244, Training Accuracy: 0.96206\n",
      "Epoch 44, Test Loss: 0.16143, Test Accuracy: 0.94388\n",
      "\n",
      "Epoch 45, Training Loss: 0.11419, Training Accuracy: 0.95954\n",
      "Epoch 45, Test Loss: 0.20246, Test Accuracy: 0.93006\n",
      "\n",
      "Epoch 46, Training Loss: 0.11016, Training Accuracy: 0.96172\n",
      "Epoch 46, Test Loss: 0.19666, Test Accuracy: 0.92858\n",
      "\n",
      "Epoch 47, Training Loss: 0.11126, Training Accuracy: 0.96191\n",
      "Epoch 47, Test Loss: 0.15979, Test Accuracy: 0.94367\n",
      "\n",
      "Epoch 48, Training Loss: 0.10583, Training Accuracy: 0.96299\n",
      "Epoch 48, Test Loss: 0.16382, Test Accuracy: 0.94327\n",
      "\n",
      "Epoch 49, Training Loss: 0.11003, Training Accuracy: 0.96262\n",
      "Epoch 49, Test Loss: 0.15952, Test Accuracy: 0.94699\n",
      "\n",
      "Epoch 50, Training Loss: 0.11113, Training Accuracy: 0.96157\n",
      "Epoch 50, Test Loss: 0.16304, Test Accuracy: 0.94308\n",
      "\n",
      "Epoch 51, Training Loss: 0.11004, Training Accuracy: 0.96172\n",
      "Epoch 51, Test Loss: 0.16045, Test Accuracy: 0.94250\n",
      "\n",
      "Epoch 52, Training Loss: 0.11345, Training Accuracy: 0.96077\n",
      "Epoch 52, Test Loss: 0.16158, Test Accuracy: 0.94492\n",
      "\n",
      "Epoch 53, Training Loss: 0.11115, Training Accuracy: 0.96014\n",
      "Epoch 53, Test Loss: 0.16026, Test Accuracy: 0.94225\n",
      "\n",
      "Epoch 54, Training Loss: 0.10841, Training Accuracy: 0.96161\n",
      "Epoch 54, Test Loss: 0.14887, Test Accuracy: 0.94959\n",
      "\n",
      "Epoch 55, Training Loss: 0.10820, Training Accuracy: 0.96351\n",
      "Epoch 55, Test Loss: 0.14896, Test Accuracy: 0.94946\n",
      "\n",
      "Epoch 56, Training Loss: 0.11153, Training Accuracy: 0.96138\n",
      "Epoch 56, Test Loss: 0.17407, Test Accuracy: 0.94143\n",
      "\n",
      "Epoch 57, Training Loss: 0.11451, Training Accuracy: 0.96165\n",
      "Epoch 57, Test Loss: 0.16033, Test Accuracy: 0.94416\n",
      "\n",
      "Epoch 58, Training Loss: 0.10384, Training Accuracy: 0.96391\n",
      "Epoch 58, Test Loss: 0.16235, Test Accuracy: 0.94423\n",
      "\n",
      "Epoch 59, Training Loss: 0.10743, Training Accuracy: 0.96192\n",
      "Epoch 59, Test Loss: 0.20821, Test Accuracy: 0.92275\n",
      "\n",
      "Epoch 60, Training Loss: 0.11212, Training Accuracy: 0.96225\n",
      "Epoch 60, Test Loss: 0.17956, Test Accuracy: 0.93945\n",
      "\n",
      "Epoch 61, Training Loss: 0.10745, Training Accuracy: 0.96386\n",
      "Epoch 61, Test Loss: 0.15753, Test Accuracy: 0.94982\n",
      "\n",
      "Epoch 62, Training Loss: 0.11189, Training Accuracy: 0.96177\n",
      "Epoch 62, Test Loss: 0.15122, Test Accuracy: 0.94366\n",
      "\n",
      "Epoch 63, Training Loss: 0.11201, Training Accuracy: 0.96197\n",
      "Epoch 63, Test Loss: 0.18027, Test Accuracy: 0.93798\n",
      "\n",
      "Epoch 64, Training Loss: 0.11214, Training Accuracy: 0.96088\n",
      "Epoch 64, Test Loss: 0.16448, Test Accuracy: 0.94288\n",
      "\n",
      "Epoch 65, Training Loss: 0.10868, Training Accuracy: 0.96220\n",
      "Epoch 65, Test Loss: 0.16389, Test Accuracy: 0.94806\n",
      "\n",
      "Epoch 66, Training Loss: 0.10436, Training Accuracy: 0.96544\n",
      "Epoch 66, Test Loss: 0.17090, Test Accuracy: 0.94413\n",
      "\n",
      "Epoch 67, Training Loss: 0.11030, Training Accuracy: 0.96286\n",
      "Epoch 67, Test Loss: 0.17567, Test Accuracy: 0.94321\n",
      "\n",
      "Epoch 68, Training Loss: 0.10827, Training Accuracy: 0.96259\n",
      "Epoch 68, Test Loss: 0.15616, Test Accuracy: 0.94223\n",
      "\n",
      "Epoch 69, Training Loss: 0.10921, Training Accuracy: 0.96259\n",
      "Epoch 69, Test Loss: 0.16235, Test Accuracy: 0.94665\n",
      "\n",
      "Epoch 70, Training Loss: 0.10554, Training Accuracy: 0.96330\n",
      "Epoch 70, Test Loss: 0.20446, Test Accuracy: 0.93538\n",
      "\n",
      "Epoch 71, Training Loss: 0.10942, Training Accuracy: 0.96323\n",
      "Epoch 71, Test Loss: 0.16581, Test Accuracy: 0.94211\n",
      "\n",
      "Epoch 72, Training Loss: 0.10886, Training Accuracy: 0.96256\n",
      "Epoch 72, Test Loss: 0.19413, Test Accuracy: 0.92698\n",
      "\n",
      "Epoch 73, Training Loss: 0.10933, Training Accuracy: 0.96214\n",
      "Epoch 73, Test Loss: 0.15812, Test Accuracy: 0.94610\n",
      "\n",
      "Epoch 74, Training Loss: 0.10803, Training Accuracy: 0.96306\n",
      "Epoch 74, Test Loss: 0.16148, Test Accuracy: 0.94044\n",
      "\n",
      "Epoch 75, Training Loss: 0.10764, Training Accuracy: 0.96177\n",
      "Epoch 75, Test Loss: 0.15974, Test Accuracy: 0.94812\n",
      "\n",
      "Epoch 76, Training Loss: 0.10359, Training Accuracy: 0.96268\n",
      "Epoch 76, Test Loss: 0.17905, Test Accuracy: 0.94263\n",
      "\n",
      "Epoch 77, Training Loss: 0.10929, Training Accuracy: 0.96191\n",
      "Epoch 77, Test Loss: 0.17850, Test Accuracy: 0.94075\n",
      "\n",
      "Epoch 78, Training Loss: 0.10508, Training Accuracy: 0.96486\n",
      "Epoch 78, Test Loss: 0.16233, Test Accuracy: 0.94280\n",
      "\n",
      "Epoch 79, Training Loss: 0.10911, Training Accuracy: 0.96318\n",
      "Epoch 79, Test Loss: 0.16252, Test Accuracy: 0.94552\n",
      "\n",
      "Epoch 80, Training Loss: 0.10879, Training Accuracy: 0.96220\n",
      "Epoch 80, Test Loss: 0.15498, Test Accuracy: 0.94371\n",
      "\n",
      "Epoch 81, Training Loss: 0.10609, Training Accuracy: 0.96368\n",
      "Epoch 81, Test Loss: 0.16131, Test Accuracy: 0.94479\n",
      "\n",
      "Epoch 82, Training Loss: 0.10558, Training Accuracy: 0.96444\n",
      "Epoch 82, Test Loss: 0.15898, Test Accuracy: 0.93952\n",
      "\n",
      "Epoch 83, Training Loss: 0.10304, Training Accuracy: 0.96442\n",
      "Epoch 83, Test Loss: 0.20325, Test Accuracy: 0.93430\n",
      "\n",
      "Epoch 84, Training Loss: 0.10144, Training Accuracy: 0.96529\n",
      "Epoch 84, Test Loss: 0.16609, Test Accuracy: 0.94159\n",
      "\n",
      "Epoch 85, Training Loss: 0.09967, Training Accuracy: 0.96521\n",
      "Epoch 85, Test Loss: 0.16883, Test Accuracy: 0.94388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 85\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, labels in zip(X_train, y_train):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if len(labels.size()) == 0:\n",
    "            labels = labels.unsqueeze(0).expand(64, 1).float()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        predicted = outputs > 0.5  # Assuming threshold of 0.5 for binary classification\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.byte()).sum().item()\n",
    "    \n",
    "    # Print training loss and accuracy\n",
    "    train_loss = running_loss / len(y_train)\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:0.5f}, Training Accuracy: {train_accuracy:0.5f}\")\n",
    "    \n",
    "    # Test loss and accuracy calculation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in zip(X_test, y_test):\n",
    "            test_outputs = model(test_inputs)\n",
    "            if len(test_labels.size()) == 0:\n",
    "                test_labels = test_labels.unsqueeze(0).expand(64, 1).float()\n",
    "            test_loss += criterion(test_outputs, test_labels).item()\n",
    "            test_predicted = test_outputs > 0.5\n",
    "            test_total += test_labels.size(0)\n",
    "            test_correct += (test_predicted == test_labels.byte()).sum().item()\n",
    "    \n",
    "    # Print test loss and accuracy\n",
    "    test_loss /= len(y_test)\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"Epoch {epoch+1}, Test Loss: {test_loss:0.5f}, Test Accuracy: {test_accuracy:0.5f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './models/luminar_llama2_4k.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

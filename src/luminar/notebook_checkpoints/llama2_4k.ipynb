{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nutzer\\AppData\\Local\\Temp\\ipykernel_3364\\1828077501.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'against', 'have', \"mightn't\", 'wasn', \"wasn't\", 'theirs', \"aren't\", 'won', 'by', 'they', 'both', 'i', 'should', 'a', 'those', 'just', \"hasn't\", \"you've\", 'did', 'me', 'here', 'once', 'is', 'herself', 'so', 'his', \"that'll\", 'having', 'below', 'before', 'shouldn', 'because', 'of', 'wouldn', 'whom', 'does', 've', 'yours', 'myself', 'up', 'hasn', 'he', 'very', 'it', 'o', \"wouldn't\", 'y', \"you're\", 'couldn', 'out', 'haven', \"you'll\", 'between', 'through', 'these', 'off', 'then', 'do', 'she', \"should've\", 'my', 'don', 'where', 'now', 'd', 'any', \"won't\", \"it's\", 'you', 'under', 'why', \"hadn't\", \"don't\", 'am', 'in', 'only', 'are', 'until', 'above', 'same', 'ours', 'there', 'most', 'mightn', 'be', 'at', 'during', 'isn', 'needn', 'from', 'few', 'while', 'm', 'which', 'how', 'will', 'on', 'such', \"mustn't\", 'with', 'doesn', 'their', 'and', 'yourself', 'about', 'nor', 'than', \"isn't\", \"weren't\", 'each', 'this', 'but', 'being', 'themselves', 'to', 'its', 'weren', 'over', 'can', 'were', 'again', 'the', 'been', 'other', 'when', 'all', 'an', 'doing', 's', 'ma', 'itself', 'who', 'that', 'we', 'into', 'aren', \"she's\", 'down', 'had', 'too', 'or', 'hadn', 'her', 'ain', 'them', 're', \"needn't\", \"haven't\", 'himself', 'as', 'him', 'yourselves', 'for', \"doesn't\", 'ourselves', 'hers', 'didn', \"didn't\", 'what', 't', 'own', 'our', 'after', \"shan't\", \"shouldn't\", \"you'd\", 'll', 'shan', 'no', 'mustn', 'not', \"couldn't\", 'your', 'if', 'was', 'further', 'some', 'more', 'has'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nutzer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nutzer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "fdist = FreqDist(english_stopwords)\n",
    "top_stopwords = set(word.lower() for word, _ in fdist.most_common(1000)) \n",
    "print(top_stopwords)\n",
    "\n",
    "test_text = '''\n",
    "Do you have a problem that you are struggling to solve? Why don't you ask your friends for advice? When people ask for advice on solving a problem, often times they speak to more than one person. This is because different views are better for figuring out a tough problem, many opinions are better than one, and other people may have experienced a problem like yours and may be able to help you in making better decisions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Text Image Recognition\n",
    "\n",
    "Is there some similarity to Malware detection, where it's common to translate malware code to bytes and then images, which are then fed into NN/DL networks? Let's test a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3992\n",
      "3998\n"
     ]
    }
   ],
   "source": [
    "ai = json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\4k\\\\generator_1_daryl149_llama-2-7b-chat-hf_4000_outputs.json\", encoding='utf8'))\n",
    "#ai = ai + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\4k\\\\generator_1_gpt2_4000_outputs.json\", encoding='utf8'))\n",
    "#ai = ai + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_1_gpt2_100_outputs.json\", encoding='utf8'))\n",
    "\n",
    "human = json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\4k\\\\generator_0_daryl149_llama-2-7b-chat-hf_4000_outputs.json\", encoding='utf8'))\n",
    "#human = human + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\4k\\\\generator_0_gpt2_4000_outputs.json\", encoding='utf8'))\n",
    "#human = human + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_0_gpt2_100_outputs.json\", encoding='utf8'))\n",
    "print(len(ai))\n",
    "print(len(human))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis as Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_image(detector_outputs):\n",
    "    for output in detector_outputs:\n",
    "        meta = output['metadata']\n",
    "        height = meta['sample_rate']\n",
    "        width = meta['sample_sequence_length']\n",
    "        results = output['ensemble_results'][0]['sample_results']\n",
    "        \n",
    "        image = Image.new('RGBA', (100, 20), color=(0,0,0,255))\n",
    "        pixels = image.load()\n",
    "\n",
    "        x = 1\n",
    "        offset_x = x\n",
    "        y = 1\n",
    "\n",
    "        for result in results:\n",
    "            steps = result['model_outputs']['steps']\n",
    "            steps.sort(key= lambda x: x['step'])\n",
    "\n",
    "            for step in steps:                \n",
    "                r = 255\n",
    "                g = 255\n",
    "                b = 255\n",
    "                \n",
    "                # Let's try to ignore the stopwords as they are probably useless\n",
    "                vis = int(step['target_prob'] * 255)\n",
    "                if step['target'].lower() in english_stopwords:\n",
    "                    vis = 0\n",
    "\n",
    "                all_probs = step['top_k_probs']\n",
    "                all_probs.sort(reverse=True)\n",
    "                other_probs = all_probs[1:]\n",
    "                \n",
    "                # mid\n",
    "                pixels[x, y] = (r, g, b, vis)\n",
    "                # right\n",
    "                pixels[x+1, y] = (r, g, b, int(other_probs[0] * 255))\n",
    "                # left\n",
    "                pixels[x-1, y] = (r, g, b, int(other_probs[1] * 255))\n",
    "                # down\n",
    "                pixels[x, y-1] = (r, g, b, int(other_probs[2] * 255))\n",
    "                # up\n",
    "                pixels[x, y+1] = (r, g, b, int(other_probs[3] * 255))\n",
    "                \n",
    "                x += 3\n",
    "            \n",
    "            offset_x = int((offset_x % 2 == 1))\n",
    "            x = offset_x\n",
    "            y += 3\n",
    "        display(image)\n",
    "\n",
    "def draw_image(width, height):\n",
    "    image = Image.new('RGBA', (width, height))\n",
    "    pixels = image.load()\n",
    "    # iterate over all pixels\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            r = random.randint(0, 255)\n",
    "            g = random.randint(0, 255)\n",
    "            b = random.randint(0, 255)\n",
    "            a = random.randint(0, 1)\n",
    "            pixels[x, y] = (r, g, b, int(a * 255))  # Set the pixel RGBA values\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAUCAYAAAB7wJiVAAAG3klEQVR4Ae2YS29WVRSGC7TcoV9pKVAoVu6CEgURiIIDjTghRh0Qo3EqIyf+DP+AExKd+Q8cOGNi4thEnZkYY4JyCXIttPV9Pvb6eMAegRkDdvJ2vWfttdde+5yz3u/A0NDQ0ELGUmzjMQO+R3yTeE98mXjoYO2w+Crxrr289nH4euX0XhvkXyk+Lr5EfLn4OnH7XfOoYtaKz4hPiIcO7ska8UHO8vVtHsZw8HuwMlgS8IRWBS81fix2b+OHY3sN+2KXBT8GIwFrwdKAnOTZGIwF3wbkYA4/8djLwfrG7za7PPZiQA3E3Akq92zj5P0ieDdYFxCHxU9NWwL2xb8h2BMcbyBXr4E9fg5WBxV/IHxFQK1vB+T7LaDOyYD1J4PxgPz7g52Ns/Zg4682Sww5Kv8r7fqb2PcfeBj3HhzxNMfgKZr7zTb3m+G3zWvdOYO34X/28trH4a7Be7lzViRRdb47pyu/63ROn9H53Zmux53gvRw/8FeNWApg9Pp/7/2hJWvUPNc3yxl7Q9yUNq/hPEhBDQ73qMGNXGxwk2qMF4mdE58Vvy1+SRz5XWyMyDkv3n9j2/VV+e+KXxPfIj4t7rNvlv8+zRtLe9OybwS9gJZ5J6gW+yh8SfB18ElAK18IaGv84PVgbYDc0No7AqTi42BzMBpsD1gL8LFPgRxw7MrGNzV7I5aWB18GZ4N9AbFvBpVjb3it4Uz455pdEzvd8HzssQZirgTUzj3gTOMBNV4KOE/lt2Vv6rkalPzW/Itt7q3YPQH+o0Hl4ny7m/9InkS/g2WJX3C7dbUqT7G/OPFuYfvdks7jNu9a6zyWDvvNV+ei6rHfP8b2m092rLXEOf5JOZ1WtU2I+1y98tuWJLk93f6jCa7Bza4xViS2J25psrxcV4xzWposF9sVz42vwe9YjZtFYi0FlguFDHmvC54Qt9zJPcQL+6jBF12NO0ViL4r7jFfkv0/zttOitMlUs3djadldQa/Z+mpCdpAL4l8ImIdvCyYapx3J9XnAlwYyQAxyMhacCU4EowF+pKTa+Uh4tfP+8PUBMdSIpMDJgVzAPwuQj2vN7oil9pIKYmoNcSVl5ScW/mnAPEDe8M03uy72pwBbdX7Q5uZiWcPcd8HGoHJjN7dr9uW+IbnHmq8flyfR7yRZ/AtdrWoJcrvxlvcTZa2/KOiQ8luavNZ7WV4cs015/G8Jr3X+hA/2dQ32+yz2Pw63/HbFd8W4zkVrSMLBQynJ8tcCG9YgsIZl7UY5Yx1j/5Ri3M6WF6+1pFzS2lvi/mryWn4TavxT5CHrM1qCphXn2uR+4CvOfvM5XfTEffYx+S3p9915w5Ea5ODNgPa7GCAVvYD2oQ1p7V+DyWBFgJ/WrhjsaLuGIztbA2QNOTsbIF8lj+TcFHwf0M609nBwJihpei2cfUoW4Mgptjj1sg/X7FMStLL5SnaYZz9QNVDjoYC5IwEy+EMwFeBjX2yBvUaCP4Ne8zs/tbF/5Uc2x1oc95h7R67tAfeXenlAvFgGMZ3/dcIb0w9ODE+xuP2WEcuO29O8S6a6YrxvF1+v2rpi7KdD6ixdEuf4J+U95bfkeq/BvapYbEmWv2TczpsTtNjoP502YRnhptZwC1suZiogtivebT6qeLe83ENdX0fjCvKXm+XCdU4rnhenhuvxl1LNYx1/QxNe6/vGw/nvyJu/g7c/OBXQYqcbdsbSaocD5m8HJQmz4bQw/sKq8HMBklNxWK7PBwcbWHcywM/adYHjqeFoA5yYE8FM40hB7bki/ECA3FSdc22e+oljvtbuCkcy8FNPyd2h8IrZEf5cQAx+7gH8wwDJgnNWLOA8y4PjDSWXU7mu/NTC9VfBdMAerN2bJ9LvVFn8C3zf9yfC/dU0KX/oIKaL85ZUnq4Y78Xbtli8JchS1uuI78rjtRNaa/9AOnL2rYqxLJt7bcIH9bsG3wfUp87oezuIr3lsSdbNXNS4XiT2sjib1HAbUmyNLunwF5TzzNfC2FHxW+KW0DsdfufcohhL5d8dfrmH/tAFN7HG7iKxPq85L1oNc5/d99ayWevySO593fwSW61cbViSQqshDfjfC2hJQPxw8FewJmB+vlnaGCkCtHdJUH1pzMS3LGANOeoLiZzkxrcrGA+IeTnYEODHVy1/KnxtcLoBaZgIDgesux0sbXw2lrrwF6jtXMBZKw7L9fngYAPrTgach7W9wPFjua4zwok5Ecw03imzeRL97pFlbf9l6E88xHmK5Xerus3dzjzpijd3C1uOnJ+uq7WOd8u7Bkuf/ZPK4xq6uM/SFdO1l+N9LtfTS1Cdy/Ej5betm0ngs/EU3IH6DXkKSnlWAnfgX/nP3Dy0035bAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_to_image(human[:2])\n",
    "output_to_image(ai[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Let's try some CNN detection on the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D CNN\n",
    "\n",
    "Let's start by creating 2d arrays with x channels per \"pixel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(detector_outputs, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for output in detector_outputs:\n",
    "        results = output['ensemble_results'][0]['sample_results']\n",
    "        rows = []\n",
    "\n",
    "        for result in results:\n",
    "            steps = result['model_outputs']['steps']\n",
    "            steps.sort(key= lambda x: x['step'])\n",
    "            columns = []\n",
    "            for step in steps:\n",
    "                prob = step['target_prob']\n",
    "                # Common target stepwords, we mask out\n",
    "                #if step['target'].lower() in english_stopwords:\n",
    "                #    prob = 0\n",
    "                channels = [prob]\n",
    "                # Again, mask out common stopwords\n",
    "                top_k_tokens = step['top_k_tokens']\n",
    "                top_k_probs = step['top_k_probs']\n",
    "                for i in range(0 , len(top_k_tokens)):\n",
    "                    token = top_k_tokens[i]\n",
    "                    #if token.lower() in english_stopwords:\n",
    "                    #    top_k_probs[i] = 0\n",
    "                top_k_probs.sort()\n",
    "                channels = channels + top_k_probs\n",
    "                columns.append(channels)\n",
    "                \n",
    "            rows.append(columns)\n",
    "          \n",
    "        data.append(rows)\n",
    "        labels.append(label)  \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7990\n",
      "torch.Size([7990, 11, 6, 32])\n",
      "tensor([[[[1.4948e-02, 0.0000e+00, 0.0000e+00,  ..., 8.7772e-01,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.7504e-01, 9.9988e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           4.6888e-01, 0.0000e+00],\n",
      "          [0.0000e+00, 8.9913e-01, 9.2838e-01,  ..., 0.0000e+00,\n",
      "           5.4406e-02, 5.1559e-01],\n",
      "          [5.3736e-02, 1.0000e+00, 1.0000e+00,  ..., 1.3157e-01,\n",
      "           9.9854e-01, 0.0000e+00],\n",
      "          [8.9472e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           1.1258e-04, 9.9999e-01],\n",
      "          [1.2509e-01, 0.0000e+00, 1.8191e-01,  ..., 5.0755e-01,\n",
      "           2.1886e-04, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 1.1921e-04, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 8.2722e-05],\n",
      "          [0.0000e+00, 3.8597e-11, 9.1448e-09,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.4786e-09, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           3.6531e-04, 1.3180e-08],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.0243e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [0.0000e+00, 2.4706e-04, 1.4194e-06,  ..., 0.0000e+00,\n",
      "           5.3669e-05, 2.1003e-04],\n",
      "          [0.0000e+00, 2.9091e-10, 2.0017e-08,  ..., 0.0000e+00,\n",
      "           7.8861e-07, 0.0000e+00],\n",
      "          [9.3571e-07, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           4.5403e-04, 2.8191e-08],\n",
      "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 1.3005e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.7973e-02, 1.7276e-06, 5.6889e-03,  ..., 1.5090e-02,\n",
      "           1.1799e-02, 2.8059e-07],\n",
      "          [8.7532e-03, 4.1640e-06, 2.3832e-02,  ..., 1.5819e-04,\n",
      "           1.0294e-04, 3.0231e-03],\n",
      "          [1.9548e-03, 3.3454e-02, 1.5002e-04,  ..., 1.2344e-06,\n",
      "           5.4406e-02, 9.0924e-03],\n",
      "          [1.2152e-02, 3.0247e-07, 4.1782e-07,  ..., 1.1656e-01,\n",
      "           3.3171e-04, 2.8596e-02],\n",
      "          [1.2599e-02, 0.0000e+00, 6.1592e-05,  ..., 0.0000e+00,\n",
      "           8.3586e-02, 9.4828e-07],\n",
      "          [6.9282e-03, 8.2770e-08, 1.2373e-01,  ..., 1.1954e-02,\n",
      "           8.2962e-03, 1.7600e-02]],\n",
      "\n",
      "         [[3.6603e-02, 2.0251e-05, 6.2744e-02,  ..., 2.4267e-02,\n",
      "           4.2772e-02, 9.2111e-05],\n",
      "          [3.7504e-01, 1.0091e-04, 7.2345e-02,  ..., 3.1877e-04,\n",
      "           4.6321e-02, 4.1403e-03],\n",
      "          [2.5907e-02, 5.4718e-02, 7.1137e-02,  ..., 1.7477e-06,\n",
      "           8.3840e-02, 4.6430e-01],\n",
      "          [3.4829e-02, 2.5234e-06, 8.6524e-07,  ..., 1.4949e-01,\n",
      "           9.0790e-04, 1.0526e-01],\n",
      "          [8.5607e-02, 3.8042e-03, 7.1353e-04,  ..., 8.7365e-07,\n",
      "           1.1298e-01, 2.6535e-06],\n",
      "          [8.9762e-03, 3.8180e-07, 1.8191e-01,  ..., 4.1020e-02,\n",
      "           6.9136e-02, 2.4928e-02]],\n",
      "\n",
      "         [[4.7149e-02, 2.0830e-05, 3.6222e-01,  ..., 8.7772e-01,\n",
      "           6.2084e-01, 9.5363e-04],\n",
      "          [5.9479e-01, 9.9988e-01, 2.0370e-01,  ..., 6.5755e-04,\n",
      "           4.6888e-01, 4.4334e-02],\n",
      "          [1.6875e-01, 8.9913e-01, 9.2838e-01,  ..., 4.1641e-06,\n",
      "           7.3822e-01, 5.1559e-01],\n",
      "          [1.0978e-01, 1.0000e+00, 1.0000e+00,  ..., 2.2018e-01,\n",
      "           9.9854e-01, 5.2323e-01],\n",
      "          [8.9472e-01, 6.2406e-01, 1.8052e-03,  ..., 8.7394e-05,\n",
      "           7.8377e-01, 9.9999e-01],\n",
      "          [5.4360e-01, 8.2000e-06, 3.2415e-01,  ..., 3.6570e-01,\n",
      "           9.3654e-02, 3.2066e-02]]]])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "data_ai, labels_ai = create_dataset(ai, 1)\n",
    "data_hu, labels_hu = create_dataset(human, 0)\n",
    "data = np.concatenate((data_ai, data_hu), axis=0)\n",
    "labels = np.concatenate((labels_hu, labels_ai))\n",
    "print(len(labels))\n",
    "\n",
    "# Convert numpy arrays to tensors\n",
    "# We need: [batch_size, channels, height, width]\n",
    "# data_tensor = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1).unsqueeze(-2)\n",
    "data_tensor = torch.tensor(data, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "print(data_tensor.shape)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "print(data_tensor[:1])\n",
    "print(labels_tensor[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape torch.Size([6392, 11, 6, 32])\n",
      "x_test shape torch.Size([1598, 11, 6, 32])\n",
      "y_train shape torch.Size([6392])\n",
      "y_test shape torch.Size([1598])\n",
      "tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
    "print(\"x_train shape\",X_train.shape)\n",
    "print(\"x_test shape\",X_test.shape)\n",
    "print(\"y_train shape\",y_train.shape)\n",
    "print(\"y_test shape\",y_test.shape)\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(11, 32, kernel_size=(2, 8), stride=(1, 1), padding=(0, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Dropout(p=0.25, inplace=False)\n",
      "  (4): Conv2d(32, 64, kernel_size=(2, 6), stride=(1, 1), padding=(0, 1))\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Dropout(p=0.25, inplace=False)\n",
      "  (8): Flatten(start_dim=1, end_dim=-1)\n",
      "  (9): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.4, inplace=False)\n",
      "  (12): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (13): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=11, out_channels=32, kernel_size=(2,8), padding=(0, 1)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(1,2), stride=(1, 2)),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2,6), padding=(0,1)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(1,2), stride=(1,2)),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=20, out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.4),\n",
    "\n",
    "    nn.Linear(in_features=64, out_features=1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.51574, Training Accuracy: 0.74052\n",
      "Epoch 1, Test Loss: 0.48548, Test Accuracy: 0.77341\n",
      "\n",
      "Epoch 2, Training Loss: 0.42904, Training Accuracy: 0.81510\n",
      "Epoch 2, Test Loss: 0.51961, Test Accuracy: 0.74962\n",
      "\n",
      "Epoch 3, Training Loss: 0.39321, Training Accuracy: 0.83878\n",
      "Epoch 3, Test Loss: 0.43014, Test Accuracy: 0.81310\n",
      "\n",
      "Epoch 4, Training Loss: 0.35986, Training Accuracy: 0.85630\n",
      "Epoch 4, Test Loss: 0.58915, Test Accuracy: 0.72323\n",
      "\n",
      "Epoch 5, Training Loss: 0.33031, Training Accuracy: 0.87125\n",
      "Epoch 5, Test Loss: 0.39631, Test Accuracy: 0.83915\n",
      "\n",
      "Epoch 6, Training Loss: 0.30617, Training Accuracy: 0.88214\n",
      "Epoch 6, Test Loss: 0.45426, Test Accuracy: 0.80577\n",
      "\n",
      "Epoch 7, Training Loss: 0.29605, Training Accuracy: 0.88599\n",
      "Epoch 7, Test Loss: 0.35204, Test Accuracy: 0.85042\n",
      "\n",
      "Epoch 8, Training Loss: 0.28474, Training Accuracy: 0.88797\n",
      "Epoch 8, Test Loss: 0.35783, Test Accuracy: 0.84328\n",
      "\n",
      "Epoch 9, Training Loss: 0.27146, Training Accuracy: 0.88910\n",
      "Epoch 9, Test Loss: 0.30641, Test Accuracy: 0.86127\n",
      "\n",
      "Epoch 10, Training Loss: 0.25938, Training Accuracy: 0.89009\n",
      "Epoch 10, Test Loss: 0.28672, Test Accuracy: 0.87020\n",
      "\n",
      "Epoch 11, Training Loss: 0.25373, Training Accuracy: 0.89230\n",
      "Epoch 11, Test Loss: 0.31247, Test Accuracy: 0.85726\n",
      "\n",
      "Epoch 12, Training Loss: 0.24352, Training Accuracy: 0.89803\n",
      "Epoch 12, Test Loss: 0.29070, Test Accuracy: 0.86208\n",
      "\n",
      "Epoch 13, Training Loss: 0.23565, Training Accuracy: 0.90110\n",
      "Epoch 13, Test Loss: 0.27386, Test Accuracy: 0.87092\n",
      "\n",
      "Epoch 14, Training Loss: 0.23355, Training Accuracy: 0.90328\n",
      "Epoch 14, Test Loss: 0.29214, Test Accuracy: 0.85969\n",
      "\n",
      "Epoch 15, Training Loss: 0.22443, Training Accuracy: 0.90679\n",
      "Epoch 15, Test Loss: 0.25701, Test Accuracy: 0.87559\n",
      "\n",
      "Epoch 16, Training Loss: 0.22244, Training Accuracy: 0.90742\n",
      "Epoch 16, Test Loss: 0.26324, Test Accuracy: 0.87141\n",
      "\n",
      "Epoch 17, Training Loss: 0.21981, Training Accuracy: 0.90895\n",
      "Epoch 17, Test Loss: 0.31518, Test Accuracy: 0.85158\n",
      "\n",
      "Epoch 18, Training Loss: 0.21871, Training Accuracy: 0.90883\n",
      "Epoch 18, Test Loss: 0.28626, Test Accuracy: 0.86228\n",
      "\n",
      "Epoch 19, Training Loss: 0.21329, Training Accuracy: 0.91207\n",
      "Epoch 19, Test Loss: 0.31802, Test Accuracy: 0.86226\n",
      "\n",
      "Epoch 20, Training Loss: 0.20432, Training Accuracy: 0.91910\n",
      "Epoch 20, Test Loss: 0.24753, Test Accuracy: 0.89745\n",
      "\n",
      "Epoch 21, Training Loss: 0.20460, Training Accuracy: 0.91871\n",
      "Epoch 21, Test Loss: 0.30115, Test Accuracy: 0.87424\n",
      "\n",
      "Epoch 22, Training Loss: 0.20074, Training Accuracy: 0.92096\n",
      "Epoch 22, Test Loss: 0.29376, Test Accuracy: 0.87590\n",
      "\n",
      "Epoch 23, Training Loss: 0.20049, Training Accuracy: 0.92099\n",
      "Epoch 23, Test Loss: 0.32461, Test Accuracy: 0.86150\n",
      "\n",
      "Epoch 24, Training Loss: 0.19123, Training Accuracy: 0.92397\n",
      "Epoch 24, Test Loss: 0.27050, Test Accuracy: 0.89006\n",
      "\n",
      "Epoch 25, Training Loss: 0.20182, Training Accuracy: 0.92064\n",
      "Epoch 25, Test Loss: 0.29873, Test Accuracy: 0.87688\n",
      "\n",
      "Epoch 26, Training Loss: 0.19376, Training Accuracy: 0.92336\n",
      "Epoch 26, Test Loss: 0.29600, Test Accuracy: 0.87671\n",
      "\n",
      "Epoch 27, Training Loss: 0.18787, Training Accuracy: 0.92453\n",
      "Epoch 27, Test Loss: 0.32279, Test Accuracy: 0.86537\n",
      "\n",
      "Epoch 28, Training Loss: 0.18723, Training Accuracy: 0.92283\n",
      "Epoch 28, Test Loss: 0.22318, Test Accuracy: 0.91045\n",
      "\n",
      "Epoch 29, Training Loss: 0.18386, Training Accuracy: 0.92582\n",
      "Epoch 29, Test Loss: 0.21784, Test Accuracy: 0.91387\n",
      "\n",
      "Epoch 30, Training Loss: 0.17966, Training Accuracy: 0.92819\n",
      "Epoch 30, Test Loss: 0.27621, Test Accuracy: 0.88592\n",
      "\n",
      "Epoch 31, Training Loss: 0.18076, Training Accuracy: 0.92768\n",
      "Epoch 31, Test Loss: 0.23756, Test Accuracy: 0.90603\n",
      "\n",
      "Epoch 32, Training Loss: 0.18296, Training Accuracy: 0.92664\n",
      "Epoch 32, Test Loss: 0.25907, Test Accuracy: 0.89183\n",
      "\n",
      "Epoch 33, Training Loss: 0.17675, Training Accuracy: 0.92954\n",
      "Epoch 33, Test Loss: 0.25508, Test Accuracy: 0.89671\n",
      "\n",
      "Epoch 34, Training Loss: 0.17373, Training Accuracy: 0.93081\n",
      "Epoch 34, Test Loss: 0.25540, Test Accuracy: 0.89723\n",
      "\n",
      "Epoch 35, Training Loss: 0.17455, Training Accuracy: 0.92891\n",
      "Epoch 35, Test Loss: 0.21348, Test Accuracy: 0.91425\n",
      "\n",
      "Epoch 36, Training Loss: 0.17226, Training Accuracy: 0.92938\n",
      "Epoch 36, Test Loss: 0.27127, Test Accuracy: 0.89191\n",
      "\n",
      "Epoch 37, Training Loss: 0.17156, Training Accuracy: 0.93021\n",
      "Epoch 37, Test Loss: 0.30790, Test Accuracy: 0.87709\n",
      "\n",
      "Epoch 38, Training Loss: 0.16990, Training Accuracy: 0.93064\n",
      "Epoch 38, Test Loss: 0.27899, Test Accuracy: 0.88912\n",
      "\n",
      "Epoch 39, Training Loss: 0.16551, Training Accuracy: 0.93288\n",
      "Epoch 39, Test Loss: 0.29664, Test Accuracy: 0.88294\n",
      "\n",
      "Epoch 40, Training Loss: 0.16615, Training Accuracy: 0.93296\n",
      "Epoch 40, Test Loss: 0.32931, Test Accuracy: 0.86814\n",
      "\n",
      "Epoch 41, Training Loss: 0.16464, Training Accuracy: 0.93311\n",
      "Epoch 41, Test Loss: 0.30944, Test Accuracy: 0.87614\n",
      "\n",
      "Epoch 42, Training Loss: 0.16295, Training Accuracy: 0.93457\n",
      "Epoch 42, Test Loss: 0.26196, Test Accuracy: 0.89213\n",
      "\n",
      "Epoch 43, Training Loss: 0.15903, Training Accuracy: 0.93532\n",
      "Epoch 43, Test Loss: 0.28987, Test Accuracy: 0.88230\n",
      "\n",
      "Epoch 44, Training Loss: 0.15795, Training Accuracy: 0.93549\n",
      "Epoch 44, Test Loss: 0.32683, Test Accuracy: 0.87165\n",
      "\n",
      "Epoch 45, Training Loss: 0.15927, Training Accuracy: 0.93566\n",
      "Epoch 45, Test Loss: 0.22720, Test Accuracy: 0.91179\n",
      "\n",
      "Epoch 46, Training Loss: 0.15890, Training Accuracy: 0.93594\n",
      "Epoch 46, Test Loss: 0.25154, Test Accuracy: 0.90426\n",
      "\n",
      "Epoch 47, Training Loss: 0.15918, Training Accuracy: 0.93571\n",
      "Epoch 47, Test Loss: 0.25013, Test Accuracy: 0.90447\n",
      "\n",
      "Epoch 48, Training Loss: 0.15855, Training Accuracy: 0.93560\n",
      "Epoch 48, Test Loss: 0.21937, Test Accuracy: 0.91432\n",
      "\n",
      "Epoch 49, Training Loss: 0.15359, Training Accuracy: 0.93806\n",
      "Epoch 49, Test Loss: 0.27654, Test Accuracy: 0.89493\n",
      "\n",
      "Epoch 50, Training Loss: 0.15385, Training Accuracy: 0.93695\n",
      "Epoch 50, Test Loss: 0.25091, Test Accuracy: 0.90402\n",
      "\n",
      "Epoch 51, Training Loss: 0.15247, Training Accuracy: 0.93865\n",
      "Epoch 51, Test Loss: 0.24979, Test Accuracy: 0.90470\n",
      "\n",
      "Epoch 52, Training Loss: 0.15367, Training Accuracy: 0.93747\n",
      "Epoch 52, Test Loss: 0.26510, Test Accuracy: 0.89770\n",
      "\n",
      "Epoch 53, Training Loss: 0.15148, Training Accuracy: 0.93945\n",
      "Epoch 53, Test Loss: 0.30397, Test Accuracy: 0.88051\n",
      "\n",
      "Epoch 54, Training Loss: 0.14798, Training Accuracy: 0.94014\n",
      "Epoch 54, Test Loss: 0.24373, Test Accuracy: 0.91156\n",
      "\n",
      "Epoch 55, Training Loss: 0.15000, Training Accuracy: 0.93921\n",
      "Epoch 55, Test Loss: 0.25922, Test Accuracy: 0.89760\n",
      "\n",
      "Epoch 56, Training Loss: 0.14996, Training Accuracy: 0.93892\n",
      "Epoch 56, Test Loss: 0.24095, Test Accuracy: 0.90746\n",
      "\n",
      "Epoch 57, Training Loss: 0.14693, Training Accuracy: 0.93956\n",
      "Epoch 57, Test Loss: 0.24537, Test Accuracy: 0.90903\n",
      "\n",
      "Epoch 58, Training Loss: 0.15292, Training Accuracy: 0.93752\n",
      "Epoch 58, Test Loss: 0.25411, Test Accuracy: 0.90619\n",
      "\n",
      "Epoch 59, Training Loss: 0.14754, Training Accuracy: 0.93948\n",
      "Epoch 59, Test Loss: 0.21848, Test Accuracy: 0.91947\n",
      "\n",
      "Epoch 60, Training Loss: 0.14882, Training Accuracy: 0.93938\n",
      "Epoch 60, Test Loss: 0.25590, Test Accuracy: 0.90185\n",
      "\n",
      "Epoch 61, Training Loss: 0.14712, Training Accuracy: 0.94052\n",
      "Epoch 61, Test Loss: 0.26417, Test Accuracy: 0.89908\n",
      "\n",
      "Epoch 62, Training Loss: 0.14693, Training Accuracy: 0.94152\n",
      "Epoch 62, Test Loss: 0.27629, Test Accuracy: 0.89481\n",
      "\n",
      "Epoch 63, Training Loss: 0.14337, Training Accuracy: 0.94166\n",
      "Epoch 63, Test Loss: 0.30744, Test Accuracy: 0.88396\n",
      "\n",
      "Epoch 64, Training Loss: 0.14633, Training Accuracy: 0.94067\n",
      "Epoch 64, Test Loss: 0.22715, Test Accuracy: 0.91649\n",
      "\n",
      "Epoch 65, Training Loss: 0.14464, Training Accuracy: 0.94102\n",
      "Epoch 65, Test Loss: 0.31759, Test Accuracy: 0.88089\n",
      "\n",
      "Epoch 66, Training Loss: 0.14559, Training Accuracy: 0.94107\n",
      "Epoch 66, Test Loss: 0.26846, Test Accuracy: 0.89758\n",
      "\n",
      "Epoch 67, Training Loss: 0.14053, Training Accuracy: 0.94253\n",
      "Epoch 67, Test Loss: 0.22872, Test Accuracy: 0.91882\n",
      "\n",
      "Epoch 68, Training Loss: 0.14275, Training Accuracy: 0.94138\n",
      "Epoch 68, Test Loss: 0.25189, Test Accuracy: 0.90593\n",
      "\n",
      "Epoch 69, Training Loss: 0.14595, Training Accuracy: 0.94052\n",
      "Epoch 69, Test Loss: 0.22735, Test Accuracy: 0.91546\n",
      "\n",
      "Epoch 70, Training Loss: 0.14171, Training Accuracy: 0.94262\n",
      "Epoch 70, Test Loss: 0.25995, Test Accuracy: 0.90227\n",
      "\n",
      "Epoch 71, Training Loss: 0.14014, Training Accuracy: 0.94296\n",
      "Epoch 71, Test Loss: 0.24536, Test Accuracy: 0.91403\n",
      "\n",
      "Epoch 72, Training Loss: 0.13908, Training Accuracy: 0.94406\n",
      "Epoch 72, Test Loss: 0.28801, Test Accuracy: 0.89270\n",
      "\n",
      "Epoch 73, Training Loss: 0.14367, Training Accuracy: 0.94275\n",
      "Epoch 73, Test Loss: 0.27452, Test Accuracy: 0.90212\n",
      "\n",
      "Epoch 74, Training Loss: 0.14400, Training Accuracy: 0.94207\n",
      "Epoch 74, Test Loss: 0.25012, Test Accuracy: 0.90943\n",
      "\n",
      "Epoch 75, Training Loss: 0.14035, Training Accuracy: 0.94365\n",
      "Epoch 75, Test Loss: 0.23320, Test Accuracy: 0.91213\n",
      "\n",
      "Epoch 76, Training Loss: 0.13804, Training Accuracy: 0.94386\n",
      "Epoch 76, Test Loss: 0.23905, Test Accuracy: 0.91573\n",
      "\n",
      "Epoch 77, Training Loss: 0.13867, Training Accuracy: 0.94371\n",
      "Epoch 77, Test Loss: 0.26062, Test Accuracy: 0.90535\n",
      "\n",
      "Epoch 78, Training Loss: 0.13888, Training Accuracy: 0.94352\n",
      "Epoch 78, Test Loss: 0.26079, Test Accuracy: 0.90250\n",
      "\n",
      "Epoch 79, Training Loss: 0.13755, Training Accuracy: 0.94326\n",
      "Epoch 79, Test Loss: 0.23316, Test Accuracy: 0.91315\n",
      "\n",
      "Epoch 80, Training Loss: 0.13945, Training Accuracy: 0.94297\n",
      "Epoch 80, Test Loss: 0.27294, Test Accuracy: 0.89611\n",
      "\n",
      "Epoch 81, Training Loss: 0.13535, Training Accuracy: 0.94517\n",
      "Epoch 81, Test Loss: 0.27074, Test Accuracy: 0.90088\n",
      "\n",
      "Epoch 82, Training Loss: 0.13738, Training Accuracy: 0.94433\n",
      "Epoch 82, Test Loss: 0.25519, Test Accuracy: 0.90596\n",
      "\n",
      "Epoch 83, Training Loss: 0.13519, Training Accuracy: 0.94502\n",
      "Epoch 83, Test Loss: 0.22800, Test Accuracy: 0.91485\n",
      "\n",
      "Epoch 84, Training Loss: 0.13883, Training Accuracy: 0.94245\n",
      "Epoch 84, Test Loss: 0.23404, Test Accuracy: 0.91639\n",
      "\n",
      "Epoch 85, Training Loss: 0.13781, Training Accuracy: 0.94458\n",
      "Epoch 85, Test Loss: 0.26908, Test Accuracy: 0.89798\n",
      "\n",
      "Epoch 86, Training Loss: 0.13331, Training Accuracy: 0.94546\n",
      "Epoch 86, Test Loss: 0.26692, Test Accuracy: 0.89978\n",
      "\n",
      "Epoch 87, Training Loss: 0.13566, Training Accuracy: 0.94476\n",
      "Epoch 87, Test Loss: 0.30331, Test Accuracy: 0.88665\n",
      "\n",
      "Epoch 88, Training Loss: 0.13541, Training Accuracy: 0.94457\n",
      "Epoch 88, Test Loss: 0.23074, Test Accuracy: 0.91364\n",
      "\n",
      "Epoch 89, Training Loss: 0.13208, Training Accuracy: 0.94699\n",
      "Epoch 89, Test Loss: 0.24473, Test Accuracy: 0.90871\n",
      "\n",
      "Epoch 90, Training Loss: 0.13092, Training Accuracy: 0.94706\n",
      "Epoch 90, Test Loss: 0.25948, Test Accuracy: 0.90614\n",
      "\n",
      "Epoch 91, Training Loss: 0.13643, Training Accuracy: 0.94527\n",
      "Epoch 91, Test Loss: 0.26430, Test Accuracy: 0.90282\n",
      "\n",
      "Epoch 92, Training Loss: 0.13263, Training Accuracy: 0.94654\n",
      "Epoch 92, Test Loss: 0.27164, Test Accuracy: 0.89972\n",
      "\n",
      "Epoch 93, Training Loss: 0.12717, Training Accuracy: 0.94827\n",
      "Epoch 93, Test Loss: 0.25878, Test Accuracy: 0.90891\n",
      "\n",
      "Epoch 94, Training Loss: 0.13281, Training Accuracy: 0.94608\n",
      "Epoch 94, Test Loss: 0.22693, Test Accuracy: 0.91954\n",
      "\n",
      "Epoch 95, Training Loss: 0.12784, Training Accuracy: 0.94861\n",
      "Epoch 95, Test Loss: 0.23937, Test Accuracy: 0.91978\n",
      "\n",
      "Epoch 96, Training Loss: 0.12855, Training Accuracy: 0.94743\n",
      "Epoch 96, Test Loss: 0.25730, Test Accuracy: 0.91296\n",
      "\n",
      "Epoch 97, Training Loss: 0.13493, Training Accuracy: 0.94582\n",
      "Epoch 97, Test Loss: 0.23429, Test Accuracy: 0.91981\n",
      "\n",
      "Epoch 98, Training Loss: 0.13137, Training Accuracy: 0.94733\n",
      "Epoch 98, Test Loss: 0.30200, Test Accuracy: 0.88849\n",
      "\n",
      "Epoch 99, Training Loss: 0.13067, Training Accuracy: 0.94725\n",
      "Epoch 99, Test Loss: 0.33767, Test Accuracy: 0.87927\n",
      "\n",
      "Epoch 100, Training Loss: 0.12699, Training Accuracy: 0.94897\n",
      "Epoch 100, Test Loss: 0.22677, Test Accuracy: 0.91879\n",
      "\n",
      "Epoch 101, Training Loss: 0.12766, Training Accuracy: 0.94808\n",
      "Epoch 101, Test Loss: 0.32104, Test Accuracy: 0.88578\n",
      "\n",
      "Epoch 102, Training Loss: 0.13193, Training Accuracy: 0.94642\n",
      "Epoch 102, Test Loss: 0.25791, Test Accuracy: 0.90793\n",
      "\n",
      "Epoch 103, Training Loss: 0.12430, Training Accuracy: 0.94966\n",
      "Epoch 103, Test Loss: 0.30887, Test Accuracy: 0.88689\n",
      "\n",
      "Epoch 104, Training Loss: 0.12885, Training Accuracy: 0.94853\n",
      "Epoch 104, Test Loss: 0.22137, Test Accuracy: 0.92472\n",
      "\n",
      "Epoch 105, Training Loss: 0.13053, Training Accuracy: 0.94699\n",
      "Epoch 105, Test Loss: 0.23148, Test Accuracy: 0.91859\n",
      "\n",
      "Epoch 106, Training Loss: 0.12903, Training Accuracy: 0.94777\n",
      "Epoch 106, Test Loss: 0.23535, Test Accuracy: 0.91830\n",
      "\n",
      "Epoch 107, Training Loss: 0.12782, Training Accuracy: 0.94810\n",
      "Epoch 107, Test Loss: 0.33320, Test Accuracy: 0.88390\n",
      "\n",
      "Epoch 108, Training Loss: 0.12816, Training Accuracy: 0.94778\n",
      "Epoch 108, Test Loss: 0.23759, Test Accuracy: 0.92063\n",
      "\n",
      "Epoch 109, Training Loss: 0.12553, Training Accuracy: 0.94967\n",
      "Epoch 109, Test Loss: 0.28726, Test Accuracy: 0.89937\n",
      "\n",
      "Epoch 110, Training Loss: 0.12570, Training Accuracy: 0.94913\n",
      "Epoch 110, Test Loss: 0.26647, Test Accuracy: 0.90975\n",
      "\n",
      "Epoch 111, Training Loss: 0.12560, Training Accuracy: 0.94894\n",
      "Epoch 111, Test Loss: 0.29231, Test Accuracy: 0.89936\n",
      "\n",
      "Epoch 112, Training Loss: 0.12584, Training Accuracy: 0.94892\n",
      "Epoch 112, Test Loss: 0.24598, Test Accuracy: 0.91217\n",
      "\n",
      "Epoch 113, Training Loss: 0.12663, Training Accuracy: 0.94875\n",
      "Epoch 113, Test Loss: 0.23906, Test Accuracy: 0.91760\n",
      "\n",
      "Epoch 114, Training Loss: 0.12458, Training Accuracy: 0.94919\n",
      "Epoch 114, Test Loss: 0.25467, Test Accuracy: 0.91106\n",
      "\n",
      "Epoch 115, Training Loss: 0.12769, Training Accuracy: 0.94851\n",
      "Epoch 115, Test Loss: 0.25066, Test Accuracy: 0.91441\n",
      "\n",
      "Epoch 116, Training Loss: 0.12503, Training Accuracy: 0.94923\n",
      "Epoch 116, Test Loss: 0.24140, Test Accuracy: 0.91905\n",
      "\n",
      "Epoch 117, Training Loss: 0.12306, Training Accuracy: 0.95024\n",
      "Epoch 117, Test Loss: 0.23363, Test Accuracy: 0.92323\n",
      "\n",
      "Epoch 118, Training Loss: 0.11966, Training Accuracy: 0.95137\n",
      "Epoch 118, Test Loss: 0.35389, Test Accuracy: 0.86950\n",
      "\n",
      "Epoch 119, Training Loss: 0.12305, Training Accuracy: 0.94968\n",
      "Epoch 119, Test Loss: 0.24874, Test Accuracy: 0.91531\n",
      "\n",
      "Epoch 120, Training Loss: 0.12753, Training Accuracy: 0.94910\n",
      "Epoch 120, Test Loss: 0.24280, Test Accuracy: 0.91667\n",
      "\n",
      "Epoch 121, Training Loss: 0.12363, Training Accuracy: 0.94977\n",
      "Epoch 121, Test Loss: 0.33291, Test Accuracy: 0.88460\n",
      "\n",
      "Epoch 122, Training Loss: 0.12158, Training Accuracy: 0.94956\n",
      "Epoch 122, Test Loss: 0.23311, Test Accuracy: 0.92563\n",
      "\n",
      "Epoch 123, Training Loss: 0.12105, Training Accuracy: 0.95065\n",
      "Epoch 123, Test Loss: 0.25165, Test Accuracy: 0.91801\n",
      "\n",
      "Epoch 124, Training Loss: 0.12413, Training Accuracy: 0.94974\n",
      "Epoch 124, Test Loss: 0.24900, Test Accuracy: 0.91581\n",
      "\n",
      "Epoch 125, Training Loss: 0.12316, Training Accuracy: 0.95016\n",
      "Epoch 125, Test Loss: 0.29674, Test Accuracy: 0.89334\n",
      "\n",
      "Epoch 126, Training Loss: 0.12078, Training Accuracy: 0.95128\n",
      "Epoch 126, Test Loss: 0.26157, Test Accuracy: 0.90566\n",
      "\n",
      "Epoch 127, Training Loss: 0.12093, Training Accuracy: 0.95126\n",
      "Epoch 127, Test Loss: 0.23616, Test Accuracy: 0.92341\n",
      "\n",
      "Epoch 128, Training Loss: 0.12286, Training Accuracy: 0.95007\n",
      "Epoch 128, Test Loss: 0.23799, Test Accuracy: 0.91716\n",
      "\n",
      "Epoch 129, Training Loss: 0.12244, Training Accuracy: 0.95028\n",
      "Epoch 129, Test Loss: 0.25948, Test Accuracy: 0.90685\n",
      "\n",
      "Epoch 130, Training Loss: 0.12198, Training Accuracy: 0.95024\n",
      "Epoch 130, Test Loss: 0.30348, Test Accuracy: 0.88882\n",
      "\n",
      "Epoch 131, Training Loss: 0.11757, Training Accuracy: 0.95272\n",
      "Epoch 131, Test Loss: 0.25036, Test Accuracy: 0.91961\n",
      "\n",
      "Epoch 132, Training Loss: 0.11583, Training Accuracy: 0.95306\n",
      "Epoch 132, Test Loss: 0.26460, Test Accuracy: 0.91248\n",
      "\n",
      "Epoch 133, Training Loss: 0.12328, Training Accuracy: 0.94959\n",
      "Epoch 133, Test Loss: 0.23062, Test Accuracy: 0.92012\n",
      "\n",
      "Epoch 134, Training Loss: 0.12235, Training Accuracy: 0.95052\n",
      "Epoch 134, Test Loss: 0.26438, Test Accuracy: 0.90701\n",
      "\n",
      "Epoch 135, Training Loss: 0.11822, Training Accuracy: 0.95212\n",
      "Epoch 135, Test Loss: 0.25445, Test Accuracy: 0.91536\n",
      "\n",
      "Epoch 136, Training Loss: 0.11988, Training Accuracy: 0.95067\n",
      "Epoch 136, Test Loss: 0.23661, Test Accuracy: 0.91626\n",
      "\n",
      "Epoch 137, Training Loss: 0.12092, Training Accuracy: 0.95030\n",
      "Epoch 137, Test Loss: 0.24941, Test Accuracy: 0.91026\n",
      "\n",
      "Epoch 138, Training Loss: 0.12165, Training Accuracy: 0.95056\n",
      "Epoch 138, Test Loss: 0.24628, Test Accuracy: 0.91686\n",
      "\n",
      "Epoch 139, Training Loss: 0.12198, Training Accuracy: 0.95119\n",
      "Epoch 139, Test Loss: 0.44313, Test Accuracy: 0.84590\n",
      "\n",
      "Epoch 140, Training Loss: 0.11740, Training Accuracy: 0.95187\n",
      "Epoch 140, Test Loss: 0.25539, Test Accuracy: 0.90862\n",
      "\n",
      "Epoch 141, Training Loss: 0.12078, Training Accuracy: 0.95051\n",
      "Epoch 141, Test Loss: 0.26179, Test Accuracy: 0.91093\n",
      "\n",
      "Epoch 142, Training Loss: 0.12215, Training Accuracy: 0.95157\n",
      "Epoch 142, Test Loss: 0.24538, Test Accuracy: 0.91514\n",
      "\n",
      "Epoch 143, Training Loss: 0.11938, Training Accuracy: 0.95214\n",
      "Epoch 143, Test Loss: 0.22914, Test Accuracy: 0.92639\n",
      "\n",
      "Epoch 144, Training Loss: 0.11958, Training Accuracy: 0.95181\n",
      "Epoch 144, Test Loss: 0.26168, Test Accuracy: 0.90955\n",
      "\n",
      "Epoch 145, Training Loss: 0.11751, Training Accuracy: 0.95207\n",
      "Epoch 145, Test Loss: 0.23385, Test Accuracy: 0.92369\n",
      "\n",
      "Epoch 146, Training Loss: 0.11651, Training Accuracy: 0.95251\n",
      "Epoch 146, Test Loss: 0.29891, Test Accuracy: 0.89648\n",
      "\n",
      "Epoch 147, Training Loss: 0.11866, Training Accuracy: 0.95133\n",
      "Epoch 147, Test Loss: 0.27381, Test Accuracy: 0.90410\n",
      "\n",
      "Epoch 148, Training Loss: 0.11970, Training Accuracy: 0.95157\n",
      "Epoch 148, Test Loss: 0.25800, Test Accuracy: 0.90911\n",
      "\n",
      "Epoch 149, Training Loss: 0.11695, Training Accuracy: 0.95247\n",
      "Epoch 149, Test Loss: 0.24371, Test Accuracy: 0.92141\n",
      "\n",
      "Epoch 150, Training Loss: 0.11777, Training Accuracy: 0.95220\n",
      "Epoch 150, Test Loss: 0.25997, Test Accuracy: 0.90513\n",
      "\n",
      "Epoch 151, Training Loss: 0.12003, Training Accuracy: 0.95140\n",
      "Epoch 151, Test Loss: 0.27863, Test Accuracy: 0.89843\n",
      "\n",
      "Epoch 152, Training Loss: 0.11881, Training Accuracy: 0.95161\n",
      "Epoch 152, Test Loss: 0.25797, Test Accuracy: 0.90523\n",
      "\n",
      "Epoch 153, Training Loss: 0.11980, Training Accuracy: 0.95115\n",
      "Epoch 153, Test Loss: 0.27932, Test Accuracy: 0.89732\n",
      "\n",
      "Epoch 154, Training Loss: 0.12070, Training Accuracy: 0.95143\n",
      "Epoch 154, Test Loss: 0.23155, Test Accuracy: 0.91901\n",
      "\n",
      "Epoch 155, Training Loss: 0.11647, Training Accuracy: 0.95244\n",
      "Epoch 155, Test Loss: 0.23843, Test Accuracy: 0.91554\n",
      "\n",
      "Epoch 156, Training Loss: 0.11519, Training Accuracy: 0.95278\n",
      "Epoch 156, Test Loss: 0.25244, Test Accuracy: 0.91382\n",
      "\n",
      "Epoch 157, Training Loss: 0.11968, Training Accuracy: 0.95144\n",
      "Epoch 157, Test Loss: 0.24025, Test Accuracy: 0.91120\n",
      "\n",
      "Epoch 158, Training Loss: 0.12027, Training Accuracy: 0.95055\n",
      "Epoch 158, Test Loss: 0.22953, Test Accuracy: 0.91607\n",
      "\n",
      "Epoch 159, Training Loss: 0.11746, Training Accuracy: 0.95282\n",
      "Epoch 159, Test Loss: 0.23585, Test Accuracy: 0.92018\n",
      "\n",
      "Epoch 160, Training Loss: 0.11849, Training Accuracy: 0.95184\n",
      "Epoch 160, Test Loss: 0.23968, Test Accuracy: 0.91595\n",
      "\n",
      "Epoch 161, Training Loss: 0.11420, Training Accuracy: 0.95374\n",
      "Epoch 161, Test Loss: 0.24843, Test Accuracy: 0.91356\n",
      "\n",
      "Epoch 162, Training Loss: 0.11335, Training Accuracy: 0.95324\n",
      "Epoch 162, Test Loss: 0.26858, Test Accuracy: 0.90693\n",
      "\n",
      "Epoch 163, Training Loss: 0.12372, Training Accuracy: 0.95031\n",
      "Epoch 163, Test Loss: 0.25061, Test Accuracy: 0.91429\n",
      "\n",
      "Epoch 164, Training Loss: 0.12013, Training Accuracy: 0.95111\n",
      "Epoch 164, Test Loss: 0.31248, Test Accuracy: 0.88812\n",
      "\n",
      "Epoch 165, Training Loss: 0.11497, Training Accuracy: 0.95257\n",
      "Epoch 165, Test Loss: 0.24125, Test Accuracy: 0.92240\n",
      "\n",
      "Epoch 166, Training Loss: 0.12048, Training Accuracy: 0.95061\n",
      "Epoch 166, Test Loss: 0.26323, Test Accuracy: 0.90720\n",
      "\n",
      "Epoch 167, Training Loss: 0.11665, Training Accuracy: 0.95280\n",
      "Epoch 167, Test Loss: 0.24572, Test Accuracy: 0.91752\n",
      "\n",
      "Epoch 168, Training Loss: 0.11737, Training Accuracy: 0.95202\n",
      "Epoch 168, Test Loss: 0.24327, Test Accuracy: 0.91518\n",
      "\n",
      "Epoch 169, Training Loss: 0.11603, Training Accuracy: 0.95201\n",
      "Epoch 169, Test Loss: 0.23991, Test Accuracy: 0.91948\n",
      "\n",
      "Epoch 170, Training Loss: 0.11904, Training Accuracy: 0.95194\n",
      "Epoch 170, Test Loss: 0.23976, Test Accuracy: 0.91218\n",
      "\n",
      "Epoch 171, Training Loss: 0.11343, Training Accuracy: 0.95438\n",
      "Epoch 171, Test Loss: 0.23285, Test Accuracy: 0.92158\n",
      "\n",
      "Epoch 172, Training Loss: 0.11412, Training Accuracy: 0.95428\n",
      "Epoch 172, Test Loss: 0.24492, Test Accuracy: 0.91792\n",
      "\n",
      "Epoch 173, Training Loss: 0.11386, Training Accuracy: 0.95393\n",
      "Epoch 173, Test Loss: 0.25498, Test Accuracy: 0.90989\n",
      "\n",
      "Epoch 174, Training Loss: 0.11476, Training Accuracy: 0.95342\n",
      "Epoch 174, Test Loss: 0.29823, Test Accuracy: 0.89421\n",
      "\n",
      "Epoch 175, Training Loss: 0.11496, Training Accuracy: 0.95315\n",
      "Epoch 175, Test Loss: 0.23459, Test Accuracy: 0.92828\n",
      "\n",
      "Epoch 176, Training Loss: 0.11325, Training Accuracy: 0.95459\n",
      "Epoch 176, Test Loss: 0.24857, Test Accuracy: 0.91888\n",
      "\n",
      "Epoch 177, Training Loss: 0.11581, Training Accuracy: 0.95377\n",
      "Epoch 177, Test Loss: 0.26884, Test Accuracy: 0.90005\n",
      "\n",
      "Epoch 178, Training Loss: 0.11530, Training Accuracy: 0.95309\n",
      "Epoch 178, Test Loss: 0.37539, Test Accuracy: 0.87008\n",
      "\n",
      "Epoch 179, Training Loss: 0.11825, Training Accuracy: 0.95141\n",
      "Epoch 179, Test Loss: 0.33596, Test Accuracy: 0.87903\n",
      "\n",
      "Epoch 180, Training Loss: 0.11709, Training Accuracy: 0.95302\n",
      "Epoch 180, Test Loss: 0.23170, Test Accuracy: 0.92367\n",
      "\n",
      "Epoch 181, Training Loss: 0.11279, Training Accuracy: 0.95396\n",
      "Epoch 181, Test Loss: 0.23975, Test Accuracy: 0.92427\n",
      "\n",
      "Epoch 182, Training Loss: 0.11343, Training Accuracy: 0.95431\n",
      "Epoch 182, Test Loss: 0.29814, Test Accuracy: 0.89616\n",
      "\n",
      "Epoch 183, Training Loss: 0.11413, Training Accuracy: 0.95323\n",
      "Epoch 183, Test Loss: 0.23660, Test Accuracy: 0.91435\n",
      "\n",
      "Epoch 184, Training Loss: 0.11514, Training Accuracy: 0.95303\n",
      "Epoch 184, Test Loss: 0.30331, Test Accuracy: 0.89107\n",
      "\n",
      "Epoch 185, Training Loss: 0.11239, Training Accuracy: 0.95436\n",
      "Epoch 185, Test Loss: 0.27769, Test Accuracy: 0.90304\n",
      "\n",
      "Epoch 186, Training Loss: 0.11317, Training Accuracy: 0.95412\n",
      "Epoch 186, Test Loss: 0.24199, Test Accuracy: 0.92332\n",
      "\n",
      "Epoch 187, Training Loss: 0.11115, Training Accuracy: 0.95448\n",
      "Epoch 187, Test Loss: 0.26865, Test Accuracy: 0.90483\n",
      "\n",
      "Epoch 188, Training Loss: 0.11479, Training Accuracy: 0.95329\n",
      "Epoch 188, Test Loss: 0.24494, Test Accuracy: 0.91230\n",
      "\n",
      "Epoch 189, Training Loss: 0.11409, Training Accuracy: 0.95443\n",
      "Epoch 189, Test Loss: 0.25810, Test Accuracy: 0.90790\n",
      "\n",
      "Epoch 190, Training Loss: 0.11603, Training Accuracy: 0.95373\n",
      "Epoch 190, Test Loss: 0.31725, Test Accuracy: 0.88486\n",
      "\n",
      "Epoch 191, Training Loss: 0.11495, Training Accuracy: 0.95338\n",
      "Epoch 191, Test Loss: 0.23978, Test Accuracy: 0.91546\n",
      "\n",
      "Epoch 192, Training Loss: 0.11440, Training Accuracy: 0.95335\n",
      "Epoch 192, Test Loss: 0.24259, Test Accuracy: 0.91700\n",
      "\n",
      "Epoch 193, Training Loss: 0.11486, Training Accuracy: 0.95408\n",
      "Epoch 193, Test Loss: 0.30477, Test Accuracy: 0.89021\n",
      "\n",
      "Epoch 194, Training Loss: 0.11108, Training Accuracy: 0.95445\n",
      "Epoch 194, Test Loss: 0.26053, Test Accuracy: 0.91069\n",
      "\n",
      "Epoch 195, Training Loss: 0.11104, Training Accuracy: 0.95499\n",
      "Epoch 195, Test Loss: 0.24015, Test Accuracy: 0.91646\n",
      "\n",
      "Epoch 196, Training Loss: 0.11393, Training Accuracy: 0.95388\n",
      "Epoch 196, Test Loss: 0.23957, Test Accuracy: 0.91592\n",
      "\n",
      "Epoch 197, Training Loss: 0.11445, Training Accuracy: 0.95299\n",
      "Epoch 197, Test Loss: 0.22817, Test Accuracy: 0.92012\n",
      "\n",
      "Epoch 198, Training Loss: 0.11768, Training Accuracy: 0.95390\n",
      "Epoch 198, Test Loss: 0.22167, Test Accuracy: 0.92566\n",
      "\n",
      "Epoch 199, Training Loss: 0.11304, Training Accuracy: 0.95415\n",
      "Epoch 199, Test Loss: 0.29719, Test Accuracy: 0.88989\n",
      "\n",
      "Epoch 200, Training Loss: 0.10838, Training Accuracy: 0.95634\n",
      "Epoch 200, Test Loss: 0.25508, Test Accuracy: 0.91213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, labels in zip(X_train, y_train):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if len(labels.size()) == 0:\n",
    "            labels = labels.unsqueeze(0).expand(64, 1).float()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        predicted = outputs > 0.5  # Assuming threshold of 0.5 for binary classification\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.byte()).sum().item()\n",
    "    \n",
    "    # Print training loss and accuracy\n",
    "    train_loss = running_loss / len(y_train)\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:0.5f}, Training Accuracy: {train_accuracy:0.5f}\")\n",
    "    \n",
    "    # Test loss and accuracy calculation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in zip(X_test, y_test):\n",
    "            test_outputs = model(test_inputs)\n",
    "            if len(test_labels.size()) == 0:\n",
    "                test_labels = test_labels.unsqueeze(0).expand(64, 1).float()\n",
    "            test_loss += criterion(test_outputs, test_labels).item()\n",
    "            test_predicted = test_outputs > 0.5\n",
    "            test_total += test_labels.size(0)\n",
    "            test_correct += (test_predicted == test_labels.byte()).sum().item()\n",
    "    \n",
    "    # Print test loss and accuracy\n",
    "    test_loss /= len(y_test)\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"Epoch {epoch+1}, Test Loss: {test_loss:0.5f}, Test Accuracy: {test_accuracy:0.5f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nutzer\\AppData\\Local\\Temp\\ipykernel_4456\\2368509373.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "test_text = '''\n",
    "Do you have a problem that you are struggling to solve? Why don't you ask your friends for advice? When people ask for advice on solving a problem, often times they speak to more than one person. This is because different views are better for figuring out a tough problem, many opinions are better than one, and other people may have experienced a problem like yours and may be able to help you in making better decisions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Text Image Recognition\n",
    "\n",
    "Is there some similarity to Malware detection, where it's common to translate malware code to bytes and then images, which are then fed into NN/DL networks? Let's test a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988\n",
      "3992\n"
     ]
    }
   ],
   "source": [
    "ai = json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\4k\\\\generator_0_gpt2_4000_outputs.json\", encoding='utf8'))\n",
    "#ai = ai + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_1_daryl149_llama-2-7b-chat-hf_100_outputs.json\", encoding='utf8'))\n",
    "#ai = ai + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_1_gpt2_100_outputs.json\", encoding='utf8'))\n",
    "\n",
    "human = json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\4k\\\\generator_1_gpt2_4000_outputs.json\", encoding='utf8'))\n",
    "#human = human + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_0_daryl149_llama-2-7b-chat-hf_100_outputs.json\", encoding='utf8'))\n",
    "#human = human + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_0_gpt2_100_outputs.json\", encoding='utf8'))\n",
    "print(len(ai))\n",
    "print(len(human))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis as Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_image(detector_outputs):\n",
    "    for output in detector_outputs:\n",
    "        meta = output['metadata']\n",
    "        height = meta['sample_rate']\n",
    "        width = meta['sample_sequence_length']\n",
    "        results = output['ensemble_results'][0]['sample_results']\n",
    "        \n",
    "        image = Image.new('RGBA', (100, 20), color=(0,0,0,255))\n",
    "        pixels = image.load()\n",
    "\n",
    "        x = 1\n",
    "        offset_x = x\n",
    "        y = 1\n",
    "\n",
    "        for result in results:\n",
    "            steps = result['model_outputs']['steps']\n",
    "            steps.sort(key= lambda x: x['step'])\n",
    "\n",
    "            for step in steps:                \n",
    "                r = 255\n",
    "                g = 255\n",
    "                b = 255\n",
    "                \n",
    "                all_probs = step['top_k_probs']\n",
    "                all_probs.sort(reverse=True)\n",
    "                other_probs = all_probs[1:]\n",
    "                \n",
    "                # mid\n",
    "                pixels[x, y] = (r, g, b, int(step['target_prob'] * 255))\n",
    "                # right\n",
    "                pixels[x+1, y] = (r, g, b, int(other_probs[0] * 255))\n",
    "                # left\n",
    "                pixels[x-1, y] = (r, g, b, int(other_probs[1] * 255))\n",
    "                # down\n",
    "                pixels[x, y-1] = (r, g, b, int(other_probs[2] * 255))\n",
    "                # up\n",
    "                pixels[x, y+1] = (r, g, b, int(other_probs[3] * 255))\n",
    "                \n",
    "                x += 3\n",
    "            \n",
    "            offset_x = int((offset_x % 2 == 1))\n",
    "            x = offset_x\n",
    "            y += 3\n",
    "        display(image)\n",
    "\n",
    "def draw_image(width, height):\n",
    "    image = Image.new('RGBA', (width, height))\n",
    "    pixels = image.load()\n",
    "    # iterate over all pixels\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            r = random.randint(0, 255)\n",
    "            g = random.randint(0, 255)\n",
    "            b = random.randint(0, 255)\n",
    "            a = random.randint(0, 1)\n",
    "            pixels[x, y] = (r, g, b, int(a * 255))  # Set the pixel RGBA values\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAUCAYAAAB7wJiVAAAHgklEQVR4Ae2YS4gVdBTGZ3yUr3lP6mi+UiyfY/mAMgrBalFBkJi1iTZlSBupTbRzE2SLIIgeENG2hAyCoKIiKiKpFgVRGRFBkY9s1PLd73e75/Lp3Eu1a+GBb75vzj3/8z//1/GOXV1dXeewKXJTTws9I3R36Amh098b/kmhM+fk8GeejJ8YMchWbZeGznkzJv3zIz5zXhL+odBZ2/TwZ/1ZZ/ozZ9aZ8Vlby1/zyBM4jH7Es/AsYBGPwNeBhej74RXAwQdk4OF9BU8DbugheCIYRE+Fh4CFnoYvByvQu+BRYKHegOnADXofXg2M/7Lp9xKcQW8FLuCsDEbQjjVnH9LadwBjTsLWYP798BSwDO2aNgA363PY+pzrTvh2cAW6D14MpqLlZcCL5TyDwD2ZCy8FHtgTsPkH0BvhYVDr0m+eh+EHgXu1Bl4PXO/PcA9YjH4KXgmPM/yNDa8XkqebNyZPN2+2CWtsxltA+TM+debM+PRn/ryFnWLS37qFrDHnzbkyZ8Zn10h/js29yvxZQ/pzLS1/7ZOsUzv9NzV+Nk67+XtqT76dWXg76wlnzaPLDWhnmSfjvaFlZ0vALrqsvwTsJWhnOdZNLRsoAWdM1pn7c6ZDfM6bOnPG0PPmavl99i7+XXg2GEH7PJcDN9QnPwDmoH2OPnNfwZuwLcv2Mh++EvSid8PGz0Rvg32u5pkM9wE39x54JnBT3oZtg4vQa+EbgS2lC/Zpm38TbE1eju2wMGcv7FyXoYfgJcCb/CtsrV4g67FOD69anwdvPY6djX4Ath0Z/7E+MNyMsW3l2NQe2Ad8Pge4b7ZftTFn5KaueRuXDN8lTb8tehmx4wx/40D9oXai0m5O6XyqTtrOnzH5zDM+nzlpWnksuHKm7pQzYzzoGps5s4b0p/aA2o3tVGeOzRad/v+ka365cWqODssnmdpbWZbjXHSZN7rMV1Xmppbl88821XgZzaD+CoY9zLLUbmLZsRJw1umtL8safM3tLPNnq8n1Zs2nIomHU5ZtNuMzf38FJ9ezOojTZ+rEn4DrgQveCV4HP4IF4Cj4HlisL8kJ/gRuguPngt9AFWq+X4AFehBHwF5wFzgOjPMmmmsl0H4C14F94BB4CdzX1FCXG/sHeA9sB1+Bw2AecJ7amDPoyrkfbf2u43dwAswAY+ANcCvQ/ynYCBy7A7wCrGEVOAW+BXOB5rzGWY+XycM3xrUcBJeDQeD6tH2g4ofQm8Fn3d3d5jnPLmxZ3Xx6TmAmK+2EpTNmQvgnhfZmVLxFl0a2dOZPnXkyf+bJGjLnjMjvRtW8mcdNa+fPPB5sxeRc6c/4jMk6c10Z01v5ky1Sy2dlgjJvUllqJyxL7YRlp0vANU+4GtIbVZY647MeD6qsXkL9Xny2BOyNLMux5ZM71e9hlHnIZVmPl7TMDS7L/cx9y5xeinHmt6whvP4BNAJ60H4r8I8jN/cwPBG4eGP9NuVkflMw3qSn4NXAp7wTvgVY9KPwDcDF+K2jvjW9rw849ju4F7gpB2C/cZnHnBuBN9w/1laBFU19B9qNeAfuB9Z5FLamAfTj8NXAdU2HNwBrXgRfBRzrWtYBa3sVHgXGuNYpTb0XXgsG8bsfy8Ei9HF4CZiD3gUb41xPymAW2nmt39pOws5n/m9g1zsf7T7dDI8z/Od9s3Kj6qkaXNrkpb3Bpd3M0unP+E7PnKGtsakzp4db+bO2nCvHdorJNpj1pD9rzvweYtXgxpbOsRmfNWRtqQcrT7JJLrR88vm5m1SWMfmcXVBZjnUBZb7CMhdXlvHZatKfbS1bR7ajnGugksOZJ9tdjp0e8bnGXFfWkDEeQtlgCbhT/rGIaUmfkk/vOXgBcIP2wD45N9rn1QNcwJuwT9KFOc6248Sb4S3A9uI4x/SiV8CjQP8XsGOdawxeDYz5CLbteANfhhcAF39WBtZwAh4Azgs18gyjZ6IXgqXoUdgWZP22ymvBTLR1zgLWYL4+4CE9A9u++tCL5aa+Db4buImOmw2swbbjevS/B9sevaAfwoPAg3wMdu3z0JNg83tID8HWbys7BptzBP08fA08zvA3ktczdEPa6caG+BlmkorJJ5wxmSfjO8V0is+WlXnSny0u83voVaebWdoDKp15sjXlXJk/68y53PzKmfEZk/O26qlxssGaicqcsMwEZRXr7xmT7SXja9yFbOHtLHO2+1yfiyjrVHPmGatgONtLtqyTEZNjc65clwfYzjJPrjHbcu5V5m/l80l7msOw/7/Tg94NbwID6EXwKjAHfS+8FRj/AzwFWNyL8AgYRPs8twEP7yg8EVjQ0/B64MK6YZ+z+V+AbwLm/Bq2Bgs90tTeWJ/kGn9HvgYPAeupNuvi9+CzXVq/LbYPGP8WbHxjE+GpwDo3w1uAcxlvq/E1LYFHgXV+AddcY+jV4N+0WdvXCWIHHI8+Ds8Dw2jbVbVZW+A6fOMMf6Nv13PzlpTOZ2jx5c+Y9HsQFeNE/6QzT6ex6U+d+T3Qmit1xqTONps1dNL+O1L5M6ZTPdkGU+d+tuqs3HIlt9iL9j/YAU/4ov2PduAvZ1MHS0Sk+PsAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_to_image(human[:2])\n",
    "output_to_image(ai[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Let's try some CNN detection on the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D CNN\n",
    "\n",
    "Let's start by creating 2d arrays with x channels per \"pixel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(detector_outputs, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for output in detector_outputs:\n",
    "        results = output['ensemble_results'][0]['sample_results']\n",
    "        rows = []\n",
    "\n",
    "        for result in results:\n",
    "            steps = result['model_outputs']['steps']\n",
    "            steps.sort(key= lambda x: x['step'])\n",
    "            columns = []\n",
    "            for step in steps:\n",
    "                channels = [step['target_prob']]\n",
    "                top_k_probs = step['top_k_probs']\n",
    "                top_k_probs.sort()\n",
    "                channels = channels + top_k_probs\n",
    "                columns.append(channels)\n",
    "                \n",
    "            rows.append(columns)\n",
    "          \n",
    "        data.append(rows)\n",
    "        labels.append(label)  \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7980\n",
      "torch.Size([7980, 11, 6, 32])\n",
      "tensor([[[[5.5349e-02, 3.1169e-02, 6.8112e-02,  ..., 3.6298e-02,\n",
      "           2.9099e-02, 9.1484e-01],\n",
      "          [9.0897e-04, 5.1601e-01, 1.0902e-02,  ..., 1.0621e-02,\n",
      "           8.6073e-02, 5.9349e-03],\n",
      "          [4.2450e-02, 8.3047e-01, 6.8794e-02,  ..., 4.2377e-03,\n",
      "           1.8796e-02, 2.9923e-02],\n",
      "          [2.5172e-04, 1.7383e-02, 4.3808e-04,  ..., 9.9983e-01,\n",
      "           1.4946e-01, 4.2450e-02],\n",
      "          [2.2887e-01, 1.5550e-01, 4.9840e-02,  ..., 6.5941e-02,\n",
      "           2.3018e-02, 1.3634e-01],\n",
      "          [1.3649e-01, 3.6298e-02, 2.9099e-02,  ..., 5.3489e-01,\n",
      "           2.2832e-02, 6.4482e-03]],\n",
      "\n",
      "         [[2.0131e-04, 2.0213e-04, 4.0885e-06,  ..., 2.3299e-06,\n",
      "           1.5963e-05, 7.3132e-05],\n",
      "          [6.2795e-06, 4.0680e-04, 8.4170e-04,  ..., 2.6957e-04,\n",
      "           1.8569e-04, 2.4884e-05],\n",
      "          [2.6726e-04, 1.2189e-04, 5.4611e-04,  ..., 8.9511e-05,\n",
      "           4.2483e-04, 6.4260e-05],\n",
      "          [5.0063e-04, 1.8412e-05, 1.8213e-05,  ..., 2.4834e-07,\n",
      "           3.0573e-03, 2.6548e-04],\n",
      "          [8.3108e-04, 1.8533e-04, 1.2486e-04,  ..., 3.9392e-03,\n",
      "           3.0150e-05, 1.6634e-04],\n",
      "          [1.5713e-05, 3.5366e-06, 4.4838e-03,  ..., 1.2422e-04,\n",
      "           2.6455e-04, 6.6161e-06]],\n",
      "\n",
      "         [[2.4315e-04, 1.0646e-03, 4.8264e-06,  ..., 5.2813e-05,\n",
      "           1.9784e-05, 1.9409e-04],\n",
      "          [1.2228e-04, 9.8778e-04, 2.0166e-03,  ..., 9.1320e-04,\n",
      "           5.8938e-04, 1.4494e-04],\n",
      "          [1.1562e-03, 2.3481e-03, 1.0660e-03,  ..., 9.9302e-05,\n",
      "           8.1806e-04, 4.5541e-04],\n",
      "          [5.5261e-04, 1.9120e-04, 3.9142e-05,  ..., 6.1678e-07,\n",
      "           3.8000e-03, 3.5075e-03],\n",
      "          [1.4280e-03, 9.6364e-04, 2.8428e-04,  ..., 3.9403e-03,\n",
      "           1.0790e-02, 3.9055e-04],\n",
      "          [3.2787e-04, 6.6161e-03, 7.7209e-03,  ..., 3.3830e-04,\n",
      "           3.1332e-03, 4.6938e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.8481e-02, 2.2449e-02, 9.0326e-02,  ..., 2.2901e-02,\n",
      "           1.1457e-01, 5.2079e-03],\n",
      "          [2.1378e-02, 4.4589e-02, 2.0292e-02,  ..., 1.1767e-02,\n",
      "           7.5888e-02, 7.5693e-02],\n",
      "          [1.6230e-01, 3.0895e-02, 4.7296e-02,  ..., 4.8363e-02,\n",
      "           1.5499e-02, 3.6085e-02],\n",
      "          [5.9541e-02, 2.0764e-02, 8.0592e-03,  ..., 3.2432e-05,\n",
      "           1.6861e-01, 1.6230e-01],\n",
      "          [1.4282e-01, 3.8097e-02, 7.7992e-02,  ..., 6.5941e-02,\n",
      "           3.1969e-02, 4.4113e-02],\n",
      "          [5.6304e-02, 3.2675e-02, 1.1457e-01,  ..., 8.0323e-02,\n",
      "           1.4506e-01, 5.3363e-02]],\n",
      "\n",
      "         [[1.9348e-01, 4.8804e-02, 3.1154e-01,  ..., 3.6298e-02,\n",
      "           1.5405e-01, 5.3016e-02],\n",
      "          [2.4768e-02, 2.9746e-01, 2.1924e-02,  ..., 1.8381e-02,\n",
      "           8.6073e-02, 2.5399e-01],\n",
      "          [1.6964e-01, 5.2676e-02, 2.4599e-01,  ..., 7.3631e-02,\n",
      "           1.4703e-01, 4.9782e-02],\n",
      "          [7.1333e-02, 5.4646e-02, 1.7898e-02,  ..., 3.6169e-05,\n",
      "           1.7357e-01, 1.6964e-01],\n",
      "          [2.2887e-01, 1.4535e-01, 1.1367e-01,  ..., 8.9461e-02,\n",
      "           5.1572e-02, 1.3634e-01],\n",
      "          [1.3649e-01, 5.1544e-02, 1.5405e-01,  ..., 8.6616e-02,\n",
      "           1.8263e-01, 1.0133e-01]],\n",
      "\n",
      "         [[3.8229e-01, 1.6456e-01, 3.6387e-01,  ..., 4.5597e-01,\n",
      "           2.4096e-01, 9.1484e-01],\n",
      "          [7.9502e-01, 5.1601e-01, 1.8743e-01,  ..., 5.4807e-02,\n",
      "           7.6143e-01, 2.6364e-01],\n",
      "          [1.9982e-01, 8.3047e-01, 3.1198e-01,  ..., 1.4494e-01,\n",
      "           1.8373e-01, 5.0470e-02],\n",
      "          [1.5792e-01, 1.4919e-01, 9.4833e-01,  ..., 9.9983e-01,\n",
      "           3.2403e-01, 1.9982e-01],\n",
      "          [2.3730e-01, 1.5550e-01, 1.3261e-01,  ..., 5.6105e-01,\n",
      "           5.6253e-02, 3.5501e-01],\n",
      "          [5.7643e-01, 4.5597e-01, 2.4096e-01,  ..., 5.3489e-01,\n",
      "           1.9072e-01, 4.8774e-01]]]])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "data_ai, labels_ai = create_dataset(ai, 1)\n",
    "data_hu, labels_hu = create_dataset(human, 0)\n",
    "data = np.concatenate((data_ai, data_hu), axis=0)\n",
    "labels = np.concatenate((labels_hu, labels_ai))\n",
    "print(len(labels))\n",
    "\n",
    "# Convert numpy arrays to tensors\n",
    "# We need: [batch_size, channels, height, width]\n",
    "# data_tensor = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1).unsqueeze(-2)\n",
    "data_tensor = torch.tensor(data, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "print(data_tensor.shape)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "print(data_tensor[:1])\n",
    "print(labels_tensor[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape torch.Size([6384, 11, 6, 32])\n",
      "x_test shape torch.Size([1596, 11, 6, 32])\n",
      "y_train shape torch.Size([6384])\n",
      "y_test shape torch.Size([1596])\n",
      "tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
    "print(\"x_train shape\",X_train.shape)\n",
    "print(\"x_test shape\",X_test.shape)\n",
    "print(\"y_train shape\",y_train.shape)\n",
    "print(\"y_test shape\",y_test.shape)\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(11, 32, kernel_size=(2, 8), stride=(1, 1), padding=(0, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Dropout(p=0.25, inplace=False)\n",
      "  (4): Conv2d(32, 64, kernel_size=(2, 6), stride=(1, 1), padding=(0, 1))\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Dropout(p=0.25, inplace=False)\n",
      "  (8): Flatten(start_dim=1, end_dim=-1)\n",
      "  (9): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.4, inplace=False)\n",
      "  (12): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (13): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=11, out_channels=32, kernel_size=(2,8), padding=(0, 1)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(1,2), stride=(1, 2)),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2,6), padding=(0,1)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(1,2), stride=(1,2)),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=20, out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.4),\n",
    "\n",
    "    nn.Linear(in_features=64, out_features=1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x20 and 4x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_train, y_train):\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     14\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mE:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mE:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x20 and 4x64)"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, labels in zip(X_train, y_train):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if len(labels.size()) == 0:\n",
    "            labels = labels.unsqueeze(0).expand(64, 1).float()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        predicted = outputs > 0.5  # Assuming threshold of 0.5 for binary classification\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.byte()).sum().item()\n",
    "    \n",
    "    # Print training loss and accuracy\n",
    "    train_loss = running_loss / len(y_train)\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:0.5f}, Training Accuracy: {train_accuracy:0.5f}\")\n",
    "    \n",
    "    # Test loss and accuracy calculation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in zip(X_test, y_test):\n",
    "            test_outputs = model(test_inputs)\n",
    "            if len(test_labels.size()) == 0:\n",
    "                test_labels = test_labels.unsqueeze(0).expand(64, 1).float()\n",
    "            test_loss += criterion(test_outputs, test_labels).item()\n",
    "            test_predicted = test_outputs > 0.5\n",
    "            test_total += test_labels.size(0)\n",
    "            test_correct += (test_predicted == test_labels.byte()).sum().item()\n",
    "    \n",
    "    # Print test loss and accuracy\n",
    "    test_loss /= len(y_test)\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"Epoch {epoch+1}, Test Loss: {test_loss:0.5f}, Test Accuracy: {test_accuracy:0.5f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from detector import Detector\n",
    "import glob\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "test_text = '''\n",
    "Do you have a problem that you are struggling to solve? Why don't you ask your friends for advice? When people ask for advice on solving a problem, often times they speak to more than one person. This is because different views are better for figuring out a tough problem, many opinions are better than one, and other people may have experienced a problem like yours and may be able to help you in making better decisions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Text Image Recognition\n",
    "\n",
    "Is there some similarity to Malware detection, where it's common to translate malware code to bytes and then images, which are then fed into NN/DL networks? Let's test a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "ai = json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_1_mistralai_Mistral-7B-v0.1_100_outputs.json\", encoding='utf8'))\n",
    "ai = ai + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_1_daryl149_llama-2-7b-chat-hf_100_outputs.json\", encoding='utf8'))\n",
    "ai = ai + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_1_gpt2_100_outputs.json\", encoding='utf8'))\n",
    "\n",
    "human = json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_0_mistralai_Mistral-7B-v0.1_100_outputs.json\", encoding='utf8'))\n",
    "human = human + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_0_daryl149_llama-2-7b-chat-hf_100_outputs.json\", encoding='utf8'))\n",
    "human = human + json.load(open(\"E:\\\\Python_Projects\\\\PrismAI\\\\PrismAI\\\\src\\\\outputs\\\\generator_0_gpt2_100_outputs.json\", encoding='utf8'))\n",
    "print(len(ai))\n",
    "print(len(human))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vis as Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_image(detector_outputs):\n",
    "    for output in detector_outputs:\n",
    "        meta = output['metadata']\n",
    "        height = meta['sample_rate']\n",
    "        width = meta['sample_sequence_length']\n",
    "        results = output['ensemble_results'][0]['sample_results']\n",
    "        \n",
    "        image = Image.new('RGBA', (100, 20), color=(0,0,0,255))\n",
    "        pixels = image.load()\n",
    "\n",
    "        x = 1\n",
    "        offset_x = x\n",
    "        y = 1\n",
    "\n",
    "        for result in results:\n",
    "            steps = result['model_outputs']['steps']\n",
    "            steps.sort(key= lambda x: x['step'])\n",
    "\n",
    "            for step in steps:                \n",
    "                r = 255\n",
    "                g = 255\n",
    "                b = 255\n",
    "                \n",
    "                all_probs = step['top_k_probs']\n",
    "                all_probs.sort(reverse=True)\n",
    "                other_probs = all_probs[1:]\n",
    "                \n",
    "                # mid\n",
    "                pixels[x, y] = (r, g, b, int(step['target_prob'] * 255))\n",
    "                # right\n",
    "                pixels[x+1, y] = (r, g, b, int(other_probs[0] * 255))\n",
    "                # left\n",
    "                pixels[x-1, y] = (r, g, b, int(other_probs[1] * 255))\n",
    "                # down\n",
    "                pixels[x, y-1] = (r, g, b, int(other_probs[2] * 255))\n",
    "                # up\n",
    "                pixels[x, y+1] = (r, g, b, int(other_probs[3] * 255))\n",
    "                \n",
    "                x += 3\n",
    "            \n",
    "            offset_x = int((offset_x % 2 == 1))\n",
    "            x = offset_x\n",
    "            y += 3\n",
    "        display(image)\n",
    "\n",
    "def draw_image(width, height):\n",
    "    image = Image.new('RGBA', (width, height))\n",
    "    pixels = image.load()\n",
    "    # iterate over all pixels\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            r = random.randint(0, 255)\n",
    "            g = random.randint(0, 255)\n",
    "            b = random.randint(0, 255)\n",
    "            a = random.randint(0, 1)\n",
    "            pixels[x, y] = (r, g, b, int(a * 255))  # Set the pixel RGBA values\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAUCAYAAAB7wJiVAAAFdklEQVR4Ae2Xu29cVRDGN3acxIlfuwY7FoSNnZjEYCWQIuLlgAQUKCAalAKlAAp6GiQkJAqq0CJR0vBsqCLR8AekQxRUSCkQAiGeig0CG8fh97u5sxqt9sZrbK5SeKTP33fnzjlnzmv2utFoNG5gg3KpByr0WPIjO/F7ks79ZP9wihlN+mDSOT7rfSkG2Rk36xy/vyKmKj63zTGdcaO/OniAzTjAQM/BB4GTeR+eAS7iZ/AhMIlehmfBXrS7GDGfoMdK/zvwmVIbMwHmCbfd/aCJfgm+F8yg34QfAPZ/Cb4AXIgr8Glgbh/D9unCrcPmrP4LHgJ3ol+AbWvOq/A0ME/fOy/jzeeAQO6Bl4BjHYPngQfxKux8PShPw8/CtRvjejl6nrzsd4Jxi7I/63y7sr+qbVV8brtVXTXWVvvpxCOKedfBLojmiQgrTlM8JF5POktPWVhum/v0FIdNhIAtcZuZpXIzcxPCcp7j4ezinGdVDt6k2s3rP8WoXmGv7h1o6MY5UCQKHwFu3O9wXP8VtFd7BP+ncAu4cHfB7VJbHqbAUfzPwI+DE2h9D4GT6Cdh37lAlqBB4FjL8F4wjXb8xdK/BhuzgP8N+EVQLBx8CtjPZfg8mEU7mYXSb7lznub8PTwK3DDnsQRs+yNsfsZAN47DtZsD9/NDaGK9SpYb18tvPQ+/NyT0dsaim04/WbuY0X/29zNWVf7e/KLPOjlK1nVnUZoLGVZ1bccjAM5tk7uxmh5+TTqXlGbyDyWdy10uKS58WNYuXFjMyeecQ7yXW+mhKv8UUp+0ZDl5r69l5xDaq90EnmqvexsUk9cHnMy1UusfQo8AF+ISvB+4kfIwcKHP6gOe2OulnkZbQmaBZfN1eA5YLp6AjwFzM978/J34HN5Xats2gTlsyCBKzWTp/wN2jsb8BFsGD6NX4BOl3/5tm/ux/H6Jr5/fL0J31hi3OGDF9ezSLnIvv8n38ptZL3+OryoRua0bGv3ktjlmO7qfPj0ARQ51sguu5bJw03Pz70Z+SNpEw6KPeA6u6jO3ddJhuR9vUi/zBoTltuHr5tFuR49nN6eXVfl7xe6Yz+vs5NuwJcIkvoItQ/6oyZYFF3cZtgQZ848+YMzXsCXLBbIsWKrs85vS7yJ+i7bsWJosJ/cAS9Zr8AUwh34PNodJ9GqpLVlX0H69WT4sL35Z6be0toD5qJ2Lm2qelkf7mYOnSr+fTWPAuZinMep34VeB+T8Pj4NiI+Gj+Go3xi0ObXE9d3WnVLoRxZrUyVEmmukY5K+d5G54Ev+r5fKSfyizP/edc/DjIizr8N2KW7d6Wb6LNfAxf1VuZ759DNs7xGs+w6u/4YdLbUlpAxP9EF4ELtzPsNdc/4oMXLgfZGDMU7DlxZJ1GX4EWDregi+CNnoCPgOMsRweAZYOS8oc8EvvFXgcmNtv8ElgufsFtn8XK38dDeAT+i138TV1DX289L+c9AL6PjBM/Dl4BJxCuw6ngTkPwo5ZuzFusThRspxUaJPZaW19jj6rxnJzI8bfqdDILWk3erO2OYc8bscffdTBcV3zP1BOIMwTFGaCYS5SWPaHT46+1d6GsDxW7ieXi5zDWjSEi5tUPuc+/d8iLOdT9ZWY+8njrkcncM4huf9f6TX3i+WqDJzMn7D+cfQ6PA+MgTpfIGfRS8Cr/RE8A2y7IQNP2hfwg8ASJFsSW+g12PiJUrfRxecsfBgYPw23gJtqLpYgb5Z9PwaM128/d6NNrg2M/wBeBObQXWbtxzL7nQya6Edhy2B3mX0b30Xe126MWxwG//Srnfhm8W5QxFRpJxsx/ZSXHJ/LS9Y5ph+dc+sZj7PIsQ6OZExk126DFfCk79pttAL/AmhI4pVREippAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAUCAYAAAB7wJiVAAAFdklEQVR4Ae2YTYiVZRTHZ/xo/BidO45jatM4hFBSfgV+VTS4SIwIhIioINCidrqOoFq0qBZtom0tpA8IbFObdq2yoKUrJYiWUSg1Gopjv9/lnuHf9b0wM403oQ78+f/fc897nvM87/OceXRgYGDgBjYudzS0IL2qR/ym8C8PPRgaOTfWcOix0BnT6907Ij7raYV/Y+isZ134G/PX7/3gZXyMrQx0DH4BWNAVeKijz8JrwTL8g/CKjn9WBqP434WPdPkn8fulHwMrkWvg9WA1+jT8BHAhfoZHQAs9A0+CXeg/4GeA9dRYLvRT+LZ3/CfhU8D6X4P3AvNDN/aDB9DPwdPADferPrAW/S08Bdajne8EWIP+FD4AnO/lDiP7a4zbLmCxJ6RxV5HTj7XYnK7AQt7NHZ/1uOCVJ09R5l8RMemf0/V7P9idoFlUmZMrc3eXucuazJ3VZDPhbIVOWePry3EzJmvwNDTZbDj9AE12NZwbQvvRyoZK/Ftsy2ox+FuwR97J67sTePyfhp/3GW0vmAbtCcBbgadgJbwBuLi2hVUC/T1su9uCPg7bLlz08/Am4Id8qaMd6230KHCH/wkvB7a1j+FDYBL9KrwDGPMLPAzM+Tl8FIygz8HWtg39MPwgML5a32b0Lnz3gDG081I7R3McA8Zf77AhfTXGbU+qjrYTLO0HKu2HKG2Bpf0QpdO/UN0rj4uzFPmznl5/+HPuc/E1fj/YRdAspOx6CdjCyy6X6OLZeM7WF+6BbDvp92SUuehN5gcpy3rSX793c81Pf8bnfC/GSzmXcPdPLuN02DpsD3d3Tsp78MvAE/ElPAa8wnr0vYE4mQvwNuAkqxXYf0/gewX4YT6AXwT6B+F1wPhr8Aowij4DTwCvvOfhFqiYqm01PlucMR/B08CFfgR+FDjWRXhNx38afhZsxP8GvA+MoD+EncM42ta3p+P/Aj4MzPkbPCTQ78An4b4b497odQPxirjU7cIJVk4nXjr989F+uMW+6+I3vduYs2L7wXWkxxiszBNTNlOii1vxnG3KCTVZtohsX3nzSX/myPxVr79nu8t4LwJl+W755GxNWVvW3yt/5llybVvYTlZvF9NgP/px+BCwuFm5S9u+LuGzjbTQn3W0E7DteEOz138NHwRb0OOwebeh34R3A2POwjvAVrQ3IluH4/4A7wV+pB/h1cCFs561YAIN/eM268ezrVabfR/t8w783rIegvtujNueeB1hF7a0xTRpF6387sLSveLT78I2xWfOXvG9Wk3GZ5vtlTPje+m5dwlo19sPrhaQR/uKFXasfq/nJp5PTL6Xt7j0b46HjaEzPltNK2Ky/myzLmRZtqZsjy58mZuxLN8t3y1nW9YGR4FtNS7ucfgEsKX8BA8Di74KrwRObAb2puQELsDeoFyUc/AYML7a3Tj6Xnz3dfldFPNvBtvRR+FpYNuUF9M2Hdd6ai4LbZsHePcwME+7bcJ9N8Zvb4b2sezSFtbkt8il9o9EzoW2zfnUma3SjddUv5vkJn/5+sHVbrx+Nln93v1bKxxemctcmLK87WSejGlVMPx76Guhs72kduHKMmf55NF4yNbnv7HKsraM6ZWz3rslXLesJzkZ9wOLOAmfAu7Y1+G94C70ELwf5H9r29au6QP+Mf0OngJO+Ao8AbxGfwLbDpy87W452IXeBx8BLvRX8DSYQsv+5kk5Ax8E+nfD+q3TcQeB79qmRnxGX4KHwRT6OLwTuGn0rwK2R9uqLdH6T8PWbJ5v4D0d/yS8E1/fjXH/9n9ZFnDTsSUm/S54xbgIpTMm/akzJttI+lP7EZvyZ0yv/CM93vVf/fPOWbH94JoIY/1vt8MKZP+8Her5z9fwF2gx917V1VG6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAAAUCAYAAAB7wJiVAAAFD0lEQVR4Ae2YzWucVRTGY9LW1KbRyZh+JMRMSa2g1kpQFLVtCOIHooiK4qKIC0EE6R/gQuhCcOMfUFwquFFXgiBFtC5UFEFwKa0fWHEhiqW2aZP6+73MGQ6T921mTPsmggeePk/OnHvec+9775k7HRgYGLiIDcpt3Ux6Q9LITkyOz/6sx1P8lqRHks7xWV9VETOa/JuSzmOz7qXO4Yo8Q+Gvkwd5GZt54I/wKNiBfhq+Azjhz/UBF2hJBr6kJ2Hjsl99Ht8QmEAfhueBi/gh3AIN9MfwGHCxHoQfAjmPC/ETvu1dfse+i28SuIjWMgLMswD7XPO8Dz8OfPHH4T0g5zfeOp27dTbgZ7pipvAfwfcIXLvxXA9HZ/dvTDr7+9VVeapOXS/5O7u2vYBxqnsZWxWTc5bG4CyeUwe7W7rtQrej5G8nEZZzZP/5CIB9CWFV+T1JK9liCih2Ufq7THpKwnKd4ZNzzdm/Jtpjm4/z1VTxG75h4GRsMzcAJ/Me/Bhwcb+FZ8BO9Fvw7WAafQ98AJjzggxsiTfBt7b9Z+FNbX0G3gB2EQNdnAMu0Cl4DJhnUQY3o9+G94Kt6GdhazPmHBwt6xh6HviCn4CfA030p/D1wPg/4FEwiX4Fvqvtt5X5LGN2w9fCtRvPLTab//wbbfExNmsnspI/x/jSIz77V6N9uZGzqrYVW2jkqIPLjnG/Rzi3mjzWBQjL/plwwu70sKUQsN8/YbndldVrnBeQsPzc3OLic9lbX1huobvCuVY8yJFwAr/CHuEm2mO9Azh5W8pG4KK/A+8HLu438HbgkT8Ee5NxrG1qChgDFW3KxXLsTrAHvRneC1pog+4G1vA7bPsy3vZj3Da0Y22P3ohehQ8CF/Q72FptiU14RKC/gFvAnNGCGuh5fNuAc3ketmZj3oRvAzNo6zkAnPtx2DZZu/FcT3XnaPerr+lzbPGyHIO5INFSqp57uWJy/lxD9rsZltUTvjq4qgVYZK92ptfAdlxuI8VOKBnvji6zqno9Jf1YrsGLTNhCiLXiaFkef29C7sYl+RLaE/E9n3v8m+iD8BxwrC0rxi52aZ/lD7pf4C1+hj4H2xLzc21Tb+CbBn5/HIG3AhfOOhvA+LP6wAR6Ft4PvPW9Dj8KjDkFW6faemK+auu0HmsYBZ6ak/AkMP4o/AJcu/HcVbUsC152zFeZ05NwuXNW1bnub1n20TJzB5XZeJmzy5dbTb7hdIV1/uwlphOMyDex7Henh+UawifndpdvjHlsjr+iOo7wCXZ0C8zytFvg+4AT+BMeBramz+BpoP9reB+wXTwF2zKcwA/wFDDmNOxYF+sB+GEwhvbbPNrjJ+h7gWOjjRSLgu+lLr8xC/ii5mit0/htlbu74q0BV/Hj0f+jew3tD1dvYodk0EB/AI+BfWhzzgE32XWw42o3nmt36LQIJ17WLqr8fqeUxeecWVflyTH95sxj+9W5Hl/WsrmErw52F3WbixGWj3Px1uKDxH8nXdXWnHRYVZ7cdnLOGCfnPNnfb5vNeXI9p1PSsrVJH18ZGcffm8YQaPAYfxCNA9tL0a7QTuAkPNHWxns7suiX4cPAtvYV7Fjjo6W40D/j8yaj/y/YdpFjHHsM343AX+kfwdHKvkTfCRr4j8L3A8eegFtgFt1Pm/Xlveg4YJ5Ltlk+r92oq9gk/vNf1S5sWf39+l38ZXnCVwdHwRbyv62DFViTPrkO5r1uS/gH7Rj1tseeaHwAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=100x20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_to_image(human[:2])\n",
    "output_to_image(ai[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Let's try some CNN detection on the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D CNN\n",
    "\n",
    "Let's start by creating 2d arrays with x channels per \"pixel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(detector_outputs, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for output in detector_outputs:\n",
    "        results = output['ensemble_results'][0]['sample_results']\n",
    "        rows = []\n",
    "\n",
    "        for result in results:\n",
    "            steps = result['model_outputs']['steps']\n",
    "            steps.sort(key= lambda x: x['step'])\n",
    "            columns = []\n",
    "            for step in steps:\n",
    "                channels = [step['target_prob']]\n",
    "                top_k_probs = step['top_k_probs']\n",
    "                top_k_probs.sort()\n",
    "                channels = channels + top_k_probs\n",
    "                columns.append(channels)\n",
    "                \n",
    "            rows.append(columns)\n",
    "          \n",
    "        data.append(rows)\n",
    "        labels.append(label)  \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "torch.Size([600, 6, 6, 24])\n",
      "tensor([[[[3.9993e-01, 9.6910e-01, 7.8637e-01, 5.3396e-03, 9.9930e-01,\n",
      "           5.7553e-01, 9.9835e-01, 8.9255e-02, 5.3373e-02, 6.4691e-01,\n",
      "           8.9303e-01, 4.7557e-03, 3.0601e-02, 9.9994e-01, 3.3034e-01,\n",
      "           4.3163e-01, 5.0898e-01, 3.1150e-02, 4.1730e-01, 5.7956e-03,\n",
      "           7.9316e-01, 1.5728e-02, 6.6157e-01, 9.6784e-01],\n",
      "          [5.4341e-01, 2.0752e-02, 3.5485e-01, 8.5465e-02, 2.8040e-04,\n",
      "           6.5149e-02, 9.3025e-01, 8.1803e-01, 4.4634e-01, 8.6016e-01,\n",
      "           4.0067e-01, 5.4798e-01, 9.7516e-01, 6.6756e-01, 5.7694e-01,\n",
      "           9.2365e-01, 6.7627e-01, 8.3559e-01, 2.8993e-01, 9.9992e-01,\n",
      "           2.5412e-01, 7.1047e-02, 3.0996e-01, 9.9353e-01],\n",
      "          [3.8078e-02, 5.5823e-01, 7.6015e-01, 2.1506e-01, 8.6310e-01,\n",
      "           9.9965e-01, 4.0253e-01, 5.8665e-01, 2.4741e-02, 9.8571e-01,\n",
      "           7.1991e-01, 8.6504e-01, 9.8645e-01, 4.4676e-02, 6.5583e-01,\n",
      "           2.4257e-01, 9.4451e-01, 6.4040e-01, 5.0905e-01, 6.2726e-01,\n",
      "           5.2029e-01, 6.8852e-01, 9.3777e-01, 5.3832e-01],\n",
      "          [3.8229e-01, 3.2795e-01, 9.9991e-01, 9.9752e-01, 6.1855e-03,\n",
      "           6.7590e-01, 8.6318e-01, 7.3206e-01, 9.5381e-02, 5.2331e-01,\n",
      "           7.9163e-01, 2.5127e-01, 8.0654e-01, 8.8821e-01, 5.1949e-04,\n",
      "           3.8078e-02, 5.5823e-01, 7.6015e-01, 2.1506e-01, 8.6310e-01,\n",
      "           9.9965e-01, 4.0253e-01, 5.8665e-01, 2.4741e-02],\n",
      "          [3.5837e-02, 3.8212e-01, 8.9047e-01, 9.8228e-01, 9.6778e-01,\n",
      "           8.8801e-01, 8.3879e-01, 9.9900e-01, 1.5661e-01, 9.9985e-01,\n",
      "           8.8466e-01, 3.8229e-01, 3.2795e-01, 9.9991e-01, 9.9752e-01,\n",
      "           6.1855e-03, 6.7590e-01, 8.6318e-01, 7.3206e-01, 9.5381e-02,\n",
      "           5.2331e-01, 7.9163e-01, 2.5127e-01, 8.0654e-01],\n",
      "          [3.3034e-01, 4.3163e-01, 5.0898e-01, 3.1150e-02, 4.1730e-01,\n",
      "           5.7956e-03, 7.9316e-01, 1.5728e-02, 6.6157e-01, 9.6784e-01,\n",
      "           2.7264e-01, 7.1797e-01, 2.6531e-01, 7.0479e-01, 2.0538e-01,\n",
      "           9.8716e-01, 8.7171e-01, 9.4752e-01, 4.4815e-01, 9.4650e-01,\n",
      "           1.3998e-01, 6.5039e-01, 7.6493e-01, 1.2255e-01]],\n",
      "\n",
      "         [[1.0029e-02, 1.6113e-05, 3.4273e-04, 1.7559e-03, 6.3196e-07,\n",
      "           3.9789e-03, 1.6485e-06, 6.8966e-02, 3.1207e-03, 3.8880e-05,\n",
      "           8.2837e-05, 2.9479e-03, 2.9243e-03, 1.0863e-07, 6.6593e-05,\n",
      "           1.0016e-04, 1.3668e-02, 4.4007e-03, 5.7797e-02, 2.7177e-02,\n",
      "           8.7214e-06, 1.5728e-02, 2.8659e-03, 2.2778e-04],\n",
      "          [1.4920e-02, 1.3717e-02, 2.0121e-02, 5.7737e-02, 1.0341e-03,\n",
      "           8.5085e-03, 6.5795e-05, 2.9984e-03, 1.2404e-03, 8.2558e-04,\n",
      "           5.2817e-05, 1.1366e-02, 2.2442e-03, 6.4915e-03, 7.6379e-04,\n",
      "           4.9429e-05, 3.5646e-03, 2.4029e-04, 1.6591e-05, 7.1561e-10,\n",
      "           3.5132e-02, 6.5262e-02, 1.1947e-02, 5.5085e-05],\n",
      "          [2.4688e-02, 5.2610e-04, 4.3175e-02, 1.3359e-03, 3.0166e-04,\n",
      "           2.2197e-09, 4.3652e-04, 3.4209e-08, 2.5050e-03, 4.6042e-07,\n",
      "           3.0005e-02, 1.1039e-03, 1.3279e-05, 3.4030e-03, 6.3252e-04,\n",
      "           4.7105e-03, 7.1501e-04, 8.9109e-04, 2.5750e-04, 7.4139e-04,\n",
      "           1.0121e-02, 2.0039e-04, 4.6667e-03, 5.5485e-04],\n",
      "          [2.3259e-02, 3.6417e-02, 5.1651e-07, 5.1246e-05, 6.8538e-03,\n",
      "           1.9308e-02, 1.5436e-03, 5.7289e-04, 3.4678e-03, 1.2058e-02,\n",
      "           1.0719e-03, 4.2264e-03, 5.7918e-06, 1.4403e-02, 7.9950e-03,\n",
      "           2.9200e-03, 8.8658e-06, 4.3175e-02, 4.1980e-03, 2.5975e-04,\n",
      "           2.4904e-07, 3.6039e-03, 6.9336e-04, 4.5206e-03],\n",
      "          [1.6605e-03, 8.9594e-04, 6.3713e-03, 4.3418e-11, 1.7455e-03,\n",
      "           2.0005e-03, 5.0315e-04, 1.0692e-05, 5.2819e-04, 1.1120e-07,\n",
      "           2.0088e-05, 6.2017e-02, 3.6417e-02, 3.3042e-10, 4.2455e-06,\n",
      "           6.8538e-03, 7.2829e-03, 4.0935e-08, 1.2351e-05, 1.2945e-02,\n",
      "           1.0626e-02, 1.0719e-03, 2.2620e-02, 9.9151e-05],\n",
      "          [1.5636e-02, 1.0016e-04, 3.2702e-03, 1.7416e-03, 5.7797e-02,\n",
      "           5.7956e-03, 9.9254e-04, 5.4187e-03, 7.5144e-03, 4.3475e-04,\n",
      "           1.4639e-02, 5.6291e-05, 2.0422e-03, 9.2760e-03, 3.0751e-04,\n",
      "           1.2901e-03, 4.2194e-03, 1.4757e-05, 6.2065e-02, 5.9574e-04,\n",
      "           4.8393e-02, 9.2490e-06, 9.9334e-04, 4.6279e-03]],\n",
      "\n",
      "         [[8.4998e-02, 6.1464e-04, 1.3820e-03, 3.1750e-03, 2.0903e-06,\n",
      "           3.4741e-02, 2.2924e-06, 8.9255e-02, 1.2861e-02, 5.7780e-03,\n",
      "           1.3608e-04, 4.9409e-02, 7.5997e-02, 7.5408e-07, 6.4244e-03,\n",
      "           2.3637e-04, 2.5649e-02, 3.1719e-02, 6.4997e-02, 7.8452e-02,\n",
      "           1.3024e-02, 5.3797e-02, 7.5144e-03, 2.6921e-04],\n",
      "          [2.3891e-02, 1.5396e-02, 2.1384e-02, 8.5465e-02, 2.0535e-03,\n",
      "           4.4462e-02, 1.0725e-03, 2.7170e-02, 6.4582e-02, 2.0484e-03,\n",
      "           8.3847e-03, 1.5639e-02, 2.9116e-03, 2.1072e-02, 1.0428e-01,\n",
      "           7.7983e-04, 6.6803e-03, 3.7527e-03, 5.8341e-02, 4.7335e-08,\n",
      "           7.2628e-02, 7.1047e-02, 2.8925e-02, 5.2261e-04],\n",
      "          [2.6267e-02, 5.9775e-04, 5.1080e-02, 1.7621e-02, 1.1650e-03,\n",
      "           1.2411e-05, 1.9500e-02, 6.9336e-04, 2.0527e-02, 1.9016e-06,\n",
      "           3.5420e-02, 4.5886e-03, 1.4545e-03, 8.5068e-03, 3.3637e-02,\n",
      "           5.7662e-03, 3.4330e-03, 1.7359e-03, 9.7204e-03, 5.6213e-03,\n",
      "           4.4699e-02, 2.5330e-04, 7.0726e-03, 3.1747e-03],\n",
      "          [9.9633e-02, 4.0735e-02, 9.9567e-07, 5.3736e-05, 6.6069e-02,\n",
      "           1.9473e-02, 6.6679e-03, 2.7883e-03, 1.2945e-02, 2.0226e-02,\n",
      "           5.4061e-03, 2.2620e-02, 3.7815e-04, 1.8195e-02, 1.0239e-02,\n",
      "           4.0188e-02, 3.3008e-03, 5.1080e-02, 1.7621e-02, 1.2825e-03,\n",
      "           3.8986e-07, 1.3714e-02, 9.8147e-03, 3.9116e-02],\n",
      "          [3.5837e-02, 2.8954e-03, 9.2031e-03, 3.4071e-05, 4.5370e-03,\n",
      "           2.5065e-02, 3.7353e-03, 1.8376e-05, 1.6961e-03, 7.2141e-06,\n",
      "           1.8248e-03, 9.9633e-02, 8.1698e-02, 1.7812e-07, 3.1432e-05,\n",
      "           6.6069e-02, 1.9473e-02, 2.2945e-03, 9.1462e-04, 1.3909e-02,\n",
      "           2.0226e-02, 4.8682e-03, 3.8698e-02, 3.7815e-04],\n",
      "          [3.1021e-02, 2.3637e-04, 1.3668e-02, 3.8044e-03, 6.4997e-02,\n",
      "           2.3706e-02, 7.6289e-03, 5.3797e-02, 9.6738e-03, 5.1479e-03,\n",
      "           6.5323e-02, 9.5456e-03, 1.5817e-02, 2.3520e-02, 2.2266e-02,\n",
      "           1.8873e-03, 1.0099e-02, 1.9316e-03, 7.0535e-02, 6.5583e-04,\n",
      "           6.3860e-02, 8.6795e-03, 9.5534e-03, 4.7528e-03]],\n",
      "\n",
      "         [[1.0140e-01, 7.8838e-04, 2.0938e-03, 5.6498e-03, 1.6607e-05,\n",
      "           6.3892e-02, 1.3216e-05, 1.1380e-01, 1.6234e-02, 3.4019e-02,\n",
      "           4.7886e-02, 6.1887e-02, 1.7699e-01, 7.6718e-07, 3.1021e-02,\n",
      "           2.6384e-04, 1.0185e-01, 3.4028e-02, 1.7031e-01, 8.1574e-02,\n",
      "           3.9231e-02, 5.3911e-02, 3.5929e-02, 5.1479e-03],\n",
      "          [5.9157e-02, 1.9356e-02, 3.4788e-02, 8.9402e-02, 5.1709e-02,\n",
      "           6.5149e-02, 5.3589e-03, 2.9575e-02, 9.5800e-02, 9.5340e-03,\n",
      "           3.7909e-02, 3.1434e-02, 3.0222e-03, 4.8587e-02, 1.1749e-01,\n",
      "           1.6866e-03, 1.1056e-02, 5.5617e-02, 1.1906e-01, 3.2281e-07,\n",
      "           9.1126e-02, 7.1657e-02, 8.6349e-02, 5.4813e-04],\n",
      "          [3.8078e-02, 1.1688e-02, 5.4333e-02, 8.4134e-02, 8.3877e-03,\n",
      "           1.5365e-05, 2.4504e-02, 1.2734e-02, 2.4741e-02, 1.9532e-05,\n",
      "           6.3666e-02, 4.3583e-02, 2.1960e-03, 3.5318e-02, 4.5047e-02,\n",
      "           7.7058e-03, 1.1791e-02, 4.8166e-03, 7.6251e-02, 5.5979e-02,\n",
      "           8.1070e-02, 5.7558e-02, 7.3125e-03, 2.6735e-02],\n",
      "          [1.3770e-01, 2.0937e-01, 8.6835e-06, 7.5494e-05, 8.4710e-02,\n",
      "           6.1371e-02, 3.3627e-02, 2.8389e-02, 9.5381e-02, 1.2110e-01,\n",
      "           2.8962e-02, 1.0162e-01, 2.4920e-03, 1.9108e-02, 1.0329e-02,\n",
      "           1.0929e-01, 1.1688e-02, 5.4333e-02, 8.4134e-02, 8.3877e-03,\n",
      "           4.2685e-05, 4.9731e-02, 1.2734e-02, 1.5803e-01],\n",
      "          [6.9290e-02, 5.1628e-03, 3.0040e-02, 4.1754e-05, 9.4413e-03,\n",
      "           3.4173e-02, 1.2161e-02, 3.2356e-05, 8.0861e-02, 3.7335e-05,\n",
      "           1.8064e-02, 1.3770e-01, 2.0937e-01, 8.6835e-06, 3.8120e-05,\n",
      "           8.4710e-02, 6.1436e-02, 2.9226e-03, 4.2467e-03, 9.5381e-02,\n",
      "           1.2110e-01, 2.8962e-02, 1.0162e-01, 2.4920e-03],\n",
      "          [1.3940e-01, 3.6771e-04, 5.7825e-02, 1.0330e-01, 1.7031e-01,\n",
      "           2.9597e-02, 3.9231e-02, 5.3911e-02, 3.5929e-02, 6.7438e-03,\n",
      "           1.7815e-01, 1.9274e-02, 1.2348e-01, 3.2523e-02, 1.2464e-01,\n",
      "           2.3837e-03, 1.7398e-02, 3.1727e-03, 1.2345e-01, 2.2553e-02,\n",
      "           1.3998e-01, 2.7812e-02, 1.8547e-02, 2.8571e-02]],\n",
      "\n",
      "         [[3.6695e-01, 2.6761e-02, 4.1650e-02, 1.5083e-02, 6.1061e-04,\n",
      "           2.7422e-01, 1.5080e-03, 2.6942e-01, 5.3373e-02, 2.9039e-01,\n",
      "           5.7590e-02, 1.1510e-01, 3.3757e-01, 2.6865e-06, 1.3940e-01,\n",
      "           4.3163e-01, 1.8866e-01, 2.0805e-01, 2.4548e-01, 1.7191e-01,\n",
      "           1.2328e-01, 1.0714e-01, 2.1255e-01, 6.7438e-03],\n",
      "          [1.9108e-01, 8.7117e-02, 3.5485e-01, 2.4481e-01, 9.8523e-02,\n",
      "           6.9292e-02, 2.9226e-02, 4.1991e-02, 1.7077e-01, 8.4934e-02,\n",
      "           8.4885e-02, 6.5350e-02, 9.3871e-03, 2.2144e-01, 1.5998e-01,\n",
      "           4.4869e-02, 2.7856e-01, 1.0076e-01, 1.9533e-01, 7.4334e-07,\n",
      "           1.3093e-01, 1.3003e-01, 3.0996e-01, 3.3719e-03],\n",
      "          [1.1247e-01, 3.9942e-01, 7.1306e-02, 2.1506e-01, 1.1896e-01,\n",
      "           1.9138e-04, 4.0253e-01, 3.8522e-01, 4.7590e-02, 1.4262e-02,\n",
      "           1.0851e-01, 5.8895e-02, 4.2471e-03, 8.5455e-02, 2.4606e-01,\n",
      "           2.4257e-01, 3.9120e-02, 3.4810e-01, 2.0662e-01, 1.9415e-01,\n",
      "           1.8615e-01, 2.5292e-01, 2.6913e-02, 4.3107e-01],\n",
      "          [2.4492e-01, 2.1981e-01, 8.0148e-05, 1.9610e-03, 8.5495e-02,\n",
      "           8.3426e-02, 8.6522e-02, 2.1672e-01, 2.8315e-01, 1.8201e-01,\n",
      "           1.5719e-01, 2.5127e-01, 1.8956e-01, 3.2852e-02, 4.0520e-02,\n",
      "           1.1247e-01, 3.9942e-01, 7.1306e-02, 2.1506e-01, 1.1896e-01,\n",
      "           1.9138e-04, 4.0253e-01, 3.8522e-01, 2.2581e-01],\n",
      "          [8.8918e-02, 3.8212e-01, 4.4532e-02, 1.7588e-02, 1.2136e-02,\n",
      "           4.9522e-02, 7.0928e-02, 1.7564e-04, 1.5661e-01, 4.6304e-05,\n",
      "           9.4108e-02, 2.4492e-01, 2.1981e-01, 8.0148e-05, 1.9610e-03,\n",
      "           8.5495e-02, 8.3426e-02, 8.6522e-02, 2.1672e-01, 2.8315e-01,\n",
      "           1.8201e-01, 1.5719e-01, 2.5127e-01, 1.8956e-01],\n",
      "          [1.6754e-01, 4.3163e-01, 1.8866e-01, 2.0805e-01, 2.4548e-01,\n",
      "           1.7191e-01, 1.2328e-01, 1.0714e-01, 2.1255e-01, 8.6859e-03,\n",
      "           2.7264e-01, 2.4368e-01, 2.0715e-01, 2.1873e-01, 2.0538e-01,\n",
      "           3.1084e-03, 5.6700e-02, 4.6935e-02, 1.7521e-01, 2.9039e-02,\n",
      "           2.2015e-01, 1.2376e-01, 1.8674e-01, 1.7384e-01]],\n",
      "\n",
      "         [[3.9993e-01, 9.6910e-01, 7.8637e-01, 1.9488e-01, 9.9930e-01,\n",
      "           5.7553e-01, 9.9835e-01, 3.7177e-01, 8.8743e-01, 6.4691e-01,\n",
      "           8.9303e-01, 7.0992e-01, 3.6588e-01, 9.9994e-01, 1.8825e-01,\n",
      "           5.6720e-01, 5.0898e-01, 4.0067e-01, 4.1730e-01, 4.7076e-01,\n",
      "           7.9316e-01, 6.1327e-01, 6.6157e-01, 9.6784e-01],\n",
      "          [5.4341e-01, 1.8897e-01, 4.8502e-01, 3.2644e-01, 8.0940e-01,\n",
      "           2.1946e-01, 9.3025e-01, 8.1803e-01, 4.4634e-01, 8.6016e-01,\n",
      "           2.6251e-01, 5.4798e-01, 9.7516e-01, 6.6756e-01, 5.7694e-01,\n",
      "           9.2365e-01, 6.7627e-01, 8.3559e-01, 2.8993e-01, 9.9992e-01,\n",
      "           2.5412e-01, 4.6056e-01, 4.8563e-01, 9.9353e-01],\n",
      "          [2.2098e-01, 5.5823e-01, 7.6015e-01, 4.7400e-01, 8.6310e-01,\n",
      "           9.9965e-01, 4.1136e-01, 5.8665e-01, 2.2581e-01, 9.8571e-01,\n",
      "           7.1991e-01, 8.6504e-01, 9.8645e-01, 7.3849e-01, 6.5583e-01,\n",
      "           5.4175e-01, 9.4451e-01, 6.4040e-01, 5.0905e-01, 6.2726e-01,\n",
      "           5.2029e-01, 6.8852e-01, 9.3777e-01, 5.3832e-01],\n",
      "          [3.8229e-01, 3.2795e-01, 9.9991e-01, 9.9752e-01, 7.3260e-01,\n",
      "           6.7590e-01, 8.6318e-01, 7.3206e-01, 5.6352e-01, 5.2331e-01,\n",
      "           7.9163e-01, 5.4479e-01, 8.0654e-01, 8.8821e-01, 8.4385e-01,\n",
      "           2.2098e-01, 5.5823e-01, 7.6015e-01, 4.7400e-01, 8.6310e-01,\n",
      "           9.9965e-01, 4.1136e-01, 5.8665e-01, 2.2987e-01],\n",
      "          [7.3323e-01, 6.0881e-01, 8.9047e-01, 9.8228e-01, 9.6778e-01,\n",
      "           8.8801e-01, 8.3879e-01, 9.9900e-01, 3.3016e-01, 9.9985e-01,\n",
      "           8.8466e-01, 3.8229e-01, 3.2795e-01, 9.9991e-01, 9.9752e-01,\n",
      "           7.3260e-01, 6.7590e-01, 8.6318e-01, 7.3206e-01, 5.6352e-01,\n",
      "           5.2331e-01, 7.9163e-01, 5.4479e-01, 8.0654e-01],\n",
      "          [3.3034e-01, 5.6720e-01, 5.0898e-01, 4.0067e-01, 4.1730e-01,\n",
      "           4.7076e-01, 7.9316e-01, 6.1327e-01, 6.6157e-01, 9.6784e-01,\n",
      "           3.0379e-01, 7.1797e-01, 2.6531e-01, 7.0479e-01, 5.7722e-01,\n",
      "           9.8716e-01, 8.7171e-01, 9.4752e-01, 4.4815e-01, 9.4650e-01,\n",
      "           3.3745e-01, 6.5039e-01, 7.6493e-01, 3.6125e-01]]]])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "data_ai, labels_ai = create_dataset(ai, 1)\n",
    "data_hu, labels_hu = create_dataset(human, 0)\n",
    "data = np.concatenate((data_ai, data_hu), axis=0)\n",
    "labels = np.concatenate((labels_hu, labels_ai))\n",
    "print(len(labels))\n",
    "\n",
    "# Convert numpy arrays to tensors\n",
    "# We need: [batch_size, channels, height, width]\n",
    "# data_tensor = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1).unsqueeze(-2)\n",
    "data_tensor = torch.tensor(data, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "print(data_tensor.shape)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "print(data_tensor[:1])\n",
    "print(labels_tensor[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape torch.Size([480, 6, 6, 24])\n",
      "x_test shape torch.Size([120, 6, 6, 24])\n",
      "y_train shape torch.Size([480])\n",
      "y_test shape torch.Size([120])\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
    "print(\"x_train shape\",X_train.shape)\n",
    "print(\"x_test shape\",X_test.shape)\n",
    "print(\"y_train shape\",y_train.shape)\n",
    "print(\"y_test shape\",y_test.shape)\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(6, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Dropout(p=0.25, inplace=False)\n",
      "  (4): Conv2d(16, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Dropout(p=0.25, inplace=False)\n",
      "  (8): Flatten(start_dim=1, end_dim=-1)\n",
      "  (9): Linear(in_features=36, out_features=64, bias=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.4, inplace=False)\n",
      "  (12): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (13): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(1,3), padding=(0, 1)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(1,2), stride=(1, 2)),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(1,3), padding=(0,1)),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=(1,2), stride=(1,2)),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=36, out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.4),\n",
    "\n",
    "    nn.Linear(in_features=32, out_features=1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     14\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mE:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Python_Projects\\PrismAI\\env\\lib\\site-packages\\torch\\nn\\functional.py:3113\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3111\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3116\u001b[0m     )\n\u001b[0;32m   3118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3119\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 75\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, labels in zip(X_train, y_train):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if len(labels.size()) == 0:\n",
    "            labels = labels.unsqueeze(0).expand(32, 1).float()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        predicted = outputs > 0.5  # Assuming threshold of 0.5 for binary classification\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.byte()).sum().item()\n",
    "    \n",
    "    # Print training loss and accuracy\n",
    "    train_loss = running_loss / len(y_train)\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:0.5f}, Training Accuracy: {train_accuracy:0.5f}\")\n",
    "    \n",
    "    # Test loss and accuracy calculation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in zip(X_test, y_test):\n",
    "            test_outputs = model(test_inputs)\n",
    "            if len(test_labels.size()) == 0:\n",
    "                test_labels = test_labels.unsqueeze(0).expand(32, 1).float()\n",
    "            test_loss += criterion(test_outputs, test_labels).item()\n",
    "            test_predicted = test_outputs > 0.5\n",
    "            test_total += test_labels.size(0)\n",
    "            test_correct += (test_predicted == test_labels.byte()).sum().item()\n",
    "    \n",
    "    # Print test loss and accuracy\n",
    "    test_loss /= len(y_test)\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"Epoch {epoch+1}, Test Loss: {test_loss:0.5f}, Test Accuracy: {test_accuracy:0.5f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a39fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26864b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from numpy.typing import NDArray\n",
    "from torch import Tensor, nn\n",
    "from tqdm import tqdm\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments\n",
    "from ulid import ulid\n",
    "\n",
    "from luminar.utils.training import LuminarTrainingConfig\n",
    "from luminar.classifier import LuminarCNN\n",
    "from luminar.utils import (\n",
    "    ConvolutionalLayerSpec,\n",
    "    compute_metrics,\n",
    "    get_matched_datasets,\n",
    "    get_pad_to_fixed_length_fn,\n",
    "    save_model,\n",
    ")\n",
    "\n",
    "HF_TOKEN = (Path.home() / \".hf_token\").read_text().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065414a",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf68ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from luminar.encoder import LuminarEncoder\n",
    "\n",
    "\n",
    "# encoder = LuminarEncoder()\n",
    "# encoder.device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7e778",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbf35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = \"gpt_4o_mini\"\n",
    "other_agents = \"gemma2_9b\"\n",
    "domain = \"blog_authorship_corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_len = 128\n",
    "seed = 42\n",
    "\n",
    "pad_to_fixed_length: Callable[[NDArray], NDArray] = get_pad_to_fixed_length_fn(\n",
    "    feature_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c0be7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['agent', 'id_sample', 'id_source', 'labels', 'length', 'features'],\n",
       "    num_rows: 37864\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datset_config_name = f\"{domain}-fulltext\"\n",
    "dataset_split_name = f\"human+{agent}+{other_agents}\"\n",
    "dataset: Dataset = (\n",
    "    load_dataset(\n",
    "        \"liberi-luminaris/PrismAI-encoded-gpt2\",\n",
    "        datset_config_name,\n",
    "        split=dataset_split_name,\n",
    "        token=HF_TOKEN,\n",
    "    )  # type: ignore\n",
    "    .rename_column(\"label\", \"labels\")\n",
    "    .filter(\n",
    "        lambda features: len(features) > 0,\n",
    "        input_columns=[\"features\"],\n",
    "        num_proc=8,\n",
    "    )\n",
    "    .with_format(\"numpy\", columns=[\"features\"])\n",
    "    .map(\n",
    "        lambda features: {\"features\": pad_to_fixed_length(features)},\n",
    "        input_columns=[\"features\"],\n",
    "        desc=\"Trimming & Padding Features\",\n",
    "        num_proc=8,\n",
    "    )\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe055ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_matched, dataset_unmatched = get_matched_datasets(dataset, agent)\n",
    "datasets_matched.set_format(\"torch\", columns=[\"labels\", \"features\"])\n",
    "dataset_unmatched.set_format(\"torch\", columns=[\"labels\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fe23e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ulid = ulid()\n",
    "config = LuminarTrainingConfig(\n",
    "    feature_len=feature_len,\n",
    "    feature_dim=(feature_len, 13),\n",
    "    feature_type=\"intermediate_likelihoods\",\n",
    "    feature_model=\"gpt2\",\n",
    "    feature_selection=\"first\",\n",
    "    agent=agent,\n",
    "    domain=domain,\n",
    "    other_agents=other_agents,\n",
    "    datset_config_name=other_agents,\n",
    "    dataset_split_name=\"+\".join((\"human\", agent, *other_agents)),\n",
    "    #\n",
    "    projection_dim=None,\n",
    "    conv_layer_shapes=(\n",
    "        ConvolutionalLayerSpec(32, 7),\n",
    "        ConvolutionalLayerSpec(64, 5),\n",
    "        ConvolutionalLayerSpec(128, 3),\n",
    "    ),\n",
    "    feed_forward_dim=(512, 32),\n",
    "    #\n",
    "    seed=seed,\n",
    "    run_ulid=run_ulid,\n",
    ")\n",
    "\n",
    "training_args = config.training_arguments(\n",
    "    len(datasets_matched[\"train\"]),\n",
    "    output_dir=\"../logs/hf/\" + run_ulid,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=1024,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-4,\n",
    "    # lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "    # lr_scheduler_kwargs={\n",
    "    #     \"mode\": \"min\",\n",
    "    #     \"factor\": 0.5,\n",
    "    #     \"patience\": 3,\n",
    "    #     \"threshold\": 0.001,\n",
    "    #     \"threshold_mode\": \"rel\",\n",
    "    #     # \"cooldown\": 1,\n",
    "    #     \"min_lr\": 1e-6,\n",
    "    # },\n",
    "    # metric_for_best_model=\"accuracy\",\n",
    "    # greater_is_better=True,\n",
    "    # torch_compile=True,\n",
    "    # torch_compile_mode=\"reduce-overhead\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7405313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LuminarCNN(\n",
      "  (rescale): FeatureRescaler()\n",
      "  (projection): Identity()\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv1d(13, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (classifier): Linear(in_features=4096, out_features=1, bias=True)\n",
      "  (criterion): BCEWithLogitsLoss()\n",
      ")\n",
      "Parameters \n",
      "  cnn: 37952 \n",
      "  classifier: 4097 \n",
      "  total: 124513\n"
     ]
    }
   ],
   "source": [
    "classifier = LuminarCNN(**config.asdict())\n",
    "classifier.forward(\n",
    "    torch.randn(32, feature_len, 13),\n",
    ")\n",
    "\n",
    "print(classifier)\n",
    "print(\n",
    "    \"Parameters\",\n",
    "    \"\\n  cnn:\",\n",
    "    sum(\n",
    "        param.numel()\n",
    "        for param in classifier.cnn.parameters()\n",
    "        if param.requires_grad\n",
    "    ),\n",
    "    \"\\n  classifier:\",\n",
    "    sum(\n",
    "        param.numel()\n",
    "        for param in classifier.classifier.parameters()\n",
    "        if param.requires_grad\n",
    "    ),\n",
    "    \"\\n  total:\",\n",
    "    sum(param.numel() for param in classifier.parameters() if param.requires_grad),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "493a1f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_layer_spec = (\n",
    "#     ConvolutionalLayerSpec(8, (7, 1)),\n",
    "#     ConvolutionalLayerSpec(16, (7, 1)),\n",
    "#     ConvolutionalLayerSpec(32, 7),\n",
    "#     ConvolutionalLayerSpec(64, 5),\n",
    "#     ConvolutionalLayerSpec(32, 3),\n",
    "#     ConvolutionalLayerSpec(3, 3),\n",
    "# )\n",
    "\n",
    "# classifier = LuminarCNN2D(conv_layer_shapes=conv_layer_spec, **config)\n",
    "# print(classifier)\n",
    "# print(\"num. parameters:\", sum(1 for p in classifier.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc46492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets_matched[\"train\"],\n",
    "    eval_dataset=datasets_matched[\"eval\"],\n",
    "    # data_collator=PaddingDataCollator(config[\"feature_dim\"]),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(10)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06616066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2010' max='2010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2010/2010 05:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>F1 Human</th>\n",
       "      <th>F1 Ai</th>\n",
       "      <th>N Samples</th>\n",
       "      <th>F1 Weighted Median</th>\n",
       "      <th>Accuracy Median</th>\n",
       "      <th>Roc Auc Median</th>\n",
       "      <th>F1 Human Median</th>\n",
       "      <th>F1 Ai Median</th>\n",
       "      <th>Threshold Median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.678644</td>\n",
       "      <td>0.478364</td>\n",
       "      <td>0.545952</td>\n",
       "      <td>0.644762</td>\n",
       "      <td>0.290598</td>\n",
       "      <td>0.666130</td>\n",
       "      <td>914</td>\n",
       "      <td>0.590810</td>\n",
       "      <td>0.590810</td>\n",
       "      <td>0.644762</td>\n",
       "      <td>0.590810</td>\n",
       "      <td>0.590810</td>\n",
       "      <td>0.537713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.670121</td>\n",
       "      <td>0.532217</td>\n",
       "      <td>0.575492</td>\n",
       "      <td>0.667583</td>\n",
       "      <td>0.389937</td>\n",
       "      <td>0.674497</td>\n",
       "      <td>914</td>\n",
       "      <td>0.617068</td>\n",
       "      <td>0.617068</td>\n",
       "      <td>0.667583</td>\n",
       "      <td>0.617068</td>\n",
       "      <td>0.617068</td>\n",
       "      <td>0.588092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.657367</td>\n",
       "      <td>0.631395</td>\n",
       "      <td>0.637856</td>\n",
       "      <td>0.679371</td>\n",
       "      <td>0.680193</td>\n",
       "      <td>0.582598</td>\n",
       "      <td>914</td>\n",
       "      <td>0.625821</td>\n",
       "      <td>0.625821</td>\n",
       "      <td>0.679371</td>\n",
       "      <td>0.625821</td>\n",
       "      <td>0.625821</td>\n",
       "      <td>0.469676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.642181</td>\n",
       "      <td>0.631814</td>\n",
       "      <td>0.632385</td>\n",
       "      <td>0.695814</td>\n",
       "      <td>0.646316</td>\n",
       "      <td>0.617312</td>\n",
       "      <td>914</td>\n",
       "      <td>0.647702</td>\n",
       "      <td>0.647702</td>\n",
       "      <td>0.695814</td>\n",
       "      <td>0.647702</td>\n",
       "      <td>0.647702</td>\n",
       "      <td>0.488472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.645800</td>\n",
       "      <td>0.636966</td>\n",
       "      <td>0.619332</td>\n",
       "      <td>0.626915</td>\n",
       "      <td>0.703532</td>\n",
       "      <td>0.673058</td>\n",
       "      <td>0.565605</td>\n",
       "      <td>914</td>\n",
       "      <td>0.658643</td>\n",
       "      <td>0.658643</td>\n",
       "      <td>0.703532</td>\n",
       "      <td>0.658643</td>\n",
       "      <td>0.658643</td>\n",
       "      <td>0.454453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.623700</td>\n",
       "      <td>0.594273</td>\n",
       "      <td>0.670668</td>\n",
       "      <td>0.670678</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.672470</td>\n",
       "      <td>0.668867</td>\n",
       "      <td>914</td>\n",
       "      <td>0.669584</td>\n",
       "      <td>0.669584</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.669584</td>\n",
       "      <td>0.669584</td>\n",
       "      <td>0.497003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.599600</td>\n",
       "      <td>0.612538</td>\n",
       "      <td>0.649752</td>\n",
       "      <td>0.665208</td>\n",
       "      <td>0.770600</td>\n",
       "      <td>0.576177</td>\n",
       "      <td>0.723327</td>\n",
       "      <td>914</td>\n",
       "      <td>0.689278</td>\n",
       "      <td>0.689278</td>\n",
       "      <td>0.770600</td>\n",
       "      <td>0.689278</td>\n",
       "      <td>0.689278</td>\n",
       "      <td>0.622223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.583500</td>\n",
       "      <td>0.570717</td>\n",
       "      <td>0.696384</td>\n",
       "      <td>0.696937</td>\n",
       "      <td>0.778610</td>\n",
       "      <td>0.709339</td>\n",
       "      <td>0.683429</td>\n",
       "      <td>914</td>\n",
       "      <td>0.702407</td>\n",
       "      <td>0.702407</td>\n",
       "      <td>0.778610</td>\n",
       "      <td>0.702407</td>\n",
       "      <td>0.702407</td>\n",
       "      <td>0.475692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.571700</td>\n",
       "      <td>0.563060</td>\n",
       "      <td>0.704120</td>\n",
       "      <td>0.707877</td>\n",
       "      <td>0.797040</td>\n",
       "      <td>0.737463</td>\n",
       "      <td>0.670777</td>\n",
       "      <td>914</td>\n",
       "      <td>0.717724</td>\n",
       "      <td>0.717724</td>\n",
       "      <td>0.797040</td>\n",
       "      <td>0.717724</td>\n",
       "      <td>0.717724</td>\n",
       "      <td>0.426672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.569100</td>\n",
       "      <td>0.535189</td>\n",
       "      <td>0.731598</td>\n",
       "      <td>0.731947</td>\n",
       "      <td>0.810993</td>\n",
       "      <td>0.741288</td>\n",
       "      <td>0.721907</td>\n",
       "      <td>914</td>\n",
       "      <td>0.735230</td>\n",
       "      <td>0.735230</td>\n",
       "      <td>0.810993</td>\n",
       "      <td>0.735230</td>\n",
       "      <td>0.735230</td>\n",
       "      <td>0.471266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.550300</td>\n",
       "      <td>0.533018</td>\n",
       "      <td>0.729930</td>\n",
       "      <td>0.731947</td>\n",
       "      <td>0.819674</td>\n",
       "      <td>0.706587</td>\n",
       "      <td>0.753273</td>\n",
       "      <td>914</td>\n",
       "      <td>0.739606</td>\n",
       "      <td>0.739606</td>\n",
       "      <td>0.819674</td>\n",
       "      <td>0.739606</td>\n",
       "      <td>0.739606</td>\n",
       "      <td>0.560148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.533700</td>\n",
       "      <td>0.512727</td>\n",
       "      <td>0.736253</td>\n",
       "      <td>0.736324</td>\n",
       "      <td>0.830064</td>\n",
       "      <td>0.740581</td>\n",
       "      <td>0.731924</td>\n",
       "      <td>914</td>\n",
       "      <td>0.743982</td>\n",
       "      <td>0.743982</td>\n",
       "      <td>0.830064</td>\n",
       "      <td>0.743982</td>\n",
       "      <td>0.743982</td>\n",
       "      <td>0.483699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.549167</td>\n",
       "      <td>0.701831</td>\n",
       "      <td>0.704595</td>\n",
       "      <td>0.813875</td>\n",
       "      <td>0.673123</td>\n",
       "      <td>0.730539</td>\n",
       "      <td>914</td>\n",
       "      <td>0.724289</td>\n",
       "      <td>0.724289</td>\n",
       "      <td>0.813875</td>\n",
       "      <td>0.724289</td>\n",
       "      <td>0.724289</td>\n",
       "      <td>0.589461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.526593</td>\n",
       "      <td>0.717677</td>\n",
       "      <td>0.725383</td>\n",
       "      <td>0.838917</td>\n",
       "      <td>0.764319</td>\n",
       "      <td>0.671035</td>\n",
       "      <td>914</td>\n",
       "      <td>0.748359</td>\n",
       "      <td>0.748359</td>\n",
       "      <td>0.838917</td>\n",
       "      <td>0.748359</td>\n",
       "      <td>0.748359</td>\n",
       "      <td>0.339371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.519200</td>\n",
       "      <td>0.501659</td>\n",
       "      <td>0.740080</td>\n",
       "      <td>0.742888</td>\n",
       "      <td>0.842757</td>\n",
       "      <td>0.767096</td>\n",
       "      <td>0.713065</td>\n",
       "      <td>914</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.842757</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.406187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.499156</td>\n",
       "      <td>0.731639</td>\n",
       "      <td>0.731947</td>\n",
       "      <td>0.835417</td>\n",
       "      <td>0.722537</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>914</td>\n",
       "      <td>0.743982</td>\n",
       "      <td>0.743982</td>\n",
       "      <td>0.835417</td>\n",
       "      <td>0.743982</td>\n",
       "      <td>0.743982</td>\n",
       "      <td>0.529041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>0.509578</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.742888</td>\n",
       "      <td>0.839635</td>\n",
       "      <td>0.773385</td>\n",
       "      <td>0.702908</td>\n",
       "      <td>914</td>\n",
       "      <td>0.746171</td>\n",
       "      <td>0.746171</td>\n",
       "      <td>0.839635</td>\n",
       "      <td>0.746171</td>\n",
       "      <td>0.746171</td>\n",
       "      <td>0.342036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.498500</td>\n",
       "      <td>0.515694</td>\n",
       "      <td>0.735590</td>\n",
       "      <td>0.737418</td>\n",
       "      <td>0.839362</td>\n",
       "      <td>0.713604</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>914</td>\n",
       "      <td>0.750547</td>\n",
       "      <td>0.750547</td>\n",
       "      <td>0.839362</td>\n",
       "      <td>0.750547</td>\n",
       "      <td>0.750547</td>\n",
       "      <td>0.581079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.471300</td>\n",
       "      <td>0.489505</td>\n",
       "      <td>0.763675</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.844625</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.764192</td>\n",
       "      <td>914</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.844625</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.501242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>0.484156</td>\n",
       "      <td>0.758518</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>0.849925</td>\n",
       "      <td>0.772257</td>\n",
       "      <td>0.744780</td>\n",
       "      <td>914</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.849925</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.428814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.486989</td>\n",
       "      <td>0.752999</td>\n",
       "      <td>0.753829</td>\n",
       "      <td>0.850351</td>\n",
       "      <td>0.767322</td>\n",
       "      <td>0.738676</td>\n",
       "      <td>914</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.850351</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.436630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.454300</td>\n",
       "      <td>0.552158</td>\n",
       "      <td>0.707614</td>\n",
       "      <td>0.713348</td>\n",
       "      <td>0.828685</td>\n",
       "      <td>0.748560</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>914</td>\n",
       "      <td>0.746171</td>\n",
       "      <td>0.746171</td>\n",
       "      <td>0.828685</td>\n",
       "      <td>0.746171</td>\n",
       "      <td>0.746171</td>\n",
       "      <td>0.361163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.624931</td>\n",
       "      <td>0.671354</td>\n",
       "      <td>0.687090</td>\n",
       "      <td>0.824490</td>\n",
       "      <td>0.743268</td>\n",
       "      <td>0.599440</td>\n",
       "      <td>914</td>\n",
       "      <td>0.739606</td>\n",
       "      <td>0.739606</td>\n",
       "      <td>0.824490</td>\n",
       "      <td>0.739606</td>\n",
       "      <td>0.739606</td>\n",
       "      <td>0.253610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>0.479461</td>\n",
       "      <td>0.755838</td>\n",
       "      <td>0.757112</td>\n",
       "      <td>0.851970</td>\n",
       "      <td>0.773469</td>\n",
       "      <td>0.738208</td>\n",
       "      <td>914</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.851970</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.426565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.478200</td>\n",
       "      <td>0.482739</td>\n",
       "      <td>0.762005</td>\n",
       "      <td>0.762582</td>\n",
       "      <td>0.854967</td>\n",
       "      <td>0.750288</td>\n",
       "      <td>0.773723</td>\n",
       "      <td>914</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.854967</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.567696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.481221</td>\n",
       "      <td>0.764756</td>\n",
       "      <td>0.764770</td>\n",
       "      <td>0.851472</td>\n",
       "      <td>0.766558</td>\n",
       "      <td>0.762955</td>\n",
       "      <td>914</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.851472</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.494444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.491492</td>\n",
       "      <td>0.748348</td>\n",
       "      <td>0.748359</td>\n",
       "      <td>0.843801</td>\n",
       "      <td>0.746696</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>914</td>\n",
       "      <td>0.750547</td>\n",
       "      <td>0.750547</td>\n",
       "      <td>0.843801</td>\n",
       "      <td>0.750547</td>\n",
       "      <td>0.750547</td>\n",
       "      <td>0.520506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>0.477062</td>\n",
       "      <td>0.764565</td>\n",
       "      <td>0.764770</td>\n",
       "      <td>0.853262</td>\n",
       "      <td>0.771520</td>\n",
       "      <td>0.757610</td>\n",
       "      <td>914</td>\n",
       "      <td>0.768053</td>\n",
       "      <td>0.768053</td>\n",
       "      <td>0.853262</td>\n",
       "      <td>0.768053</td>\n",
       "      <td>0.768053</td>\n",
       "      <td>0.460476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.444300</td>\n",
       "      <td>0.474161</td>\n",
       "      <td>0.775839</td>\n",
       "      <td>0.776805</td>\n",
       "      <td>0.856298</td>\n",
       "      <td>0.790554</td>\n",
       "      <td>0.761124</td>\n",
       "      <td>914</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.856298</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.418922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.413200</td>\n",
       "      <td>0.476271</td>\n",
       "      <td>0.773210</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.791075</td>\n",
       "      <td>0.755344</td>\n",
       "      <td>914</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.384342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.439900</td>\n",
       "      <td>0.484242</td>\n",
       "      <td>0.765573</td>\n",
       "      <td>0.768053</td>\n",
       "      <td>0.856552</td>\n",
       "      <td>0.789683</td>\n",
       "      <td>0.741463</td>\n",
       "      <td>914</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>0.856552</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>0.370058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.519731</td>\n",
       "      <td>0.749385</td>\n",
       "      <td>0.749453</td>\n",
       "      <td>0.840186</td>\n",
       "      <td>0.753498</td>\n",
       "      <td>0.745273</td>\n",
       "      <td>914</td>\n",
       "      <td>0.752735</td>\n",
       "      <td>0.752735</td>\n",
       "      <td>0.840186</td>\n",
       "      <td>0.752735</td>\n",
       "      <td>0.752735</td>\n",
       "      <td>0.470688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.418100</td>\n",
       "      <td>0.467667</td>\n",
       "      <td>0.764851</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>0.860828</td>\n",
       "      <td>0.780287</td>\n",
       "      <td>0.749415</td>\n",
       "      <td>914</td>\n",
       "      <td>0.776805</td>\n",
       "      <td>0.776805</td>\n",
       "      <td>0.860828</td>\n",
       "      <td>0.776805</td>\n",
       "      <td>0.776805</td>\n",
       "      <td>0.414159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.482197</td>\n",
       "      <td>0.774640</td>\n",
       "      <td>0.775711</td>\n",
       "      <td>0.862008</td>\n",
       "      <td>0.759107</td>\n",
       "      <td>0.790174</td>\n",
       "      <td>914</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.862008</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.596349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.406600</td>\n",
       "      <td>0.475980</td>\n",
       "      <td>0.763575</td>\n",
       "      <td>0.764770</td>\n",
       "      <td>0.859358</td>\n",
       "      <td>0.780388</td>\n",
       "      <td>0.746761</td>\n",
       "      <td>914</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.859358</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.410059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.480698</td>\n",
       "      <td>0.757262</td>\n",
       "      <td>0.758206</td>\n",
       "      <td>0.860071</td>\n",
       "      <td>0.742124</td>\n",
       "      <td>0.772400</td>\n",
       "      <td>914</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.860071</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.576994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>0.464292</td>\n",
       "      <td>0.775634</td>\n",
       "      <td>0.775711</td>\n",
       "      <td>0.862537</td>\n",
       "      <td>0.779807</td>\n",
       "      <td>0.771460</td>\n",
       "      <td>914</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.862537</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.476593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.389600</td>\n",
       "      <td>0.467285</td>\n",
       "      <td>0.770152</td>\n",
       "      <td>0.770241</td>\n",
       "      <td>0.863394</td>\n",
       "      <td>0.774678</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>914</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.863394</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.763676</td>\n",
       "      <td>0.460450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.381700</td>\n",
       "      <td>0.476461</td>\n",
       "      <td>0.769375</td>\n",
       "      <td>0.770241</td>\n",
       "      <td>0.861493</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>0.755245</td>\n",
       "      <td>914</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>0.861493</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>0.417909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.396100</td>\n",
       "      <td>0.468770</td>\n",
       "      <td>0.762103</td>\n",
       "      <td>0.762582</td>\n",
       "      <td>0.864443</td>\n",
       "      <td>0.772775</td>\n",
       "      <td>0.751432</td>\n",
       "      <td>914</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.864443</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.427870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.436800</td>\n",
       "      <td>0.462157</td>\n",
       "      <td>0.772215</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.866190</td>\n",
       "      <td>0.779193</td>\n",
       "      <td>0.765237</td>\n",
       "      <td>914</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.866190</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.459106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.397400</td>\n",
       "      <td>0.476220</td>\n",
       "      <td>0.770442</td>\n",
       "      <td>0.771335</td>\n",
       "      <td>0.864898</td>\n",
       "      <td>0.756126</td>\n",
       "      <td>0.784758</td>\n",
       "      <td>914</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.864898</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.588293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.459941</td>\n",
       "      <td>0.772419</td>\n",
       "      <td>0.772429</td>\n",
       "      <td>0.865731</td>\n",
       "      <td>0.773913</td>\n",
       "      <td>0.770925</td>\n",
       "      <td>914</td>\n",
       "      <td>0.776805</td>\n",
       "      <td>0.776805</td>\n",
       "      <td>0.865731</td>\n",
       "      <td>0.776805</td>\n",
       "      <td>0.776805</td>\n",
       "      <td>0.489187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.461392</td>\n",
       "      <td>0.776767</td>\n",
       "      <td>0.776805</td>\n",
       "      <td>0.866506</td>\n",
       "      <td>0.773836</td>\n",
       "      <td>0.779698</td>\n",
       "      <td>914</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.866506</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.517804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.386200</td>\n",
       "      <td>0.470511</td>\n",
       "      <td>0.763915</td>\n",
       "      <td>0.764770</td>\n",
       "      <td>0.864098</td>\n",
       "      <td>0.778122</td>\n",
       "      <td>0.749709</td>\n",
       "      <td>914</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.864098</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.421962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.384400</td>\n",
       "      <td>0.462506</td>\n",
       "      <td>0.765612</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>0.866214</td>\n",
       "      <td>0.773305</td>\n",
       "      <td>0.757919</td>\n",
       "      <td>914</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.866214</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.778993</td>\n",
       "      <td>0.466154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 01:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_loss so early stopping is disabled\n",
      "early stopping required metric_for_best_model, but did not find eval_loss so early stopping is disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on eval set\n",
      "{\n",
      "    \"eval_loss\": 0.4599413573741913,\n",
      "    \"eval_f1_weighted\": 0.7724190768052096,\n",
      "    \"eval_accuracy\": 0.7724288840262582,\n",
      "    \"eval_roc_auc\": 0.8657307432642724,\n",
      "    \"eval_f1_human\": 0.7739130434782608,\n",
      "    \"eval_f1_ai\": 0.7709251101321586,\n",
      "    \"eval_n_samples\": 914,\n",
      "    \"eval_f1_weighted_median\": 0.7768052516411379,\n",
      "    \"eval_accuracy_median\": 0.7768052516411379,\n",
      "    \"eval_roc_auc_median\": 0.8657307432642724,\n",
      "    \"eval_f1_human_median\": 0.7768052516411379,\n",
      "    \"eval_f1_ai_median\": 0.7768052516411379,\n",
      "    \"eval_threshold_median\": 0.48918724060058594,\n",
      "    \"eval_runtime\": 5.2663,\n",
      "    \"eval_samples_per_second\": 173.556,\n",
      "    \"eval_steps_per_second\": 0.19,\n",
      "    \"epoch\": 10.0\n",
      "}\n",
      "Evaluating on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_loss so early stopping is disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"test_loss\": 0.4543440341949463,\n",
      "    \"test_f1_weighted\": 0.7816154023128263,\n",
      "    \"test_accuracy\": 0.7816593886462883,\n",
      "    \"test_roc_auc\": 0.8703268911729373,\n",
      "    \"test_f1_human\": 0.7847147470398278,\n",
      "    \"test_f1_ai\": 0.778516057585825,\n",
      "    \"test_n_samples\": 1832,\n",
      "    \"test_f1_weighted_median\": 0.7805676855895196,\n",
      "    \"test_accuracy_median\": 0.7805676855895196,\n",
      "    \"test_roc_auc_median\": 0.8703268911729373,\n",
      "    \"test_f1_human_median\": 0.7805676855895196,\n",
      "    \"test_f1_ai_median\": 0.7805676855895196,\n",
      "    \"test_threshold_median\": 0.4840404987335205,\n",
      "    \"test_runtime\": 7.0609,\n",
      "    \"test_samples_per_second\": 259.459,\n",
      "    \"test_steps_per_second\": 0.283,\n",
      "    \"epoch\": 10.0\n",
      "}\n",
      "Evaluating on unmatched set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_loss so early stopping is disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"unmatched_loss\": 0.6830250024795532,\n",
      "    \"unmatched_f1_weighted\": 0.6522281964629305,\n",
      "    \"unmatched_accuracy\": 0.6583196046128501,\n",
      "    \"unmatched_roc_auc\": 0.7118686018887167,\n",
      "    \"unmatched_f1_human\": 0.7221701272605492,\n",
      "    \"unmatched_f1_ai\": 0.5563636363636364,\n",
      "    \"unmatched_n_samples\": 24280,\n",
      "    \"unmatched_f1_weighted_mean\": 0.653477910639714,\n",
      "    \"unmatched_accuracy_mean\": 0.6530065897858319,\n",
      "    \"unmatched_roc_auc_mean\": 0.7118686018887167,\n",
      "    \"unmatched_f1_human_mean\": 0.6975625516028288,\n",
      "    \"unmatched_f1_ai_mean\": 0.5930541467420181,\n",
      "    \"unmatched_threshold_mean\": 0.41250377893447876,\n",
      "    \"unmatched_n_samples_human\": 14038,\n",
      "    \"unmatched_n_samples_ai\": 10242,\n",
      "    \"unmatched_runtime\": 41.2587,\n",
      "    \"unmatched_samples_per_second\": 588.481,\n",
      "    \"unmatched_steps_per_second\": 0.582,\n",
      "    \"epoch\": 10.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer._load_best_model()\n",
    "classifier = trainer.model\n",
    "\n",
    "metrics_eval = trainer.evaluate()\n",
    "metrics_test = trainer.evaluate(datasets_matched[\"test\"], metric_key_prefix=\"test\")\n",
    "metrics_unmatched = trainer.evaluate(dataset_unmatched, metric_key_prefix=\"unmatched\")\n",
    "\n",
    "path = save_model(trainer, config)\n",
    "\n",
    "print(\"Evaluating on eval set\")\n",
    "metrics_eval = trainer.evaluate()\n",
    "print(json.dumps(metrics_eval, indent=4))\n",
    "\n",
    "print(\"Evaluating on test set\")\n",
    "metrics_test = trainer.evaluate(\n",
    "    datasets_matched[\"test\"],  # type: ignore\n",
    "    metric_key_prefix=\"test\",\n",
    ")\n",
    "print(json.dumps(metrics_test, indent=4))\n",
    "\n",
    "print(\"Evaluating on unmatched set\")\n",
    "metrics_unmatched = trainer.evaluate(\n",
    "    dataset_unmatched,  # type: ignore\n",
    "    metric_key_prefix=\"unmatched\",\n",
    ")\n",
    "print(json.dumps(metrics_unmatched, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2ea5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.image import AxesImage\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "cubehelix = sns.cubehelix_palette(as_cmap=True)\n",
    "\n",
    "\n",
    "def visualize_features(features: NDArray, cmap=cubehelix, size=4) -> AxesImage:\n",
    "    a = features.shape[0] / features.shape[1]\n",
    "    h, w = (size, size * a) if a > 1 else (size, size / a)\n",
    "    fig, ax = plt.subplots(figsize=(h, w))\n",
    "    fig = ax.imshow(\n",
    "        features,\n",
    "        cmap=cmap,\n",
    "        vmin=min(0.0, features.min()),\n",
    "        vmax=max(1.0, features.max()),\n",
    "        aspect=\"equal\",\n",
    "    )\n",
    "    fig.axes.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f4ff18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "torch.Size([128, 13])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x3938.46 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'LuminarCNN' object has no attribute 'conv_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m plt.show()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m      8\u001b[39m     conv = (\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv_layers\u001b[49m(features.T.unsqueeze(\u001b[32m0\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     10\u001b[39m         .cpu()\n\u001b[32m     11\u001b[39m         .detach()[\u001b[32m0\u001b[39m].T\n\u001b[32m     12\u001b[39m     )\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(conv.shape)\n\u001b[32m     14\u001b[39m     visualize_features(conv.clip(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).numpy().T)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'LuminarCNN' object has no attribute 'conv_layers'"
     ]
    }
   ],
   "source": [
    "sample = datasets_matched[\"test\"][4]\n",
    "print(sample[\"labels\"])\n",
    "features = sample[\"features\"]\n",
    "print(features.shape)\n",
    "visualize_features(features.T)\n",
    "plt.show()\n",
    "with torch.inference_mode():\n",
    "    conv = (\n",
    "        classifier.conv_layers(features.T.unsqueeze(0).to(\"cuda\"))\n",
    "        .cpu()\n",
    "        .detach()[0].T\n",
    "    )\n",
    "    print(conv.shape)\n",
    "    visualize_features(conv.clip(0, 1).numpy().T)\n",
    "    plt.show()\n",
    "\n",
    "    ff = classifier.projection[:-1](conv.unsqueeze(0).to(\"cuda\")).cpu().detach()[0]\n",
    "    print(ff.shape)\n",
    "    visualize_features(ff.numpy().T)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = datasets_matched[\"test\"][-1]\n",
    "print(sample[\"labels\"])\n",
    "features = sample[\"features\"]\n",
    "print(features.shape)\n",
    "visualize_features(features.T)\n",
    "plt.show()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    conv = (\n",
    "        classifier.conv_layers[:-1](features.T.unsqueeze(0).to(\"cuda\"))\n",
    "        .cpu()\n",
    "        .detach()[0]\n",
    "        .T\n",
    "    )\n",
    "    print(conv.shape)\n",
    "    visualize_features(conv.clip(0, 1).numpy().T)\n",
    "    plt.show()\n",
    "\n",
    "    ff = classifier.projection[:-1](conv.unsqueeze(0).to(\"cuda\")).cpu().detach()[0]\n",
    "    print(ff.shape)\n",
    "    visualize_features(ff.numpy().T)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bf89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b95ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(scores, indent=4))\n",
    "with open(\"../logs/luminar/gpt2_first_128-3_epochs.json\", \"w\") as f:\n",
    "    json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for subset in [\n",
    "    \"blog_authorship_corpus\",\n",
    "    \"student_essays\",\n",
    "    \"cnn_news\",\n",
    "    \"euro_court_cases\",\n",
    "    \"house_of_commons\",\n",
    "    \"arxiv_papers\",\n",
    "    \"gutenberg_en\",\n",
    "    \"en\",\n",
    "    \"bundestag\",\n",
    "    \"spiegel_articles\",\n",
    "    \"gutenberg_de\",\n",
    "    \"de\",\n",
    "]:\n",
    "    datset_config_name = f\"{subset}-fulltext\"\n",
    "    datasets[datset_config_name] = load_dataset(\n",
    "        \"liberi-luminaris/PrismAI-encoded-gpt2\",\n",
    "        datset_config_name,\n",
    "        token=HF_TOKEN,\n",
    "        split=\"human+gpt_4o_mini\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"liberi-luminaris/PrismAI-fulltext\", \"cnn_news\", split=\"human+gpt_4o_mini\"\n",
    ")\n",
    "dataset_human = dataset.filter(lambda sample: sample[\"agent\"] == \"human\")\n",
    "source_ids = set(\n",
    "    dataset_human.shuffle(seed=42).take(len(dataset_human) // 10 * 8)[\"id_source\"]\n",
    ")\n",
    "dataset_train = dataset.filter(lambda sample: sample[\"id_source\"] in source_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec3e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_truncated = {}\n",
    "for datset_config_name, dataset in datasets.items():\n",
    "    datasets_truncated[datset_config_name] = dataset.with_format(\n",
    "        \"numpy\", columns=[\"features\"], output_all_columns=True\n",
    "    ).map(\n",
    "        lambda batch: {\"features\": batch[\"features\"][:, :256]},\n",
    "        batched=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18961af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_considered = {\n",
    "    key: value\n",
    "    for key, value in datasets_truncated.items()\n",
    "    if not key.startswith(\"de-\") and not key.startswith(\"en-\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2174dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "domains = [\n",
    "    \"Web Blogs\",\n",
    "    \"Essays\",\n",
    "    \"CNN\",\n",
    "    \"ECHR\",\n",
    "    \"HoC\",\n",
    "    \"arXiv\",\n",
    "    \"Gutenberg$_{en}$\",\n",
    "    \"Bundestag$_{de}$\",\n",
    "    \"Spiegel$_{de}$\",\n",
    "    \"Gutenberg$_{de}$\",\n",
    "    \"All$_{en}$\",\n",
    "    \"All$_{de}$\",\n",
    "]\n",
    "name_map = {\n",
    "    \"blog_authorship_corpus\": \"Web Blogs\",\n",
    "    \"student_essays\": \"Essays\",\n",
    "    \"cnn_news\": \"CNN\",\n",
    "    \"euro_court_cases\": \"ECHR\",\n",
    "    \"house_of_commons\": \"HoC\",\n",
    "    \"arxiv_papers\": \"arXiv\",\n",
    "    \"gutenberg_en\": \"Gutenberg$_{en}$\",\n",
    "    \"bundestag\": \"Bundestag$_{de}$\",\n",
    "    \"spiegel_articles\": \"Spiegel$_{de}$\",\n",
    "    \"gutenberg_de\": \"Gutenberg$_{de}$\",\n",
    "    \"en\": \"All$_{en}$\",\n",
    "    \"de\": \"All$_{de}$\",\n",
    "}\n",
    "\n",
    "results = [\n",
    "    {\"domain\": name_map[key.split(\"-\", 1)[0]]}\n",
    "    | {\n",
    "        \"f1\": value[\"f1\"],\n",
    "        \"acc\": value[\"accuracy\"],\n",
    "        \"auroc\": value[\"auroc\"],\n",
    "    }\n",
    "    for key, value in scores.items()\n",
    "]\n",
    "metric_df = (\n",
    "    pd.DataFrame(results)\n",
    "    .set_index(\"domain\")\n",
    "    .sort_index(key=lambda x: list(map(domains.index, x)))\n",
    ")\n",
    "print(metric_df.to_latex(float_format=\"%.3f\", index=True))\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e71e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_detector(\n",
    "#     detector: DetectorABC, datasets: dict[str, DatasetDict]\n",
    "# ) -> dict[str, float]:\n",
    "#     scores = {}\n",
    "#     for config_name, ds in tqdm(datasets.items(), desc=\"Predicting on Datasets\"):\n",
    "#         dataset: Dataset = ds[\"test\"].map(\n",
    "#             detector.tokenize,\n",
    "#             input_columns=[\"text\"],\n",
    "#             batched=True,\n",
    "#             batch_size=1024,\n",
    "#             desc=\"Tokenizing\",\n",
    "#         )\n",
    "#         dataset = dataset.sort(\"length\")\n",
    "#         dataset = dataset.map(\n",
    "#             detector.process,\n",
    "#             batched=True,\n",
    "#             batch_size=128,\n",
    "#             desc=\"Predicting\",\n",
    "#         )\n",
    "\n",
    "#         dataset_np = dataset.select_columns([\"prediction\", \"label\"]).with_format(\n",
    "#             \"numpy\"\n",
    "#         )\n",
    "\n",
    "#         acc, f1, auroc = get_scores(dataset_np[\"label\"], dataset_np[\"prediction\"])\n",
    "#         scores[config_name] = {\"accuracy\": acc, \"f1\": f1, \"auroc\": auroc}\n",
    "\n",
    "#         acc, f1, auroc = get_scores(\n",
    "#             dataset_np[\"label\"],\n",
    "#             dataset_np[\"prediction\"],\n",
    "#             calibrated=True,\n",
    "#         )\n",
    "#         scores[config_name] |= {\n",
    "#             \"accuracy_calibrated\": acc,\n",
    "#             \"f1_calibrated\": f1,\n",
    "#             \"auroc_calibrated\": auroc,\n",
    "#         }\n",
    "#     return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7958c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model: LuminarClassifier, datasets: dict[str, DatasetDict]) -> dict:\n",
    "#     scores = {}\n",
    "#     for config_name, dataset in tqdm(datasets.items(), desc=\"Evaluating\", leave=False):\n",
    "#         ds = (\n",
    "#             dataset[\"test\"]\n",
    "#             .with_format(\"torch\", [\"features\"])\n",
    "#             .map(model.process, batched=True, batch_size=32, desc=\"Predicting\")\n",
    "#         )\n",
    "#         dataset_np = ds.select_columns([\"prediction\", \"label\"]).with_format(\"numpy\")\n",
    "\n",
    "#         acc, f1, auroc = get_scores(dataset_np[\"label\"], dataset_np[\"prediction\"])\n",
    "#         scores[config_name] = {\n",
    "#             \"accuracy\": acc,\n",
    "#             \"f1\": f1,\n",
    "#             \"auroc\": auroc,\n",
    "#         }\n",
    "\n",
    "#         acc, f1, auroc = get_scores(\n",
    "#             dataset_np[\"label\"],\n",
    "#             dataset_np[\"prediction\"],\n",
    "#             calibrated=True,\n",
    "#         )\n",
    "#         scores[config_name] |= {\n",
    "#             \"accuracy_calibrated\": acc,\n",
    "#             \"f1_calibrated\": f1,\n",
    "#             \"auroc_calibrated\": auroc,\n",
    "#         }\n",
    "\n",
    "#     return scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

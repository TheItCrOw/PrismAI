{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# About\n",
    "\n",
    "Evaluating LuminarSeq as a document-based classifier in different datasets."
   ],
   "id": "dc333bc16751375f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T07:37:03.275230Z",
     "start_time": "2025-09-17T07:37:03.059636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "9788e1264b6d7837",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-17T07:38:08.462489Z",
     "start_time": "2025-09-17T07:37:03.320468Z"
    }
   },
   "source": [
    "import torch\n",
    "import gc\n",
    "import json\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from luminar.detector import LuminarSequenceDetector\n",
    "from luminar.utils.cuda import get_best_device\n",
    "from data_hub.hub import DataHub\n",
    "from luminar.sequence_classifier import LuminarSequence\n",
    "from luminar.utils import LuminarSequenceTrainingConfig, ConvolutionalLayerSpec\n",
    "from luminar.utils.visualization import visualize_detection\n",
    "from pathlib import Path\n",
    "from data_hub.sequential_data_processor import SequentialDataProcessor\n",
    "from luminar.encoder import LuminarEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from luminar.utils import calculate_metrics\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    with torch.cuda.device(torch.cuda.current_device()):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T07:38:09.560239Z",
     "start_time": "2025-09-17T07:38:08.601294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = {\n",
    "    \"Ghostbuster\": {\n",
    "        \"in_domain_model\": \"/storage/projects/boenisch/PrismAI/models/luminar_sequence/Ghostbuster-encoded-gpt2/qnt6a8k3\",\n",
    "        \"out_of_domain\": \"PrismAI\",\n",
    "        \"feature_agent\": \"gpt2\",\n",
    "        \"evaluate\": False\n",
    "    },\n",
    "    \"SeqXGPT\": {\n",
    "        \"in_domain_model\": \"/storage/projects/boenisch/PrismAI/models/luminar_sequence/SeqXGPT-encoded-gpt2/zgfsvd82\",\n",
    "        \"out_of_domain\": \"PrismAI_v2\",\n",
    "        \"feature_agent\": \"gpt2\",\n",
    "        \"evaluate\": False\n",
    "    },\n",
    "    \"M4\": {\n",
    "        \"in_domain_model\": \"/storage/projects/boenisch/PrismAI/models/luminar_sequence/M4-encoded-gpt2/oi2ld9pf\",\n",
    "        \"out_of_domain\": \"PrismAI_v2\",\n",
    "        \"feature_agent\": \"gpt2\",\n",
    "        \"evaluate\": False\n",
    "    },\n",
    "    \"MAGE\": {\n",
    "        \"in_domain_model\": \"/storage/projects/boenisch/PrismAI/models/luminar_sequence/MAGE-encoded-gpt2/pj3t04t0\",\n",
    "        \"out_of_domain\": \"PrismAI_v2\",\n",
    "        \"feature_agent\": \"gpt2\",\n",
    "        \"evaluate\": False\n",
    "    },\n",
    "    \"PrismAI_v2\": {\n",
    "        \"in_domain_model\": \"/storage/projects/boenisch/PrismAI/models/luminar_sequence/PrismAI_v2-encoded-gpt2/e1s2k2du\",\n",
    "        \"out_of_domain\": \"M4\",\n",
    "        \"feature_agent\": \"gpt2\",\n",
    "        \"evaluate\": False\n",
    "    },\n",
    "    \"RAID_none\": {\n",
    "        \"in_domain_model\": \"/storage/projects/boenisch/PrismAI/models/luminar_sequence/RAID_none-encoded-gpt2/prkvbp96\",\n",
    "        \"out_of_domain\": \"PrismAI_v2\",\n",
    "        \"feature_agent\": \"gpt2\",\n",
    "        \"evaluate\": True\n",
    "    },\n",
    "}"
   ],
   "id": "beb348c6c13dd390",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T07:38:09.694010Z",
     "start_time": "2025-09-17T07:38:09.582540Z"
    }
   },
   "cell_type": "code",
   "source": "hub = DataHub((Path.home() / \".hf_token\").read_text().strip())",
   "id": "543acf7fb8a2f404",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "e29b08d56a4e53e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T07:38:09.779280Z",
     "start_time": "2025-09-17T07:38:09.730646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(detector, test_dataset):\n",
    "    preds = []\n",
    "    truth = []\n",
    "    max_iters = 1000000000\n",
    "    for row in tqdm(test_dataset, desc=\"Evaluating\", unit=\"sample\"):\n",
    "        if row[\"label\"] not in [0, 1]:\n",
    "            continue\n",
    "        if len(preds) > max_iters:\n",
    "            break\n",
    "        try:\n",
    "            result = detector.detect(row[\"text\"])\n",
    "        except ValueError:\n",
    "            # Text was too short for LuminarSeq, skipping it.\n",
    "            continue\n",
    "\n",
    "        # Skip NaN or infinite values\n",
    "        if np.isnan(result[\"avg\"]) or np.isinf(result[\"avg\"]):\n",
    "            continue\n",
    "\n",
    "        avg = result[\"avg\"] / 100\n",
    "        preds.append(avg)\n",
    "        truth.append(row[\"label\"])\n",
    "\n",
    "    return calculate_metrics(\n",
    "        y_true=np.array(truth),\n",
    "        y_scores=np.array(preds),\n",
    "        threshold=0.5\n",
    "    )"
   ],
   "id": "d07ae1b9961a1653",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-17T07:38:09.842655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = get_best_device()\n",
    "os.makedirs(\"sequence_metrics\", exist_ok=True)\n",
    "\n",
    "for key in datasets.keys():\n",
    "    dataset = datasets[key]\n",
    "    if not dataset[\"evaluate\"]:\n",
    "        print(f\"Not evaluating {key}.\")\n",
    "        continue\n",
    "    print(\"Doing: \", key)\n",
    "\n",
    "    # Load data\n",
    "    splits = hub.get_splits(f\"TheItCrOw/{key}\")\n",
    "    test_dataset = splits[\"test\"]\n",
    "    print(\"Test Length: \", len(test_dataset))\n",
    "\n",
    "    # In-Domain\n",
    "    detector = LuminarSequenceDetector(model_path=dataset[\"in_domain_model\"],\n",
    "                                       feature_agent=dataset[\"feature_agent\"],\n",
    "                                       device=device)\n",
    "    in_domain_metrics = evaluate(detector, test_dataset)\n",
    "    print(\"In Domain: \", in_domain_metrics)\n",
    "\n",
    "    # Out of Domain\n",
    "    ood_dataset = datasets[dataset[\"out_of_domain\"]]\n",
    "    detector = LuminarSequenceDetector(model_path=ood_dataset[\"in_domain_model\"],\n",
    "                                       feature_agent=ood_dataset[\"feature_agent\"],\n",
    "                                       device=device)\n",
    "    ood_metrics = evaluate(detector, test_dataset)\n",
    "    print(\"Out of Domain: \", ood_metrics)\n",
    "    with open(f'./sequence_metrics/results_{key}.json', 'w') as fp:\n",
    "        json.dump({\"in_domain\": in_domain_metrics, \"ood\": ood_metrics}, fp)"
   ],
   "id": "17769a28d48dd245",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not evaluating Ghostbuster.\n",
      "Not evaluating SeqXGPT.\n",
      "Not evaluating M4.\n",
      "Not evaluating MAGE.\n",
      "Not evaluating PrismAI_v2.\n",
      "Doing:  RAID_none\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/550 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2612e6c5fd14eb298a3e4db71266ec3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00000-of-00002.parquet:   0%|          | 0.00/264M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47d6b31c80d147deabff6e4bcc84e933"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00001-of-00002.parquet:   0%|          | 0.00/245M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfd4a264cd8c44e9959c9543df03ea79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/654910 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "304374c7458d4617b8e58d3b15d5f5a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter (num_proc=32):   0%|          | 0/654910 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9c09e088ffd4d81a8416138f1f75e5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/654910 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fdf38d5caf44c50bb331628aa0d13db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label ID mapping:\n",
      "0 → human\n",
      "1 → ai\n",
      "2 → fusion\n",
      "train distribution:\n",
      "  ai: 444035 (96.9%)\n",
      "  human: 14402 (3.1%)\n",
      "eval distribution:\n",
      "  ai: 63434 (96.9%)\n",
      "  human: 2057 (3.1%)\n",
      "test distribution:\n",
      "  ai: 126867 (96.9%)\n",
      "  human: 4115 (3.1%)\n",
      "Test Length:  130982\n",
      "Loading LuminarSequenceDetector from /storage/projects/boenisch/PrismAI/models/luminar_sequence/RAID_none-encoded-gpt2/prkvbp96 to device cuda:0\n",
      "LuminarSequenceTrainingConfig(feature_len=512, num_intermediate_likelihoods=13, apply_delta_augmentation=True, apply_product_augmentation=False, conv_layer_shapes=[[32, 5, 1], [64, 5, 1], [32, 3, 1]], projection_dim=64, lstm_hidden_dim=256, lstm_layers=1, stack_spans=5, hf_dataset='TheItCrOw/RAID_none-encoded-gpt2', dataset_root_path='/storage/projects/stoeckel/prismai/encoded/fulltext/', models_root_path='/storage/projects/boenisch/PrismAI/models/luminar_sequence/', domain=None, agent='gpt_4o_mini_gemma2_9b', feature_agent='gpt2', max_epochs=100, batch_size=128, early_stopping_patience=8, rescale_features=False, kfold=3, learning_rate=0.004, seed=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/staff_homes/kboenisc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                                                               | 0/130982 [00:00<?, ?sample/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Evaluating:   0%|                                                                                    | 74/130982 [00:07<1:00:17, 36.19sample/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:37:01.119078500Z",
     "start_time": "2025-09-11T11:25:36.915960Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f0513dbd0b6287d6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from functools import partial\n",
    "from typing import Generator, Iterable, NamedTuple, TypedDict\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "type _ModelOutput = dict[str, torch.Tensor | list[torch.Tensor]]\n",
    "\n",
    "\n",
    "class Metrics(NamedTuple):\n",
    "    llr: float\n",
    "    fdg: float\n",
    "\n",
    "\n",
    "class Sample(TypedDict):\n",
    "    input_ids: list[int]\n",
    "    attention_mask: list[int]\n",
    "\n",
    "\n",
    "class MetricsCalculator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        batch_size: int = 128,\n",
    "        device: str | torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model)\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self.set_model(model)\n",
    "\n",
    "    def set_model(self, model: PreTrainedModel):\n",
    "        self._model = model\n",
    "        self._requires_position_ids = \"position_ids\" in set(\n",
    "            inspect.signature(self.model.forward).parameters.keys()\n",
    "        )\n",
    "        self.to(self.device)\n",
    "\n",
    "    def to(self, device: str | torch.device):\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(self.device)\n",
    "        return self\n",
    "\n",
    "    def process(\n",
    "        self,\n",
    "        dataset: list[Sample],\n",
    "        pad_token_id: int,\n",
    "    ) -> list[Metrics]:\n",
    "        \"\"\"\n",
    "        Calculate metrics for the given pre-processed dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (list[Sample]): A sequence of pre-processed documents to be processed.\n",
    "            pad_token_id (int): The token ID to use for padding.\n",
    "\n",
    "        Returns:\n",
    "            list[Metrics]: A list of calculated metrics.\n",
    "        \"\"\"\n",
    "        return list(self._generate_scores(dataset, pad_token_id))\n",
    "\n",
    "    def _generate_scores(\n",
    "        self, dataset: list[Sample], pad_token_id: int\n",
    "    ) -> Generator[Metrics, None, None]:\n",
    "        _collate_fn = partial(collate_fn, pad_token_id=pad_token_id)\n",
    "        for input_ids, attention_mask in tqdm(\n",
    "            DataLoader(\n",
    "                dataset,\n",
    "                shuffle=False,\n",
    "                collate_fn=_collate_fn,\n",
    "                batch_size=self.batch_size,\n",
    "            ),\n",
    "            position=2,\n",
    "            leave=False,\n",
    "            desc=\"Processing Sequences\",\n",
    "        ):\n",
    "            yield from self._process_batch(input_ids, attention_mask, pad_token_id)\n",
    "\n",
    "    def _process_batch(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        pad_token_id: int,\n",
    "    ) -> list[Metrics]:\n",
    "        \"\"\"\n",
    "        Process the a batch of input sequences and calculate transition scores.\n",
    "        Runs a forward pass on the model and extracts the top k probabilities.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): A list of input sequences, each represented as a list of token IDs.\n",
    "            attention_mask (torch.Tensor): A list of attention masks for each input sequence.\n",
    "            pad_token_id (int): The token ID that has been used for padding.\n",
    "\n",
    "        Returns:\n",
    "            list[TransitionScores]: A list output probability tuples.\n",
    "        \"\"\"\n",
    "        (\n",
    "            likelihoods,\n",
    "            log_likelihoods,\n",
    "        ) = self._forward(input_ids, attention_mask)\n",
    "\n",
    "        results = []\n",
    "        for (\n",
    "            target_ids,\n",
    "            likelihood,\n",
    "            log_likelihood,\n",
    "        ) in zip(\n",
    "            input_ids.to(self.device),\n",
    "            likelihoods,\n",
    "            log_likelihoods,\n",
    "        ):\n",
    "            # Truncate the sequence to the last non-pad token\n",
    "            labels = target_ids[1:].view(-1, 1)\n",
    "            labels = labels[: labels.ne(pad_token_id).sum()]\n",
    "\n",
    "            likelihood: torch.Tensor = likelihood[: labels.size(0)]\n",
    "            log_likelihood: torch.Tensor = log_likelihood[: labels.size(0)]\n",
    "\n",
    "            target_log_probs = log_likelihood.gather(-1, labels).squeeze(-1)\n",
    "\n",
    "            # Get target likelihoods and ranks\n",
    "            _, sorted_indices = torch.sort(likelihood, descending=True)\n",
    "            _, target_ranks = torch.where(sorted_indices.eq(labels))\n",
    "\n",
    "            # Calculate DetectLLM-LLR\n",
    "            llr = self._calculate_log_likelihood_ratio(target_log_probs, target_ranks)\n",
    "\n",
    "            # Calculate Fast-DetectGPT (analytic)\n",
    "            fdg = self._calculate_fast_detect_gpt(\n",
    "                likelihood, log_likelihood, target_log_probs\n",
    "            )\n",
    "\n",
    "            results.append(Metrics(llr, fdg))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, Iterable[tuple[torch.Tensor]]]:\n",
    "        # Create `position_ids` on the fly, if required\n",
    "        # Source: https://github.com/huggingface/transformers/blob/v4.48.1/src/transformers/generation/utils.py#L414\n",
    "        position_ids = None\n",
    "        if self._requires_position_ids:\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs: _ModelOutput = self._model(\n",
    "                input_ids=input_ids.to(self.device),\n",
    "                attention_mask=attention_mask.to(self.device),\n",
    "                position_ids=position_ids.to(self.device),\n",
    "            )\n",
    "\n",
    "            likelihoods: torch.Tensor = outputs.logits.softmax(-1)\n",
    "            log_likelihoods: torch.Tensor = outputs.logits.log_softmax(-1)\n",
    "\n",
    "            del outputs\n",
    "        return (\n",
    "            likelihoods,\n",
    "            log_likelihoods,\n",
    "        )\n",
    "\n",
    "    def _calculate_log_likelihood_ratio(\n",
    "        self,\n",
    "        target_log_probs: torch.Tensor,\n",
    "        target_ranks: torch.Tensor,\n",
    "        device: torch.device = None,\n",
    "    ) -> float:\n",
    "        \"\"\"Implements the DetectLLM-LLR analytic criterion.\n",
    "\n",
    "        Args:\n",
    "            target_log_probs (torch.Tensor): A tensor of log probabilities for each target token.\n",
    "            target_ranks (torch.Tensor): A tensor of ranks for each target token.\n",
    "            device (torch.device, optional): Device to run the calculations on. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            float: The calculated log-likelihood ratio.\n",
    "\n",
    "        Source:\n",
    "            - Paper: https://aclanthology.org/2023.findings-emnlp.827.pdf\n",
    "            - GitHub: https://github.com/mbzuai-nlp/DetectLLM\n",
    "            - Implementation:\n",
    "                - https://github.com/mbzuai-nlp/DetectLLM/blob/main/baselines/all_baselines.py#L35:L42\n",
    "                - https://github.com/mbzuai-nlp/DetectLLM/blob/main/baselines/all_baselines.py#L94:L100\n",
    "        \"\"\"\n",
    "        device = device or self.device\n",
    "        return (\n",
    "            -torch.div(\n",
    "                target_log_probs.to(device).sum(),\n",
    "                target_ranks.to(device).log1p().sum(),\n",
    "            )\n",
    "            .cpu()\n",
    "            .item()\n",
    "        )\n",
    "\n",
    "    def _calculate_fast_detect_gpt(\n",
    "        self,\n",
    "        likelihood: torch.Tensor,\n",
    "        log_likelihood: torch.Tensor,\n",
    "        target_log_probs: torch.Tensor,\n",
    "        device: torch.device = None,\n",
    "    ) -> float:\n",
    "        \"\"\"Implements the Fast-DetectGPT analytic criterion.\n",
    "\n",
    "        Source:\n",
    "            - Paper: https://arxiv.org/abs/2310.05130\n",
    "            - GitHub: https://github.com/baoguangsheng/fast-detect-gpt\n",
    "            - Implementation: https://github.com/baoguangsheng/fast-detect-gpt/blob/main/scripts/fast_detect_gpt.py#L52:L70\n",
    "        \"\"\"\n",
    "        device = device or self.device\n",
    "        expectation = (likelihood.to(device) * log_likelihood.to(device)).sum(-1)\n",
    "        variance = (likelihood.to(device) * log_likelihood.to(device).square()).sum(\n",
    "            -1\n",
    "        ) - expectation.square()\n",
    "\n",
    "        fast_detect_gpt = (\n",
    "            target_log_probs.to(device).sum(-1) - expectation.sum(-1)\n",
    "        ) / variance.sum(-1).sqrt()\n",
    "\n",
    "        return fast_detect_gpt.cpu().item()\n",
    "\n",
    "\n",
    "def collate_fn(\n",
    "    batch: list[Sample],\n",
    "    pad_token_id: int,\n",
    ") -> list[Sample]:\n",
    "    input_ids, attention_mask = zip(\n",
    "        *[(sample[\"input_ids\"], sample[\"attention_mask\"]) for sample in batch]\n",
    "    )\n",
    "    input_ids = pad_sequence(\n",
    "        [torch.tensor(seq_ids) for seq_ids in input_ids],\n",
    "        batch_first=True,\n",
    "        padding_value=pad_token_id,\n",
    "    ).long()\n",
    "    attention_mask = pad_sequence(\n",
    "        [torch.tensor(mask) for mask in attention_mask],\n",
    "        batch_first=True,\n",
    "        padding_value=0,\n",
    "    ).long()\n",
    "    return input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MetricsCalculator(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dataset = [\n",
    "    tokenizer(sentence, add_special_tokens=True)\n",
    "    for sentence in [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Once upon a time, in a land far far away...\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Metrics(llr=1.5427662134170532, fdg=0.526492178440094),\n",
       " Metrics(llr=2.1564900875091553, fdg=1.4833513498306274)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.process(dataset, pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Here we collect our dataset for detecting AI-generated content. \n",
    "\n",
    "-------------------\n",
    "\n",
    "A list of possible domains to fetch content from:\n",
    "\n",
    "- Politics\n",
    "    - [German Bundestag](https://www.bundestag.de/) ✔\n",
    "    - [House of Commons](https://reshare.ukdataservice.ac.uk/854292/) ✔\n",
    "- Student/School\n",
    "    - [Kaggle Student Essays](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/data?select=train.csv) ✔\n",
    "    - Take the english dataset and translate it?\n",
    "- Research\n",
    "    - [Arxiv](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=language+model&terms-0-field=all&classification-physics_archives=all&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2005&date-to_date=2020&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first) ✔\n",
    "    - [I've seen a paper recently that creates full AI written papers. Maybe I can use it.](https://www.kaggle.com/discussions/general/527817#2958636)\n",
    "- News\n",
    "    - [Spiegel Online](https://www.spiegel.de/) ✔\n",
    "    - [CNN Articles](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail) ✔\n",
    "- Blogs/Tutorials/Forums\n",
    "- Law\n",
    "    - [German Open Legal Data](https://de.openlegaldata.io/) ✔\n",
    "    - [European Court of Human Rights Cases](https://www.kaggle.com/datasets/mathurinache/ecthrnaacl2021/data) ✔\n",
    "- Philosophy\n",
    "    - Gutenberg Project ([ENG](https://www.gutenberg.org/) | [GER](https://www.projekt-gutenberg.org/)) ✔\n",
    "- Literature\n",
    "    - Gutenberg Project ([ENG](https://www.gutenberg.org/) | [GER](https://www.projekt-gutenberg.org/)) ✔\n",
    "- Blogs, Food and Lifestyle\n",
    "    - [Food Blogs](https://detailed.com/food-blogs/)\n",
    "    - [WebBlogs](https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus?select=blogtext.csv) (ENG) ✔\n",
    "- Religion\n",
    "    - [Bible](https://github.com/mxw/grmr/blob/master/src/finaltests/bible.txt) (ENG|GER) ✔\n",
    "- Gaming\n",
    "\n",
    "Interesting Languages:\n",
    "\n",
    "- English\n",
    "- German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "from IPython.display import Javascript\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    SRC_ROOT_PATH = os.getenv('SRC_ROOT_PATH')\n",
    "    SRC_ROOT_PATH_COLL = os.getenv('SRC_ROOT_PATH_COLL')\n",
    "    DATA_ROOT_PATH = os.getenv('DATA_ROOT_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that it includes local imports. This is some next level python shit import\n",
    "sys.path.insert(0, CONFIG.SRC_ROOT_PATH)\n",
    "sys.path.insert(0, CONFIG.SRC_ROOT_PATH_COLL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collector\n",
    "import collected_item\n",
    "import collectors.bundestag_collector\n",
    "import collectors.house_of_commons_collector\n",
    "import collectors.student_essays_collector\n",
    "import collectors.arxiv_collector\n",
    "import collectors.spiegel_collector\n",
    "import collectors.cnn_news_collector\n",
    "import collectors.open_legal_data_collector\n",
    "import collectors.euro_court_cases_collector\n",
    "import collectors.religion_collector\n",
    "import collectors.gutenberg_collector\n",
    "import collectors.blog_corpus_collector\n",
    "import collectors.blog_corpus_collector\n",
    "\n",
    "import data_collector.agents.ai_agent\n",
    "import data_collector.agents.openai_agent\n",
    "\n",
    "def reload():\n",
    "    importlib.reload(collector)\n",
    "    importlib.reload(collected_item)\n",
    "    importlib.reload(collectors.bundestag_collector)\n",
    "    importlib.reload(collectors.house_of_commons_collector)\n",
    "    importlib.reload(collectors.student_essays_collector)\n",
    "    importlib.reload(collectors.arxiv_collector)\n",
    "    importlib.reload(collectors.spiegel_collector)\n",
    "    importlib.reload(collectors.cnn_news_collector)\n",
    "    importlib.reload(collectors.open_legal_data_collector)\n",
    "    importlib.reload(collectors.euro_court_cases_collector)\n",
    "    importlib.reload(collectors.religion_collector)\n",
    "    importlib.reload(collectors.gutenberg_collector)\n",
    "    importlib.reload(collectors.blog_corpus_collector)\n",
    "\n",
    "    importlib.reload(data_collector.agents.ai_agent)\n",
    "    importlib.reload(data_collector.agents.openai_agent)\n",
    "\n",
    "reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have cli arguments passed in. This is only done when this notebook is turned to a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI parsing failed - this is hence run as a notebook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--collectors COLLECTORS [COLLECTORS ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/kboenisc/.local/share/jupyter/runtime/kernel-7293b4eb-6ab8-4e8c-89c7-8b9e029b250a.json\n"
     ]
    }
   ],
   "source": [
    "run_as_script = False\n",
    "parser = argparse.ArgumentParser(description=\"Worker Script for the orchestrator.\")\n",
    "\n",
    "parser.add_argument(\"--collectors\", nargs=\"+\", help=\"Pass in the collectors of this instance. Only works of --script-mode=True\")\n",
    "\n",
    "try:\n",
    "    args = parser.parse_args()\n",
    "    print(args.collectors)\n",
    "    run_as_script = True\n",
    "except:\n",
    "    print('CLI parsing failed - this is hence run as a notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init the Collectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually defined collectors: ['BundestagCollector', 'HouseOfCommonsCollector', 'StudentEssaysCollector', 'ArxivCollector']\n"
     ]
    }
   ],
   "source": [
    "# If this is run as a script, that means the orcestrator passes in a list of collectors we are supposed to use.\n",
    "# Fill the list dynamically then. If its from within a notebook, just add them by hand.\n",
    "if run_as_script:\n",
    "    collection = []\n",
    "    for collector_name in args.collectors:\n",
    "        try:\n",
    "            module_name, class_name = collector_name.rsplit(\".\", 1)\n",
    "            # get the class\n",
    "            CollectorClass = getattr(eval(module_name), class_name)\n",
    "            # Instantiate the collector\n",
    "            collector_instance = CollectorClass(CONFIG.DATA_ROOT_PATH)\n",
    "            collection.append(collector_instance)\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error creating collector '{collector_name}': {e}\")\n",
    "else:\n",
    "    collection = [\n",
    "        collectors.bundestag_collector.BundestagCollector(CONFIG.DATA_ROOT_PATH),\n",
    "        collectors.house_of_commons_collector.HouseOfCommonsCollector(CONFIG.DATA_ROOT_PATH),\n",
    "        collectors.student_essays_collector.StudentEssaysCollector(CONFIG.DATA_ROOT_PATH),\n",
    "        collectors.arxiv_collector.ArxivCollector(CONFIG.DATA_ROOT_PATH),\n",
    "        #collectors.spiegel_collector.SpiegelCollector(CONFIG.DATA_ROOT_PATH),\n",
    "        #collectors.cnn_news_collector.CNNNewsCollector(CONFIG.DATA_ROOT_PATH),\n",
    "        #collectors.open_legal_data_collector.OpenLegalDataCollector(CONFIG.DATA_ROOT_PATH),\n",
    "        #collectors.euro_court_cases_collector.EuroCourtCasesCollector(CONFIG.DATA_ROOT_PATH),\n",
    "        #collectors.religion_collector.ReligionCollector(CONFIG.DATA_ROOT_PATH),\n",
    "        #collectors.gutenberg_collector.GutenbergCollector(CONFIG.DATA_ROOT_PATH),\n",
    "        #collectors.blog_corpus_collector.BlogCorpusCollector(CONFIG.DATA_ROOT_PATH)\n",
    "    ]\n",
    "\n",
    "# Check the loaded collection\n",
    "if run_as_script:\n",
    "    print(f'Dynamically created collectors: {[type(c).__name__ for c in collection]}')\n",
    "else:\n",
    "    print(f'Manually defined collectors: {[type(c).__name__ for c in collection]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ============== Collection started for BUNDESTAG ============== \n",
      "\n",
      "33949 speeches were already collected at 2024-12-22 16:07:26.476688, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for HOUSE OF COMMONS ============== \n",
      "\n",
      "53814 speeches were already collected at 2024-11-22 12:47:58.466366, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for STUDENT ESSAYS ============== \n",
      "\n",
      "115372 essays were already collected at 2024-11-22 13:02:19.983103, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for ARXIV PAPERS ============== \n",
      "\n",
      "9947 papers were already collected at 2024-11-23 15:24:25.733882, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ==================================================== \n",
      "\n",
      "\n",
      "All collectors finished. Total data items: 213082\n"
     ]
    }
   ],
   "source": [
    "total_items = 0\n",
    "\n",
    "for coll in collection:\n",
    "    try:\n",
    "        coll.init()\n",
    "        coll.collect()\n",
    "        total_items += coll.get_count()\n",
    "    except Exception as ex:\n",
    "        print('ERROR: Current collection failed due to an error: ')\n",
    "        print(ex)\n",
    "        print('\\n ***** Continuing with the other collectors. ***** \\n')\n",
    "\n",
    "print('\\n\\n ==================================================== \\n\\n')\n",
    "print(f'All collectors finished. Total data items: {total_items}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the AI Content\n",
    "\n",
    "After we've collected the dataset, we want to create it's AI-generated counterpart. We do this on multiple levels:\n",
    "\n",
    "- Inject passages of AI-generated content\n",
    "- Trying to rewrite the whole text as an AI agent.\n",
    "- Trying different models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [\n",
    "    data_collector.agents.openai_agent.OpenAIAgent(name='gpt-4o-mini', api_key=os.getenv('OPENAI_API_KEY'))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foreach agent, we go through all different collectors and synthesize the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_per_collector = 250\n",
    "force_synth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreach collector (so foreach domain)\n",
    "for coll in tqdm(collection, desc='Collectors', leave=False):\n",
    "    items_count = 0\n",
    "    df_count = 1\n",
    "    items_dfs = coll.get_collected_items_dfs()\n",
    "    collector_progress = tqdm(items_dfs, desc=f'Processing Collector Chunks of: {coll.get_folder_path().upper()}', leave=False)\n",
    "    \n",
    "    # We dont have a single huge dataframe file, but smaller chunks instead.\n",
    "    for df in collector_progress:\n",
    "        synth_items = []\n",
    "        stored_path = os.path.join(coll.get_synthesized_path(), f\"items_{df_count}.json\")\n",
    "        if not force_synth and os.path.exists(stored_path):\n",
    "            # print('Skipping this chunk as it is already synthesized and stored. Use force=True otherwise.')\n",
    "            df_count += 1\n",
    "            continue\n",
    "\n",
    "        if items_count >= take_per_collector:\n",
    "            break\n",
    "\n",
    "        for index, row in tqdm(df.iterrows(), desc=\"Current Chunk Items\", total=len(df), leave=False):\n",
    "            if items_count >= take_per_collector:\n",
    "                break\n",
    "            item = collected_item.CollectedItem.from_dict(row)\n",
    "\n",
    "            # We have multiple agents. Foreach agent, synthesize the item\n",
    "            for agent in agents:            \n",
    "                try:\n",
    "                    item.synthetization.append({\n",
    "                        'agent': agent.name,\n",
    "                        'synth_obj': agent.synthesize_collected_item(item, coll)\n",
    "                    })\n",
    "                    synth_items.append(item)\n",
    "                    items_count += 1\n",
    "                except Exception as ex:\n",
    "                    print(f'There was an error with collector {coll.get_folder_path()} from source df chunk {df_count}')\n",
    "                    print(ex)\n",
    "\n",
    "        # Save synthesized items of that collected chunk.\n",
    "        df = pd.DataFrame([item.__dict__ for item in synth_items])\n",
    "        df.to_json(stored_path)\n",
    "        df_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Here we collect our dataset for detecting AI-generated content. \n",
    "\n",
    "-------------------\n",
    "\n",
    "A list of possible domains to fetch content from:\n",
    "\n",
    "- Politics\n",
    "    - [German Bundestag](https://www.bundestag.de/) ✔\n",
    "    - [House of Commons](https://reshare.ukdataservice.ac.uk/854292/) ✔\n",
    "- Student/School\n",
    "    - [Kaggle Student Essays](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/data?select=train.csv) ✔\n",
    "    - Take the english dataset and translate it?\n",
    "- Research\n",
    "    - [Arxiv](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=language+model&terms-0-field=all&classification-physics_archives=all&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2005&date-to_date=2020&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first) ✔\n",
    "    - [I've seen a paper recently that creates full AI written papers. Maybe I can use it.](https://www.kaggle.com/discussions/general/527817#2958636)\n",
    "- News\n",
    "    - [Spiegel Online](https://www.spiegel.de/) ✔\n",
    "    - [CNN Articles](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail) ✔\n",
    "- Blogs/Tutorials/Forums\n",
    "- Law\n",
    "    - [German Open Legal Data](https://de.openlegaldata.io/) ✔\n",
    "    - [European Court of Human Rights Cases](https://www.kaggle.com/datasets/mathurinache/ecthrnaacl2021/data) ✔\n",
    "- Philosophy\n",
    "    - Gutenberg Project ([ENG](https://www.gutenberg.org/) | [GER](https://www.projekt-gutenberg.org/)) ✔\n",
    "- Literature\n",
    "    - Gutenberg Project ([ENG](https://www.gutenberg.org/) | [GER](https://www.projekt-gutenberg.org/)) ✔\n",
    "- Blogs, Food and Lifestyle\n",
    "    - [Food Blogs](https://detailed.com/food-blogs/)\n",
    "    - [WebBlogs](https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus?select=blogtext.csv) (ENG) ✔\n",
    "- Religion\n",
    "    - [Bible](https://github.com/mxw/grmr/blob/master/src/finaltests/bible.txt) (ENG|GER) ✔\n",
    "- Gaming\n",
    "\n",
    "Interesting Languages:\n",
    "\n",
    "- English\n",
    "- German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    SRC_ROOT_PATH = '/home/staff_homes/kboenisc/home/prismAI/PrismAI/src'\n",
    "    SRC_ROOT_PATH_COLL = '/home/staff_homes/kboenisc/home/prismAI/PrismAI/src/data_collector'\n",
    "    DATA_ROOT_PATH = '/storage/corpora/prismAI/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import queue\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# So that it includes local imports. This is some next level python shit import\n",
    "sys.path.insert(0, CONFIG.SRC_ROOT_PATH)\n",
    "sys.path.insert(0, CONFIG.SRC_ROOT_PATH_COLL)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collector\n",
    "import collected_item\n",
    "import collectors.bundestag_collector\n",
    "import collectors.house_of_commons_collector\n",
    "import collectors.student_essays_collector\n",
    "import collectors.arxiv_collector\n",
    "import collectors.spiegel_collector\n",
    "import collectors.cnn_news_collector\n",
    "import collectors.open_legal_data_collector\n",
    "import collectors.euro_court_cases_collector\n",
    "import collectors.religion_collector\n",
    "import collectors.gutenberg_collector\n",
    "import collectors.blog_corpus_collector\n",
    "import collectors.blog_corpus_collector\n",
    "\n",
    "import data_collector.agents.ai_agent\n",
    "import data_collector.agents.openai_agent\n",
    "\n",
    "def reload():\n",
    "    importlib.reload(collector)\n",
    "    importlib.reload(collected_item)\n",
    "    importlib.reload(collectors.bundestag_collector)\n",
    "    importlib.reload(collectors.house_of_commons_collector)\n",
    "    importlib.reload(collectors.student_essays_collector)\n",
    "    importlib.reload(collectors.arxiv_collector)\n",
    "    importlib.reload(collectors.spiegel_collector)\n",
    "    importlib.reload(collectors.cnn_news_collector)\n",
    "    importlib.reload(collectors.open_legal_data_collector)\n",
    "    importlib.reload(collectors.euro_court_cases_collector)\n",
    "    importlib.reload(collectors.religion_collector)\n",
    "    importlib.reload(collectors.gutenberg_collector)\n",
    "    importlib.reload(collectors.blog_corpus_collector)\n",
    "\n",
    "    importlib.reload(data_collector.agents.ai_agent)\n",
    "    importlib.reload(data_collector.agents.openai_agent)\n",
    "\n",
    "reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init the Collectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collection = [\n",
    "    collectors.bundestag_collector.BundestagCollector(CONFIG.DATA_ROOT_PATH),\n",
    "    collectors.house_of_commons_collector.HouseOfCommonsCollector(CONFIG.DATA_ROOT_PATH),\n",
    "    collectors.student_essays_collector.StudentEssaysCollector(CONFIG.DATA_ROOT_PATH),\n",
    "    collectors.arxiv_collector.ArxivCollector(CONFIG.DATA_ROOT_PATH),\n",
    "    collectors.spiegel_collector.SpiegelCollector(CONFIG.DATA_ROOT_PATH),\n",
    "    collectors.cnn_news_collector.CNNNewsCollector(CONFIG.DATA_ROOT_PATH),\n",
    "    #collectors.open_legal_data_collector.OpenLegalDataCollector(CONFIG.DATA_ROOT_PATH),\n",
    "    collectors.euro_court_cases_collector.EuroCourtCasesCollector(CONFIG.DATA_ROOT_PATH),\n",
    "    #collectors.religion_collector.ReligionCollector(CONFIG.DATA_ROOT_PATH),\n",
    "    collectors.gutenberg_collector.GutenbergCollector(CONFIG.DATA_ROOT_PATH),\n",
    "    collectors.blog_corpus_collector.BlogCorpusCollector(CONFIG.DATA_ROOT_PATH)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ============== Collection started for BUNDESTAG ============== \n",
      "\n",
      "33949 speeches were already collected at 2024-12-22 16:07:26.476688, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for HOUSE OF COMMONS ============== \n",
      "\n",
      "53814 speeches were already collected at 2024-11-22 12:47:58.466366, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for STUDENT ESSAYS ============== \n",
      "\n",
      "115372 essays were already collected at 2024-11-22 13:02:19.983103, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for ARXIV PAPERS ============== \n",
      "\n",
      "9947 papers were already collected at 2024-11-23 15:24:25.733882, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for SPIEGEL ARTICLES ============== \n",
      "\n",
      "105281 articles were already collected at 2024-11-23 05:43:32.662209, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for CNN NEWS ARTICLES ============== \n",
      "\n",
      "287226 news articles were already collected at 2024-11-24 18:53:06.344810, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for EURO COURT CASES ============== \n",
      "\n",
      "10100 cases were already collected at 2024-11-25 16:50:58.474519, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for GUTENBERG ============== \n",
      "\n",
      "28350 books were already collected at 2024-12-18 13:16:42.930970, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ============== Collection started for BLOG AUTHORSHIP CORPUS ============== \n",
      "\n",
      "200002 blogs were already collected at 2024-12-19 10:29:47.655031, hence we skip a redundant collection.\n",
      "If you'd like to collect anyway, set the force variable to True.\n",
      "\n",
      "\n",
      " ==================================================== \n",
      "\n",
      "\n",
      "All collectors finished. Total data items: 844041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/staff_homes/kboenisc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/staff_homes/kboenisc/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "total_items = 0\n",
    "\n",
    "for coll in collection:\n",
    "    try:\n",
    "        coll.init()\n",
    "        coll.collect()\n",
    "        total_items += coll.get_count()\n",
    "    except Exception as ex:\n",
    "        print('ERROR: Current collection failed due to an error: ')\n",
    "        print(ex)\n",
    "        print('\\n ***** Continuing with the other collectors. ***** \\n')\n",
    "\n",
    "print('\\n\\n ==================================================== \\n\\n')\n",
    "print(f'All collectors finished. Total data items: {total_items}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the AI Content\n",
    "\n",
    "After we've collected the dataset, we want to create it's AI-generated counterpart. We do this on multiple levels:\n",
    "\n",
    "- Inject passages of AI-generated content\n",
    "- Trying to rewrite the whole text as an AI agent.\n",
    "- Trying different models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [\n",
    "    data_collector.agents.openai_agent.OpenAIAgent(name='gpt-4o-mini', api_key=os.getenv('OPENAI_API_KEY'))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foreach agent, we go through all different collectors and synthesize the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_per_collector = 2000\n",
    "force_synth = True\n",
    "\n",
    "progress_queue = queue.Queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've never done parallelization in python before, but here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_collector(coll, agents, take_per_collector, force_synth=False):\n",
    "    '''\n",
    "    Processes a single collector, chunk by chunk, and sends progress updates\n",
    "    to the global progress_queue after each chunk.\n",
    "    '''\n",
    "    \n",
    "    items_dfs = coll.get_collected_items_dfs()\n",
    "    items_count = 0\n",
    "    df_count = 1\n",
    "    \n",
    "    for df in items_dfs:\n",
    "        stored_path = os.path.join(coll.get_synthesized_path(), f\"items_{df_count}.json\")\n",
    "        \n",
    "        # If chunk already exists & not forcing, skip it\n",
    "        if not force_synth and os.path.exists(stored_path):\n",
    "            df_count += 1\n",
    "            continue\n",
    "        \n",
    "        if items_count >= take_per_collector:\n",
    "            break\n",
    "        \n",
    "        synth_items = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            if items_count >= take_per_collector:\n",
    "                break\n",
    "            \n",
    "            # Build the item\n",
    "            item = collected_item.CollectedItem.from_dict(row)\n",
    "            \n",
    "            # For each agent, do the synth\n",
    "            for agent in agents:\n",
    "                try:\n",
    "                    item.synthetization.append({\n",
    "                        'agent': agent.name,\n",
    "                        'synth_obj': agent.synthesize_collected_item(item, coll),\n",
    "                    })\n",
    "                    synth_items.append(item)\n",
    "                    items_count += 1\n",
    "                except Exception as ex:\n",
    "                    print(f\"Error in collector {coll.get_folder_path()}, chunk {df_count}: {ex}\")\n",
    "        \n",
    "        # Save chunk\n",
    "        out_df = pd.DataFrame([item.__dict__ for item in synth_items])\n",
    "        out_df.to_json(stored_path)\n",
    "        df_count += 1\n",
    "        \n",
    "        # ---- SEND PROGRESS UPDATE ----\n",
    "        # e.g. \"I finished 1 chunk\"\n",
    "        progress_queue.put((coll.get_folder_path(), 1))\n",
    "\n",
    "    return coll.get_folder_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_progress(num_chunks_total):\n",
    "    '''\n",
    "    Listens on the queue for chunk updates. We track how many chunks have finished out of total, \n",
    "    and update a single progress bar (that's the plan at least).\n",
    "    '''\n",
    "    with tqdm(total=num_chunks_total, desc='All Chunks', position=0) as pbar:\n",
    "        chunks_done = 0\n",
    "        \n",
    "        while chunks_done < num_chunks_total:\n",
    "            # Block until an update arrives\n",
    "            folder_path, chunk_count = progress_queue.get()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(chunk_count)\n",
    "            chunks_done += chunk_count\n",
    "            pbar.set_postfix_str(f'Last update from: {folder_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac529b60c14a4a3abcb389a1855529d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All Chunks:   0%|          | 0/1954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collector blog_authorship_corpus failed: [Errno 28] No space left on device\n",
      "Collector bundestag failed: [Errno 28] No space left on device\n",
      "Collector gutenberg failed: [Errno 28] No space left on device\n",
      "Collector spiegel_articles failed: [Errno 28] No space left on device\n"
     ]
    }
   ],
   "source": [
    "# 1) Figure out how many total chunks exist across all collectors\n",
    "# so we know how many times we'll update the bar in total.\n",
    "num_chunks_total = 0\n",
    "for coll in collection:\n",
    "    num_chunks_total += len(coll.get_collected_items_dfs())\n",
    "\n",
    "# 2) Start the monitor thread that updates the tqdm bar in the main thread\n",
    "# (Maybe this is wasted effort, but I really like a real-time update)\n",
    "monitor_thread = threading.Thread(target=monitor_progress, args=(num_chunks_total,), daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "# 3) Launch parallel tasks\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    futures = {\n",
    "        executor.submit(process_one_collector, coll, agents, take_per_collector, force_synth): coll\n",
    "        for coll in collection\n",
    "    }\n",
    "\n",
    "    # wait for them to complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        coll = futures[future]\n",
    "        try:\n",
    "            res = future.result()\n",
    "        except Exception as ex:\n",
    "            print(f\"Collector {coll.get_folder_path()} failed: {ex}\")\n",
    "\n",
    "# 4) Now that all collectors are done, we know we won't get more queue updates.\n",
    "monitor_thread.join()\n",
    "\n",
    "print(\"All collectors & chunks are done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

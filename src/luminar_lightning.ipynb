{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26864b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from torch import Tensor, nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from luminar.classifier import LuminarClassifier\n",
    "from luminar.encoder import LuminarEncoder\n",
    "from luminar.utils import get_matched_datasets\n",
    "\n",
    "HF_TOKEN = (Path.home() / \".hf_token\").read_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae10567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from luminar.utils import PaddingDataloader\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    datset: Dataset | Iterable[dict[str, Tensor]],\n",
    "    feature_dim: tuple[int, ...],\n",
    "    batch_size: int = 32,\n",
    "    **kwargs,\n",
    ") -> DataLoader:\n",
    "    return PaddingDataloader(\n",
    "        dataset,\n",
    "        feature_dim=feature_dim,\n",
    "        batch_size=batch_size,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065414a",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf68ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = LuminarEncoder()\n",
    "# encoder.device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7e778",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = \"gpt_4o_mini\"\n",
    "config = {\n",
    "    # first 256 features & 13 layers for gpt2\n",
    "    \"feature_dim\": (256, 13),\n",
    "    \"feature_type\": \"intermediate_likelihoods\",\n",
    "    \"feature_selection\": \"first\",\n",
    "    \"projection_dim\": 32,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"warmup_steps\": 66,\n",
    "    \"max_epochs\": 25,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"batch_size\": 32,\n",
    "    \"seed\": 42,\n",
    "    \"agent\": agent,\n",
    "}\n",
    "feature_len = config[\"feature_dim\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: Dataset = (\n",
    "    load_dataset(\n",
    "        \"liberi-luminaris/PrismAI-encoded-gpt2\",\n",
    "        \"cnn_news-fulltext\",\n",
    "        split=f\"human+{agent}\",\n",
    "        token=HF_TOKEN,\n",
    "    )  # type: ignore\n",
    "    .map(\n",
    "        lambda features: {\"features\": features[:feature_len]},\n",
    "        input_columns=[\"features\"],\n",
    "        desc=\"Trimming Features\",\n",
    "    )\n",
    "    .rename_column(\"label\", \"labels\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebe055ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['agent', 'id_sample', 'id_source', 'labels', 'length', 'features'],\n",
       "        num_rows: 18902\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['agent', 'id_sample', 'id_source', 'labels', 'length', 'features'],\n",
       "        num_rows: 4726\n",
       "    })\n",
       "    test_unmatched: Dataset({\n",
       "        features: ['agent', 'id_sample', 'id_source', 'labels', 'length', 'features'],\n",
       "        num_rows: 6959\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_matched = get_matched_datasets(dataset, agent).with_format(\n",
    "    \"torch\", columns=[\"labels\", \"features\"]\n",
    ")\n",
    "datasets_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d638c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets_matched[\"train\"].train_test_split(\n",
    "    test_size=1 / 8,\n",
    "    shuffle=True,\n",
    "    seed=config[\"seed\"],\n",
    ").with_format(\n",
    "    \"torch\", columns=[\"labels\", \"features\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62850f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_val = get_dataloader(\n",
    "    dataset_train[\"test\"],\n",
    "    feature_dim=config[\"feature_dim\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    ")\n",
    "dataloader_test = get_dataloader(\n",
    "    datasets_matched[\"test\"],\n",
    "    feature_dim=config[\"feature_dim\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    ")\n",
    "dataloader_test_unmatched = get_dataloader(\n",
    "    datasets_matched[\"test_unmatched\"],\n",
    "    feature_dim=config[\"feature_dim\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2055ee25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "seed_everything(config[\"seed\"], verbose=False)\n",
    "classifier = LuminarClassifier(**config)\n",
    "trainer = Trainer(\n",
    "    max_epochs=config[\"max_epochs\"],\n",
    "    logger=pl_loggers.TensorBoardLogger(\n",
    "        save_dir=f\"logs/in_domain/{agent}\",\n",
    "        name=\"cnn_news\",\n",
    "    ),\n",
    "    gradient_clip_val=config[\"gradient_clip_val\"],\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)],\n",
    "    deterministic=True,\n",
    ")\n",
    "trainer.progress_bar_callback.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name        | Type              | Params | Mode  | In sizes      | Out sizes  \n",
      "----------------------------------------------------------------------------------------\n",
      "0 | conv_layers | Sequential        | 152 K  | train | [32, 13, 256] | [32, 16384]\n",
      "1 | projection  | Sequential        | 524 K  | train | [32, 16384]   | [32, 32]   \n",
      "2 | classifier  | Linear            | 33     | train | [32, 32]      | [32, 1]    \n",
      "3 | criterion   | BCEWithLogitsLoss | 0      | train | ?             | ?          \n",
      "----------------------------------------------------------------------------------------\n",
      "676 K     Trainable params\n",
      "0         Non-trainable params\n",
      "676 K     Total params\n",
      "2.706     Total estimated model params size (MB)\n",
      "17        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m dataloader_train \u001b[38;5;241m=\u001b[39m get_dataloader(\n\u001b[1;32m      2\u001b[0m     dataset_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m     feature_dim\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1054\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1083\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:138\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py:134\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py:61\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py:142\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_next_iterator()\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/src/luminar/utils.py:48\u001b[0m, in \u001b[0;36mPaddingDataloader._collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_collate_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m---> 48\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# In case we get a batch of sequences, that are all too short,\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# we need to pad them to the correct length as given by the feature_dim.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# - First dimension is the batch size.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# - Second dimension is the sequence length.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# - Third dimension is the feature dimension, if 2D features are used.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mmatch\u001b[39;00m features\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_dim:\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/utils/rnn.py:475\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value, padding_side)\u001b[0m\n\u001b[1;32m    471\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "dataloader_train = get_dataloader(\n",
    "    dataset_train[\"train\"].to_list(),\n",
    "    feature_dim=config[\"feature_dim\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "trainer.fit(\n",
    "    classifier,\n",
    "    dataloader_train,\n",
    "    dataloader_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.test(\n",
    "    classifier, (dataloader_test, dataloader_test_unmatched), verbose=False\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05efd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "scores = {}\n",
    "for config_name, dataset in tqdm(datasets_truncated.items(), desc=\"Training Models\"):\n",
    "    model = LuminarClassifier()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "    dataset_train = dataset[\"train\"].with_format(\"torch\", [\"features\", \"label\"])\n",
    "\n",
    "    tq = tqdm(range(3), desc=\"Training \" + config_name, leave=False)\n",
    "    for i in tq:\n",
    "        for batch in dataset_train.shuffle(i).batch(batch_size=32):\n",
    "            optimizer.zero_grad()\n",
    "            features = batch[\"features\"]\n",
    "            labels = batch[\"label\"].float().unsqueeze(-1)\n",
    "\n",
    "            preds = model(features)\n",
    "\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tq.set_postfix_str(f\"loss: {loss.item()}\")\n",
    "\n",
    "    model.eval()\n",
    "    scores[config_name] = evaluate(model, {config_name: dataset})[config_name]\n",
    "    ## OOD Evaluation\n",
    "    # scores[config_name] = evaluate(model, datasets_truncated)\n",
    "\n",
    "    print(config_name, json.dumps(scores[config_name], indent=4))\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea5390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542678a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d405f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bf89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b95ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(scores, indent=4))\n",
    "with open(\"../logs/luminar/gpt2_first_128-3_epochs.json\", \"w\") as f:\n",
    "    json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for subset in [\n",
    "    \"blog_authorship_corpus\",\n",
    "    \"student_essays\",\n",
    "    \"cnn_news\",\n",
    "    \"euro_court_cases\",\n",
    "    \"house_of_commons\",\n",
    "    \"arxiv_papers\",\n",
    "    \"gutenberg_en\",\n",
    "    \"en\",\n",
    "    \"bundestag\",\n",
    "    \"spiegel_articles\",\n",
    "    \"gutenberg_de\",\n",
    "    \"de\",\n",
    "]:\n",
    "    config_name = f\"{subset}-fulltext\"\n",
    "    datasets[config_name] = load_dataset(\n",
    "        \"liberi-luminaris/PrismAI-encoded-gpt2\",\n",
    "        config_name,\n",
    "        token=HF_TOKEN,\n",
    "        split=\"human+gpt_4o_mini\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"liberi-luminaris/PrismAI-fulltext\", \"cnn_news\", split=\"human+gpt_4o_mini\"\n",
    ")\n",
    "dataset_human = dataset.filter(lambda sample: sample[\"agent\"] == \"human\")\n",
    "source_ids = set(\n",
    "    dataset_human.shuffle(seed=42).take(len(dataset_human) // 10 * 8)[\"id_source\"]\n",
    ")\n",
    "dataset_train = dataset.filter(lambda sample: sample[\"id_source\"] in source_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec3e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_truncated = {}\n",
    "for config_name, dataset in datasets.items():\n",
    "    datasets_truncated[config_name] = dataset.with_format(\n",
    "        \"numpy\", columns=[\"features\"], output_all_columns=True\n",
    "    ).map(\n",
    "        lambda batch: {\"features\": batch[\"features\"][:, :256]},\n",
    "        batched=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18961af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_considered = {\n",
    "    key: value\n",
    "    for key, value in datasets_truncated.items()\n",
    "    if not key.startswith(\"de-\") and not key.startswith(\"en-\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2174dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "domains = [\n",
    "    \"Web Blogs\",\n",
    "    \"Essays\",\n",
    "    \"CNN\",\n",
    "    \"ECHR\",\n",
    "    \"HoC\",\n",
    "    \"arXiv\",\n",
    "    \"Gutenberg$_{en}$\",\n",
    "    \"Bundestag$_{de}$\",\n",
    "    \"Spiegel$_{de}$\",\n",
    "    \"Gutenberg$_{de}$\",\n",
    "    \"All$_{en}$\",\n",
    "    \"All$_{de}$\",\n",
    "]\n",
    "name_map = {\n",
    "    \"blog_authorship_corpus\": \"Web Blogs\",\n",
    "    \"student_essays\": \"Essays\",\n",
    "    \"cnn_news\": \"CNN\",\n",
    "    \"euro_court_cases\": \"ECHR\",\n",
    "    \"house_of_commons\": \"HoC\",\n",
    "    \"arxiv_papers\": \"arXiv\",\n",
    "    \"gutenberg_en\": \"Gutenberg$_{en}$\",\n",
    "    \"bundestag\": \"Bundestag$_{de}$\",\n",
    "    \"spiegel_articles\": \"Spiegel$_{de}$\",\n",
    "    \"gutenberg_de\": \"Gutenberg$_{de}$\",\n",
    "    \"en\": \"All$_{en}$\",\n",
    "    \"de\": \"All$_{de}$\",\n",
    "}\n",
    "\n",
    "results = [\n",
    "    {\"domain\": name_map[key.split(\"-\", 1)[0]]}\n",
    "    | {\n",
    "        \"f1\": value[\"f1\"],\n",
    "        \"acc\": value[\"accuracy\"],\n",
    "        \"auroc\": value[\"auroc\"],\n",
    "    }\n",
    "    for key, value in scores.items()\n",
    "]\n",
    "metric_df = (\n",
    "    pd.DataFrame(results)\n",
    "    .set_index(\"domain\")\n",
    "    .sort_index(key=lambda x: list(map(domains.index, x)))\n",
    ")\n",
    "print(metric_df.to_latex(float_format=\"%.3f\", index=True))\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e71e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_detector(\n",
    "#     detector: DetectorABC, datasets: dict[str, DatasetDict]\n",
    "# ) -> dict[str, float]:\n",
    "#     scores = {}\n",
    "#     for config_name, ds in tqdm(datasets.items(), desc=\"Predicting on Datasets\"):\n",
    "#         dataset: Dataset = ds[\"test\"].map(\n",
    "#             detector.tokenize,\n",
    "#             input_columns=[\"text\"],\n",
    "#             batched=True,\n",
    "#             batch_size=1024,\n",
    "#             desc=\"Tokenizing\",\n",
    "#         )\n",
    "#         dataset = dataset.sort(\"length\")\n",
    "#         dataset = dataset.map(\n",
    "#             detector.process,\n",
    "#             batched=True,\n",
    "#             batch_size=128,\n",
    "#             desc=\"Predicting\",\n",
    "#         )\n",
    "\n",
    "#         dataset_np = dataset.select_columns([\"prediction\", \"label\"]).with_format(\n",
    "#             \"numpy\"\n",
    "#         )\n",
    "\n",
    "#         acc, f1, auroc = get_scores(dataset_np[\"label\"], dataset_np[\"prediction\"])\n",
    "#         scores[config_name] = {\"accuracy\": acc, \"f1\": f1, \"auroc\": auroc}\n",
    "\n",
    "#         acc, f1, auroc = get_scores(\n",
    "#             dataset_np[\"label\"],\n",
    "#             dataset_np[\"prediction\"],\n",
    "#             calibrated=True,\n",
    "#         )\n",
    "#         scores[config_name] |= {\n",
    "#             \"accuracy_calibrated\": acc,\n",
    "#             \"f1_calibrated\": f1,\n",
    "#             \"auroc_calibrated\": auroc,\n",
    "#         }\n",
    "#     return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7958c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model: LuminarClassifier, datasets: dict[str, DatasetDict]) -> dict:\n",
    "#     scores = {}\n",
    "#     for config_name, dataset in tqdm(datasets.items(), desc=\"Evaluating\", leave=False):\n",
    "#         ds = (\n",
    "#             dataset[\"test\"]\n",
    "#             .with_format(\"torch\", [\"features\"])\n",
    "#             .map(model.process, batched=True, batch_size=32, desc=\"Predicting\")\n",
    "#         )\n",
    "#         dataset_np = ds.select_columns([\"prediction\", \"label\"]).with_format(\"numpy\")\n",
    "\n",
    "#         acc, f1, auroc = get_scores(dataset_np[\"label\"], dataset_np[\"prediction\"])\n",
    "#         scores[config_name] = {\n",
    "#             \"accuracy\": acc,\n",
    "#             \"f1\": f1,\n",
    "#             \"auroc\": auroc,\n",
    "#         }\n",
    "\n",
    "#         acc, f1, auroc = get_scores(\n",
    "#             dataset_np[\"label\"],\n",
    "#             dataset_np[\"prediction\"],\n",
    "#             calibrated=True,\n",
    "#         )\n",
    "#         scores[config_name] |= {\n",
    "#             \"accuracy_calibrated\": acc,\n",
    "#             \"f1_calibrated\": f1,\n",
    "#             \"auroc_calibrated\": auroc,\n",
    "#         }\n",
    "\n",
    "#     return scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c0300d",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Simple baselines for GhostWriter dataset (e.g. linear regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a8cb4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84320c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages (3.0.5)\n",
      "Requirement already satisfied: lightgbm in /home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: tabulate in /home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy in /home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: scipy in /home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages (from xgboost) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost lightgbm tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50487020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, string, numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Optional, Tuple\n",
    "from IPython.display import display, Markdown\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "from luminar.utils import get_best_device\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "rng = check_random_state(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dad88cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "_punct_tbl = str.maketrans(\"\", \"\", \"\")\n",
    "punct_set = set(string.punctuation)\n",
    "url_pat = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "sent_pat = re.compile(r\"[.!?]\")\n",
    "nonspace_pat = re.compile(r\"\\S\")\n",
    "alnum_pat = re.compile(r\"[A-Za-z0-9]\")\n",
    "\n",
    "def safe_div(a, b):\n",
    "    return a / b if b else 0.0\n",
    "\n",
    "def tokenize_whitespace_strip_punct(text):\n",
    "    toks = []\n",
    "    for tok in text.split():\n",
    "        # strip leading/trailing punctuation\n",
    "        tok = tok.strip(string.punctuation)\n",
    "        if tok and alnum_pat.search(tok):\n",
    "            toks.append(tok)\n",
    "    return toks\n",
    "\n",
    "def extract_metrics_batch(batch):\n",
    "    texts = batch[\"text\"]\n",
    "    out = {\n",
    "        \"n_chars\": [],\n",
    "        \"n_chars_nospace\": [],\n",
    "        \"n_words\": [],\n",
    "        \"avg_word_len\": [],\n",
    "        \"n_sents\": [],\n",
    "        \"n_punct\": [],\n",
    "        \"punct_ratio\": [],\n",
    "        \"upper_ratio\": [],\n",
    "        \"digit_ratio\": [],\n",
    "        \"url_count\": [],\n",
    "        \"type_token_ratio\": [],\n",
    "        \"hapax_ratio\": [],\n",
    "    }\n",
    "    for t in texts:\n",
    "        t = t if isinstance(t, str) else \"\"\n",
    "        n_chars = len(t)\n",
    "        n_chars_nospace = len(re.findall(r\"\\S\", t))\n",
    "        n_punct = sum(1 for ch in t if (not ch.isalnum()) and (not ch.isspace()))\n",
    "        n_upper = sum(1 for ch in t if ch.isupper())\n",
    "        n_digit = sum(1 for ch in t if ch.isdigit())\n",
    "        url_count = len(url_pat.findall(t))\n",
    "\n",
    "        # sentence count (min 1 for non-empty text)\n",
    "        sent_splits = [s for s in sent_pat.split(t) if s.strip()]\n",
    "        n_sents = max(1, len(sent_splits)) if t.strip() else 0\n",
    "\n",
    "        toks = tokenize_whitespace_strip_punct(t)\n",
    "        n_words = len(toks)\n",
    "        avg_word_len = safe_div(sum(len(w) for w in toks), n_words)\n",
    "\n",
    "        # lexical diversity\n",
    "        toks_lower = [w.lower() for w in toks]\n",
    "        vocab = set(toks_lower)\n",
    "        type_token_ratio = safe_div(len(vocab), n_words)\n",
    "\n",
    "        # hapax legomena ratio\n",
    "        from collections import Counter\n",
    "        cnt = Counter(toks_lower)\n",
    "        hapax_ratio = safe_div(sum(1 for w, c in cnt.items() if c == 1), n_words)\n",
    "\n",
    "        punct_ratio = safe_div(n_punct, n_chars_nospace)\n",
    "        upper_ratio = safe_div(n_upper, n_chars_nospace)\n",
    "        digit_ratio = safe_div(n_digit, n_chars_nospace)\n",
    "\n",
    "        out[\"n_chars\"].append(n_chars)\n",
    "        out[\"n_chars_nospace\"].append(n_chars_nospace)\n",
    "        out[\"n_words\"].append(n_words)\n",
    "        out[\"avg_word_len\"].append(avg_word_len)\n",
    "        out[\"n_sents\"].append(n_sents)\n",
    "        out[\"n_punct\"].append(n_punct)\n",
    "        out[\"punct_ratio\"].append(punct_ratio)\n",
    "        out[\"upper_ratio\"].append(upper_ratio)\n",
    "        out[\"digit_ratio\"].append(digit_ratio)\n",
    "        out[\"url_count\"].append(url_count)\n",
    "        out[\"type_token_ratio\"].append(type_token_ratio)\n",
    "        out[\"hapax_ratio\"].append(hapax_ratio)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01f81aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    \"n_chars\",\"n_chars_nospace\",\"n_words\",\"avg_word_len\",\"n_sents\",\"n_punct\",\n",
    "    \"punct_ratio\",\"upper_ratio\",\"digit_ratio\",\"url_count\",\"type_token_ratio\",\"hapax_ratio\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea4fc0",
   "metadata": {},
   "source": [
    "## Dataset & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "859a167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"TheItCrOw/GhostWriter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1198afc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'domain', 'date', 'source', 'lang', 'label', 'agent', 'type'],\n",
       "        num_rows: 382535\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['id', 'text', 'domain', 'date', 'source', 'lang', 'label', 'agent', 'type'],\n",
       "        num_rows: 54648\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'domain', 'date', 'source', 'lang', 'label', 'agent', 'type'],\n",
       "        num_rows: 109296\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d2f2f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty texts\n",
    "dataset01 = DatasetDict({\n",
    "    split: ds.filter(lambda ex: isinstance(ex[\"text\"], str) and ex[\"text\"].strip() != \"\")\n",
    "    for split, ds in dataset.items()\n",
    "})\n",
    "\n",
    "def clean(batch):\n",
    "    cleaned = []\n",
    "    for t in batch[\"text\"]:\n",
    "        # Collapse multiple spaces and trim\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "        cleaned.append(t)\n",
    "    return {\"text\": cleaned}\n",
    "\n",
    "dataset01 = DatasetDict({\n",
    "    split: ds.map(clean, batched=True, num_proc=32)\n",
    "    for split, ds in dataset01.items()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73ab8d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '41a1cb62-9edd-4aa2-8083-e26165fcc9c2',\n",
       " 'text': 'In today’s world, technology has woven itself into the very fabric of our daily lives, and its impact on understanding human emotions is both fascinating and concerning. As someone who has grown up with smartphones and social media, I can’t help but wonder how this tech shapes our ability to recognize genuine feelings, especially in educational settings. For instance, tools that analyze facial expressions or tone of voice can be incredibly useful for teachers trying to gauge student engagement. However, I often feel that relying too heavily on these technologies can lead to misunderstandings. Emotions are complex and nuanced; a smile doesn’t always mean happiness, and a frown doesn’t always signify sadness. By placing too much trust in algorithms, we risk oversimplifying the rich tapestry of human emotion. I believe that while technology can aid our understanding, it shouldn’t replace our instinctual ability to connect with others. After all, the most profound insights into feelings often come from genuine human interaction, not just data points on a screen. Balancing tech with empathy is key in nurturing emotional intelligence in our schools.',\n",
       " 'domain': 'student_essays',\n",
       " 'date': '2025',\n",
       " 'source': '9edd7547-57c4-4ef0-b33e-6a0f82d35499',\n",
       " 'lang': 'en-EN',\n",
       " 'label': 1,\n",
       " 'agent': 'gpt-4o-mini',\n",
       " 'type': 'fulltext',\n",
       " 'n_chars': 1161,\n",
       " 'n_chars_nospace': 981,\n",
       " 'n_words': 181,\n",
       " 'avg_word_len': 5.303867403314917,\n",
       " 'n_sents': 9,\n",
       " 'n_punct': 26,\n",
       " 'punct_ratio': 0.026503567787971458,\n",
       " 'upper_ratio': 0.011213047910295617,\n",
       " 'digit_ratio': 0.0,\n",
       " 'url_count': 0,\n",
       " 'type_token_ratio': 0.7348066298342542,\n",
       " 'hapax_ratio': 0.580110497237569}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extraction \n",
    "dataset_feats = DatasetDict({\n",
    "    split: ds.map(extract_metrics_batch, batched=True, num_proc=32)\n",
    "    for split, ds in dataset01.items()\n",
    "})\n",
    "dataset_feats['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "915de6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare arrays\n",
    "def to_xy(ds):\n",
    "    X = np.column_stack([np.array(ds[f], dtype=np.float32) for f in FEATURES])\n",
    "    y = np.array(ds[\"label\"], dtype=np.int64)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24eb9c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (382520, 12)  Test size: (109292, 12)\n",
      "Class balance (train): {0: 104303, 1: 151962, 2: 126255}\n",
      "Class balance (test): {0: 29801, 1: 43418, 2: 36073}\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = to_xy(dataset_feats[\"train\"])\n",
    "X_test,  y_test  = to_xy(dataset_feats[\"test\"])\n",
    "train_texts = dataset01[\"train\"][\"text\"]\n",
    "test_texts  = dataset01[\"test\"][\"text\"]\n",
    "\n",
    "print(\"Train size:\", X_train.shape, \" Test size:\", X_test.shape)\n",
    "print(\"Class balance (train):\", {c:int((y_train==c).sum()) for c in [0, 1, 2]})\n",
    "print(\"Class balance (test):\",  {c:int((y_test==c).sum()) for c in [0, 1, 2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e3510",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "721a089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "LABELS = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "024e7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_tpr_fpr(cm):\n",
    "    # We'll compute FPR_i via positives predicted for class i vs true negatives.\n",
    "    tp = np.diag(cm).astype(float)\n",
    "    row_sum = cm.sum(axis=1).astype(float)\n",
    "    col_sum = cm.sum(axis=0).astype(float)\n",
    "    total = cm.sum().astype(float)\n",
    "\n",
    "    tpr = np.divide(tp, np.maximum(row_sum, 1), out=np.zeros_like(tp), where=row_sum>0)\n",
    "\n",
    "    fp = col_sum - tp\n",
    "    tn = total - row_sum - col_sum + tp\n",
    "    denom = fp + tn\n",
    "    fpr = np.divide(fp, np.maximum(denom, 1), out=np.zeros_like(fp), where=denom>0)\n",
    "\n",
    "    return tpr, fpr\n",
    "\n",
    "def compute_metrics_multiclass(y_true, y_pred, y_proba):\n",
    "    \"\"\"\n",
    "    Computes macro F1, macro AUROC, per-class TPR/FPR, and macro TPR/FPR averages.\n",
    "    \"\"\"\n",
    "    f1_macro = f1_score(y_true, y_pred, labels=LABELS, average='macro', zero_division=0)\n",
    "\n",
    "    # AUROC (macro one-vs-rest)\n",
    "    uniq = np.unique(y_true)\n",
    "    if len(uniq) >= 2:\n",
    "        y_true_bin = label_binarize(y_true, classes=LABELS)\n",
    "        auroc_macro = roc_auc_score(y_true_bin, y_proba, average='macro', multi_class='ovr')\n",
    "    else:\n",
    "        auroc_macro = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=LABELS)\n",
    "    tpr, fpr = _safe_tpr_fpr(cm)\n",
    "\n",
    "    # Macro-averaged TPR/FPR\n",
    "    tpr_macro = float(np.mean(tpr))\n",
    "    fpr_macro = float(np.mean(fpr))\n",
    "\n",
    "    # Flatten per-class metrics into a dict\n",
    "    per_class = {}\n",
    "    for i, lab in enumerate(LABELS):\n",
    "        per_class[f\"TPR_{lab}\"] = float(tpr[i])\n",
    "        per_class[f\"FPR_{lab}\"] = float(fpr[i])\n",
    "\n",
    "    out = {\n",
    "        \"F1_macro\": float(f1_macro),\n",
    "        \"AUROC_macro\": float(auroc_macro),\n",
    "        \"TPR_macro\": tpr_macro,\n",
    "        \"FPR_macro\": fpr_macro,\n",
    "    }\n",
    "    out.update(per_class)\n",
    "    return out\n",
    "\n",
    "def evaluate_by_group(y_true, y_pred, y_proba, group_values, min_samples=50):\n",
    "    \"\"\"\n",
    "    Returns a pandas DataFrame with metrics per group.\n",
    "    - Skips AUROC for groups that contain <2 classes (sets AUROC_macro=nan)\n",
    "    - Drops groups with < min_samples rows\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "    group_values = np.asarray(group_values)\n",
    "\n",
    "    rows = []\n",
    "    for g in np.unique(group_values):\n",
    "        idx = np.where(group_values == g)[0]\n",
    "        if idx.size < min_samples:\n",
    "            continue\n",
    "\n",
    "        yt = y_true[idx]\n",
    "        yp = y_pred[idx]\n",
    "        probs = y_proba[idx]\n",
    "\n",
    "        metrics = compute_metrics_multiclass(yt, yp, probs)\n",
    "        # Also include size and per-class counts for context\n",
    "        counts = {f\"count_label_{lab}\": int((yt == lab).sum()) for lab in LABELS}\n",
    "        row = {\"group\": g, \"n\": int(idx.size)}\n",
    "        row.update(counts)\n",
    "        row.update(metrics)\n",
    "        rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"group\", \"n\"] + [f\"count_label_{l}\" for l in LABELS]\n",
    "                                      + [\"F1_macro\", \"AUROC_macro\"]\n",
    "                                      + [f\"TPR_{l}\" for l in LABELS] + [f\"FPR_{l}\" for l in LABELS])\n",
    "    df = pd.DataFrame(rows).sort_values(\"n\", ascending=False).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7d14cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_domains = np.array(dataset01[\"test\"][\"domain\"])\n",
    "test_agents  = np.array(dataset01[\"test\"][\"agent\"])\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba551f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression (multinomial)\n",
    "logreg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=1000, multi_class=\"multinomial\"))\n",
    "])\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_pred  = logreg.predict(X_test)\n",
    "logreg_proba = logreg.predict_proba(X_test)\n",
    "results[\"LogReg_overall\"] = compute_metrics_multiclass(y_test, logreg_pred, logreg_proba)\n",
    "logreg_by_domain = evaluate_by_group(y_test, logreg_pred, logreg_proba, test_domains, min_samples=50)\n",
    "logreg_by_agent  = evaluate_by_group(y_test, logreg_pred, logreg_proba, test_agents,  min_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca5d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# XGBoost (softmax)\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=NUM_CLASSES,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_proba = xgb.predict_proba(X_test)\n",
    "xgb_pred  = np.argmax(xgb_proba, axis=1)\n",
    "results[\"XGBoost_overall\"] = compute_metrics_multiclass(y_test, xgb_pred, xgb_proba)\n",
    "xgb_by_domain = evaluate_by_group(y_test, xgb_pred, xgb_proba, test_domains, min_samples=50)\n",
    "xgb_by_agent  = evaluate_by_group(y_test, xgb_pred, xgb_proba, test_agents,  min_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4e57e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2857\n",
      "[LightGBM] [Info] Number of data points in the train set: 382520, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -1.299481\n",
      "[LightGBM] [Info] Start training from score -0.923150\n",
      "[LightGBM] [Info] Start training from score -1.108477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# LightGBM (multiclass)\u001b[39;00m\n\u001b[32m      2\u001b[39m lgbm = LGBMClassifier(\n\u001b[32m      3\u001b[39m     n_estimators=\u001b[32m600\u001b[39m,\n\u001b[32m      4\u001b[39m     learning_rate=\u001b[32m0.05\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m,\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mlgbm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m lgbm_proba = lgbm.predict_proba(X_test)\n\u001b[32m     15\u001b[39m lgbm_pred  = np.argmax(lgbm_proba, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/luminar/lib/python3.12/site-packages/lightgbm/sklearn.py:1560\u001b[39m, in \u001b[36mLGBMClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1557\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1558\u001b[39m             valid_sets.append((valid_x, \u001b[38;5;28mself\u001b[39m._le.transform(valid_y)))\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/luminar/lib/python3.12/site-packages/lightgbm/sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/luminar/lib/python3.12/site-packages/lightgbm/engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/luminar/lib/python3.12/site-packages/lightgbm/basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# LightGBM (multiclass)\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"multiclass\",\n",
    "    num_class=NUM_CLASSES,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_proba = lgbm.predict_proba(X_test)\n",
    "lgbm_pred  = np.argmax(lgbm_proba, axis=1)\n",
    "results[\"LightGBM_overall\"] = compute_metrics_multiclass(y_test, lgbm_pred, lgbm_proba)\n",
    "lgbm_by_domain = evaluate_by_group(y_test, lgbm_pred, lgbm_proba, test_domains, min_samples=50)\n",
    "lgbm_by_agent  = evaluate_by_group(y_test, lgbm_pred, lgbm_proba, test_agents,  min_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6f88be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With permission, Mr. Speaker, I should like to make a personal statement. During the heat of debate, strong feelings are expressed on both sides of the House. I hope that, in my time here, I have always shown proper respect for the occupant of the Chair and observed his or her rulings. As you will be aware, Mr. Speaker, I was not asked by the Deputy Speaker in Westminster Hall yesterday to withdraw my remarks when they were made. However, on reflection, I accept that it would have been better if I had not used the phrase that I applied to the hon. Member for Glasgow, Kelvin , and I am sorry for the offence that was caused. With permission, Mr. Speaker, I should like to make a personal statement. In the debate in Westminster Hall yesterday, exchanges became frank to the point of being unacceptable, and I should like to apologise to the Deputy Speaker in Westminster Hall, my hon. Friend the Member for Blaydon , to you, Mr. Speaker, and to the House for my part in that. The issues under discussion were of grave urgency and importance, and they mean a very great deal to me. Exchanges on both sides of the argument were decidedly robust. None the less, I should like to say that I am sorry for stepping out of parliamentary order and for my failure to withdraw my remarks when asked to do so by the Deputy Speaker and now to withdraw them.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_list = []\n",
    "test_texts_list = []\n",
    "for t in train_texts:\n",
    "    train_texts_list.append(t)\n",
    "for t in test_texts:\n",
    "    test_texts_list.append(t)\n",
    "\n",
    "train_texts_list[0]\n",
    "test_texts_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b17184",
   "metadata": {},
   "source": [
    "Following taken from a kaggle challenge of mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7302ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers unchanged\n",
    "class _TokDataset(Dataset):\n",
    "    \"\"\"Pre-tokenized dataset (unsupervised) with optional progress for big inputs.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        tokenizer,\n",
    "        max_len: int,\n",
    "        show_progress: bool = False,\n",
    "        tokenize_batch_size: Optional[int] = None,\n",
    "    ):\n",
    "        if tokenize_batch_size and len(texts) > tokenize_batch_size:\n",
    "            pieces = []\n",
    "            rng = range(0, len(texts), tokenize_batch_size)\n",
    "            iterator = tqdm(rng, desc=\"Tokenizing\", unit=\"chunk\", total=len(rng)) if show_progress else rng\n",
    "            for start in iterator:\n",
    "                chunk = texts[start:start + tokenize_batch_size]\n",
    "                enc = tokenizer(\n",
    "                    chunk,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=max_len,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                pieces.append(enc)\n",
    "            keys = pieces[0].keys()\n",
    "            self.enc = {k: torch.cat([p[k] for p in pieces], dim=0) for k in keys}\n",
    "        else:\n",
    "            self.enc = tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.enc[\"input_ids\"].size(0)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return {k: v[idx] for k, v in self.enc.items()}\n",
    "\n",
    "\n",
    "class _TokClsDataset(_TokDataset):\n",
    "    \"\"\"Pre-tokenized dataset (supervised).\"\"\"\n",
    "    def __init__(self, texts: List[str], labels: np.ndarray, tokenizer, max_len: int,\n",
    "                 show_progress: bool = False, tokenize_batch_size: Optional[int] = None):\n",
    "        super().__init__(texts, tokenizer, max_len, show_progress, tokenize_batch_size)\n",
    "        self.labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = super().__getitem__(idx)\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "# Finetune head\n",
    "class _ClsHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_labels: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, pooled):\n",
    "        return self.out(self.dropout(pooled))\n",
    "\n",
    "\n",
    "class BERTClassifier:\n",
    "    \"\"\"\n",
    "    Dual-mode classifier:\n",
    "      - Frozen mode (default): Transformer as feature extractor -> sklearn LogisticRegression.\n",
    "      - Finetune mode: End-to-end training of transformer + torch classification head.\n",
    "\n",
    "    Public API:\n",
    "      - fit(texts, labels)\n",
    "      - predict_proba(texts)\n",
    "      - predict(texts)\n",
    "      - predict_proba_and_pred(texts)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"roberta-base\",\n",
    "        pooling: str = \"mean\",                # 'mean' or 'cls'\n",
    "        max_len: int = 512,\n",
    "        batch_size: int = 128,\n",
    "        use_scaler: bool = True,              # used in frozen mode only\n",
    "\n",
    "        # LogisticRegression params (frozen mode)\n",
    "        lr_max_iter: int = 1000,\n",
    "        lr_C: float = 1.0,\n",
    "        lr_n_jobs: int = -1,\n",
    "        lr_solver: str = \"lbfgs\",\n",
    "\n",
    "        random_state: int = 42,\n",
    "\n",
    "        # System / perf\n",
    "        device: Optional[str] = None,\n",
    "        num_workers: int = 4,\n",
    "        use_fp16: bool = True,                # IGNORED (kept for API compatibility)\n",
    "        allow_tf32: bool = True,\n",
    "\n",
    "        # Progress\n",
    "        show_progress: bool = True,\n",
    "        tokenize_batch_size: Optional[int] = 4096,\n",
    "\n",
    "        # --- Finetune controls ---\n",
    "        finetune: bool = False,               # True -> train transformer end-to-end\n",
    "        ft_epochs: int = 3,\n",
    "        ft_lr: float = 2e-5,\n",
    "        ft_weight_decay: float = 0.01,\n",
    "        ft_warmup_ratio: float = 0.06,\n",
    "        ft_max_grad_norm: float = 1.0,\n",
    "        ft_gradient_accumulation: int = 1,\n",
    "        ft_dropout: float = 0.1,\n",
    "    ):\n",
    "        # config\n",
    "        self.model_name = model_name\n",
    "        self.pooling = pooling.lower()\n",
    "        assert self.pooling in (\"mean\", \"cls\"), \"pooling must be 'mean' or 'cls'\"\n",
    "        self.max_len = int(max_len)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.use_scaler = use_scaler\n",
    "        self.random_state = random_state\n",
    "        self.num_workers = int(num_workers)\n",
    "        self.use_fp16 = False                 # hard-disable AMP/FP16\n",
    "        self.allow_tf32 = bool(allow_tf32)\n",
    "        self.show_progress = bool(show_progress)\n",
    "        self.tokenize_batch_size = tokenize_batch_size\n",
    "\n",
    "        self.finetune = bool(finetune)\n",
    "        self.ft_epochs = int(ft_epochs)\n",
    "        self.ft_lr = float(ft_lr)\n",
    "        self.ft_weight_decay = float(ft_weight_decay)\n",
    "        self.ft_warmup_ratio = float(ft_warmup_ratio)\n",
    "        self.ft_max_grad_norm = float(ft_max_grad_norm)\n",
    "        self.ft_gradient_accumulation = int(ft_gradient_accumulation)\n",
    "        self.ft_dropout = float(ft_dropout)\n",
    "\n",
    "        self.device = str(get_best_device())\n",
    "\n",
    "        # tokenizer & backbone\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.backbone.eval()\n",
    "\n",
    "        if self.device.startswith(\"cuda\") and self.allow_tf32:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "        # sklearn pipeline for frozen mode\n",
    "        self.clf: Optional[Pipeline] = None\n",
    "        if not self.finetune:\n",
    "            steps = []\n",
    "            if self.use_scaler:\n",
    "                steps.append((\"scaler\", StandardScaler(with_mean=True, with_std=True)))\n",
    "            steps.append((\"lr\", LogisticRegression(\n",
    "                max_iter=lr_max_iter,\n",
    "                multi_class=\"multinomial\",\n",
    "                n_jobs=lr_n_jobs,\n",
    "                C=lr_C,\n",
    "                solver=lr_solver,\n",
    "                random_state=random_state\n",
    "            )))\n",
    "            self.clf = Pipeline(steps)\n",
    "\n",
    "        # torch head for finetune mode (created lazily in fit once num_labels known)\n",
    "        self.num_labels: Optional[int] = None\n",
    "        self.ft_head: Optional[_ClsHead] = None\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _pool(self, last_hidden_state, attention_mask):\n",
    "        if self.pooling == \"mean\":\n",
    "            mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "            summed = (last_hidden_state * mask).sum(dim=1)\n",
    "            counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "            return summed / counts\n",
    "        else:\n",
    "            return last_hidden_state[:, 0, :]\n",
    "\n",
    "    # Frozen path: embed -> sklearn (FP32 only)\n",
    "    @torch.inference_mode()\n",
    "    def _embed_loader(self, loader: DataLoader) -> np.ndarray:\n",
    "        outs = []\n",
    "        iterator = tqdm(loader, total=len(loader), desc=\"Embedding\", unit=\"batch\") if self.show_progress else loader\n",
    "        for batch in iterator:\n",
    "            batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
    "            out = self.backbone(**batch).last_hidden_state\n",
    "            pooled = self._pool(out, batch[\"attention_mask\"])\n",
    "            outs.append(pooled.detach().cpu())\n",
    "        return torch.cat(outs, dim=0).to(dtype=torch.float32).numpy()\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        if len(texts) == 0:\n",
    "            h = getattr(self.backbone.config, \"hidden_size\", None) or 768\n",
    "            return np.zeros((0, h), dtype=np.float32)\n",
    "\n",
    "        ds = _TokDataset(\n",
    "            texts, self.tokenizer, self.max_len,\n",
    "            show_progress=self.show_progress,\n",
    "            tokenize_batch_size=self.tokenize_batch_size,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.device.startswith(\"cuda\"),\n",
    "            persistent_workers=(self.num_workers > 0),\n",
    "        )\n",
    "        return self._embed_loader(loader)\n",
    "\n",
    "    def fit(self, texts: List[str], labels: np.ndarray):\n",
    "        labels = np.asarray(labels)\n",
    "\n",
    "        if not self.finetune:\n",
    "            # Frozen mode: embed then sklearn\n",
    "            X = self.embed(texts)\n",
    "            assert self.clf is not None\n",
    "            self.clf.fit(X, labels)\n",
    "            return self\n",
    "\n",
    "        unique_labels = np.unique(labels)\n",
    "        self.num_labels = int(unique_labels.size)\n",
    "\n",
    "        ds = _TokClsDataset(\n",
    "            texts, labels,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_len=self.max_len,\n",
    "            show_progress=self.show_progress,\n",
    "            tokenize_batch_size=self.tokenize_batch_size,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.device.startswith(\"cuda\"),\n",
    "            persistent_workers=(self.num_workers > 0),\n",
    "        )\n",
    "\n",
    "        hidden_size = getattr(self.backbone.config, \"hidden_size\", 768)\n",
    "        self.ft_head = _ClsHead(hidden_size, self.num_labels, dropout=self.ft_dropout).to(self.device)\n",
    "\n",
    "        # Trainables: backbone + head\n",
    "        self.backbone.train()\n",
    "        self.ft_head.train()\n",
    "\n",
    "        # Ensure FP32 params\n",
    "        self.backbone.float()\n",
    "        self.ft_head.float()\n",
    "\n",
    "        # Optimizer & scheduler\n",
    "        optim = torch.optim.AdamW(\n",
    "            list(self.backbone.parameters()) + list(self.ft_head.parameters()),\n",
    "            lr=self.ft_lr,\n",
    "            weight_decay=self.ft_weight_decay,\n",
    "        )\n",
    "\n",
    "        steps_per_epoch = math.ceil(len(ds) / self.batch_size)\n",
    "        total_steps = (steps_per_epoch * self.ft_epochs) // max(1, self.ft_gradient_accumulation)\n",
    "        warmup_steps = int(self.ft_warmup_ratio * total_steps)\n",
    "        scheduler = get_linear_schedule_with_warmup(optim, warmup_steps, total_steps)\n",
    "\n",
    "        grad_accum = max(1, self.ft_gradient_accumulation)\n",
    "\n",
    "        for epoch in range(self.ft_epochs):\n",
    "            it = tqdm(loader, desc=f\"Finetuning (epoch {epoch+1}/{self.ft_epochs})\", unit=\"batch\") \\\n",
    "                if self.show_progress else loader\n",
    "\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "\n",
    "            for step, batch in enumerate(it, start=1):\n",
    "                labels_t = batch.pop(\"labels\").to(self.device, non_blocking=True)\n",
    "                batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "                out = self.backbone(**batch)\n",
    "                pooled = self._pool(out.last_hidden_state, batch[\"attention_mask\"])\n",
    "                logits = self.ft_head(pooled)\n",
    "                loss = self.criterion(logits, labels_t)\n",
    "                loss = loss / grad_accum  # normalize for accumulation\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                # Step optimizer/scheduler every grad_accum steps or at the end\n",
    "                if (step % grad_accum == 0) or (step == len(loader)):\n",
    "                    # Optional gradient clipping\n",
    "                    if self.ft_max_grad_norm and self.ft_max_grad_norm > 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            list(self.backbone.parameters()) + list(self.ft_head.parameters()),\n",
    "                            self.ft_max_grad_norm\n",
    "                        )\n",
    "                    optim.step()\n",
    "                    scheduler.step()\n",
    "                    optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                if self.show_progress and hasattr(it, \"set_postfix\"):\n",
    "                    it.set_postfix(loss=float(loss.detach().cpu()) * grad_accum)\n",
    "\n",
    "        # back to eval for inference\n",
    "        self.backbone.eval()\n",
    "        self.ft_head.eval()\n",
    "        return self\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def _infer_logits(self, texts: List[str]) -> torch.Tensor:\n",
    "        assert self.finetune and self.ft_head is not None, \"Only for finetune mode.\"\n",
    "        if len(texts) == 0:\n",
    "            return torch.zeros((0, self.num_labels), dtype=torch.float32)\n",
    "\n",
    "        ds = _TokDataset(\n",
    "            texts, self.tokenizer, self.max_len,\n",
    "            show_progress=self.show_progress,\n",
    "            tokenize_batch_size=self.tokenize_batch_size,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.device.startswith(\"cuda\"),\n",
    "            persistent_workers=(self.num_workers > 0),\n",
    "        )\n",
    "\n",
    "        iterator = tqdm(loader, total=len(loader), desc=\"Infer (finetune)\", unit=\"batch\") \\\n",
    "                   if self.show_progress else loader\n",
    "\n",
    "        all_logits = []\n",
    "        for batch in iterator:\n",
    "            batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
    "            last_hid = self.backbone(**batch).last_hidden_state\n",
    "            pooled = self._pool(last_hid, batch[\"attention_mask\"])\n",
    "            logits = self.ft_head(pooled)\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "        return torch.cat(all_logits, dim=0).to(dtype=torch.float32)\n",
    "\n",
    "    def predict_proba(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.finetune:\n",
    "            X = self.embed(texts)\n",
    "            assert self.clf is not None\n",
    "            return self.clf.predict_proba(X)\n",
    "        logits = self._infer_logits(texts)\n",
    "        proba = torch.softmax(logits, dim=-1).numpy()\n",
    "        return proba\n",
    "\n",
    "    def predict(self, texts: List[str]) -> np.ndarray:\n",
    "        return self.predict_proba(texts).argmax(axis=1)\n",
    "\n",
    "    def predict_proba_and_pred(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        proba = self.predict_proba(texts)\n",
    "        pred = proba.argmax(axis=1)\n",
    "        return proba, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71255178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c5ba851e224748b8935ceed873f7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/94 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0ae4bf25614187828a8d816c53ad5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finetuning (epoch 1/5):   0%|          | 0/2989 [00:08<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a18a77441f54cd2bcdea601f7ad238e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finetuning (epoch 2/5):   0%|          | 0/2989 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbb45bb7390411c997cfb3275faafa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finetuning (epoch 3/5):   0%|          | 0/2989 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7e3f696bcd4dfb8063766cf0ed6ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finetuning (epoch 4/5):   0%|          | 0/2989 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba4861a5e1741f89b7a6e1193bc810b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finetuning (epoch 5/5):   0%|          | 0/2989 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb22478191849a39725ac71fcdc6a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/27 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6ac55d6faf4c14851b752175b9849c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Infer (finetune):   0%|          | 0/854 [00:07<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Finetune\n",
    "clf = BERTClassifier(\n",
    "    model_name=\"xlm-roberta-base\",\n",
    "    pooling=\"mean\",\n",
    "    finetune=True,\n",
    "    ft_epochs=5,\n",
    "    ft_lr=2e-5,\n",
    "    batch_size=128,\n",
    "    show_progress=True\n",
    ")\n",
    "clf.fit(train_texts, y_train)\n",
    "proba, pred = clf.predict_proba_and_pred(test_texts)\n",
    "results[\"RoBERTa+Finetune_overall\"] = compute_metrics_multiclass(y_test, pred, proba)\n",
    "ro_ft_by_domain = evaluate_by_group(y_test, pred, proba, test_domains, min_samples=50)\n",
    "ro_ft_by_agent  = evaluate_by_group(y_test, pred, proba, test_agents,  min_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "580a36ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890e90322fe549968067bf902832256b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/94 [00:00<?, ?texts/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193b4d54906e4068a261964e3eb988d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding:   0%|          | 0/748 [00:05<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2133546/4189222808.py:132: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=amp_enabled):\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f205ef4aacc649c3a17fe39efa134cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/27 [00:00<?, ?texts/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403a23094df04e559dfad35a4fd8ef71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding:   0%|          | 0/214 [00:05<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2133546/4189222808.py:132: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=amp_enabled):\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/luminar/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Frozen\n",
    "ro_baseline = BERTClassifier(\n",
    "    model_name=\"xlm-roberta-base\",\n",
    "    pooling=\"mean\",\n",
    "    max_len=512,\n",
    "    batch_size=512,\n",
    "    finetune=False\n",
    ")\n",
    "ro_baseline.fit(train_texts_list, y_train)\n",
    "ro_proba, ro_pred = ro_baseline.predict_proba_and_pred(test_texts_list)\n",
    "results[\"RoBERTa+LR_overall\"] = compute_metrics_multiclass(y_test, ro_pred, ro_proba)\n",
    "ro_by_domain = evaluate_by_group(y_test, ro_pred, ro_proba, test_domains, min_samples=50)\n",
    "ro_by_agent  = evaluate_by_group(y_test, ro_pred, ro_proba, test_agents,  min_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c45e061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_to_markdown(df, title=None, max_rows=15, floatfmt=\".3f\"):\n",
    "    df_ = df.copy()\n",
    "    for col in df_.select_dtypes(include=[float]).columns:\n",
    "        df_[col] = df_[col].map(lambda x: f\"{x:{floatfmt}}\" if pd.notnull(x) else \"\")\n",
    "    md = df_.head(max_rows).to_markdown(index=False)\n",
    "    if title:\n",
    "        display(Markdown(f\"### {title}\\n\\n{md}\"))\n",
    "    else:\n",
    "        display(Markdown(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc665748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Overall Model Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Overall Results (sorted by F1_macro)\n",
       "\n",
       "| Model                    |   F1_macro |   AUROC_macro |   TPR_macro |   FPR_macro |   TPR_0 |   FPR_0 |   TPR_1 |   FPR_1 |   TPR_2 |   FPR_2 |\n",
       "|:-------------------------|-----------:|--------------:|------------:|------------:|--------:|--------:|--------:|--------:|--------:|--------:|\n",
       "| RoBERTa+Finetune_overall |      0.892 |         0.983 |       0.891 |       0.047 |   0.797 |   0.053 |   0.997 |   0.006 |   0.879 |   0.081 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Per-Domain Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RoBERTa+Finetune — Per Domain\n",
       "\n",
       "| group                  |     n |   count_label_0 |   count_label_1 |   count_label_2 |   F1_macro |   AUROC_macro |   TPR_macro |   FPR_macro |   TPR_0 |   FPR_0 |   TPR_1 |   FPR_1 |   TPR_2 |   FPR_2 |\n",
       "|:-----------------------|------:|----------------:|----------------:|----------------:|-----------:|--------------:|------------:|------------:|--------:|--------:|--------:|--------:|--------:|--------:|\n",
       "| student_essays         | 29044 |            8451 |           10736 |            9857 |      0.965 |         0.997 |       0.965 |       0.016 |   0.945 |   0.023 |   0.999 |   0.001 |   0.951 |   0.024 |\n",
       "| spiegel_articles       | 14164 |            3294 |            5579 |            5291 |      0.861 |         0.977 |       0.858 |       0.059 |   0.714 |   0.06  |   1     |   0.014 |   0.861 |   0.103 |\n",
       "| bundestag              | 13117 |            3726 |            5012 |            4379 |      0.853 |         0.959 |       0.857 |       0.065 |   0.848 |   0.127 |   0.997 |   0.004 |   0.725 |   0.064 |\n",
       "| cnn_news               | 12362 |            3773 |            5485 |            3104 |      0.956 |         0.996 |       0.956 |       0.017 |   0.937 |   0.024 |   1     |   0.001 |   0.932 |   0.025 |\n",
       "| blog_authorship_corpus | 12270 |            4032 |            4754 |            3484 |      0.892 |         0.983 |       0.894 |       0.049 |   0.813 |   0.048 |   0.985 |   0.022 |   0.883 |   0.077 |\n",
       "| house_of_commons       | 10001 |            2590 |            4052 |            3359 |      0.834 |         0.963 |       0.833 |       0.069 |   0.678 |   0.081 |   1     |   0.003 |   0.82  |   0.124 |\n",
       "| arxiv_papers           |  7844 |            1628 |            3113 |            3103 |      0.596 |         0.869 |       0.663 |       0.117 |   0.003 |   0.003 |   0.997 |   0.003 |   0.989 |   0.344 |\n",
       "| euro_court_cases       |  7435 |            1829 |            2811 |            2795 |      0.81  |         0.951 |       0.811 |       0.083 |   0.669 |   0.117 |   1     |   0     |   0.765 |   0.13  |\n",
       "| gutenberg              |  3055 |             478 |            1876 |             701 |      0.959 |         0.996 |       0.958 |       0.013 |   0.941 |   0.013 |   0.996 |   0.014 |   0.936 |   0.013 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Per-Agent Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### RoBERTa+Finetune — Per Agent\n",
       "\n",
       "| group            |     n |   count_label_0 |   count_label_1 |   count_label_2 |   F1_macro | AUROC_macro   |   TPR_macro |   FPR_macro |   TPR_0 |   FPR_0 |   TPR_1 |   FPR_1 |   TPR_2 |   FPR_2 |\n",
       "|:-----------------|------:|----------------:|----------------:|----------------:|-----------:|:--------------|------------:|------------:|--------:|--------:|--------:|--------:|--------:|--------:|\n",
       "| gemma2:9b        | 40289 |               0 |           21645 |           18644 |      0.639 |               |       0.618 |       0.024 |   0     |   0.063 |   0.996 |   0.006 |   0.859 |   0.002 |\n",
       "| gpt-4o-mini      | 33489 |               0 |           18627 |           14862 |      0.649 |               |       0.635 |       0.015 |   0     |   0.041 |   1     |   0.003 |   0.904 |   0     |\n",
       "| human            | 29801 |           29801 |               0 |               0 |      0.296 |               |       0.266 |       0.068 |   0.797 |   0     |   0     |   0.006 |   0     |   0.197 |\n",
       "| phi3:3.8b        |  2601 |               0 |            1402 |            1199 |      0.64  |               |       0.623 |       0.024 |   0     |   0.05  |   0.991 |   0.019 |   0.878 |   0.004 |\n",
       "| nemotron         |   920 |               0 |             560 |             360 |      0.654 |               |       0.642 |       0.01  |   0     |   0.028 |   0.996 |   0     |   0.931 |   0.002 |\n",
       "| deepseek-r1:1.5b |   899 |               0 |             500 |             399 |      0.637 |               |       0.62  |       0.026 |   0     |   0.049 |   0.99  |   0.023 |   0.87  |   0.008 |\n",
       "| deepseek-r1:32b  |   633 |               0 |             326 |             307 |      0.641 |               |       0.623 |       0.023 |   0     |   0.057 |   0.997 |   0.01  |   0.873 |   0.003 |\n",
       "| o3-mini          |   474 |               0 |             259 |             215 |      0.608 |               |       0.57  |       0.047 |   0     |   0.127 |   0.992 |   0.009 |   0.716 |   0.004 |\n",
       "| gpt-4-turbo      |   186 |               0 |              99 |              87 |      0.653 |               |       0.64  |       0.013 |   0     |   0.038 |   1     |   0     |   0.92  |   0     |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"## Overall Model Results\"))\n",
    "display(Markdown(\"---\"))\n",
    "\n",
    "overall_df = pd.DataFrame([\n",
    "    {\"Model\": model, **metrics} for model, metrics in results.items()\n",
    "])\n",
    "df_to_markdown(overall_df.sort_values(\"F1_macro\", ascending=False).reset_index(drop=True),\n",
    "               title=\"Overall Results (sorted by F1_macro)\")\n",
    "\n",
    "display(Markdown(\"## Per-Domain Results\"))\n",
    "display(Markdown(\"---\"))\n",
    "df_to_markdown(logreg_by_domain,  title=\"Logistic Regression — Per Domain\")\n",
    "df_to_markdown(xgb_by_domain,     title=\"XGBoost — Per Domain\")\n",
    "df_to_markdown(lgbm_by_domain,    title=\"LightGBM — Per Domain\")\n",
    "df_to_markdown(ro_by_domain,      title=\"RoBERTa+LR — Per Domain\")\n",
    "df_to_markdown(ro_ft_by_domain,      title=\"RoBERTa+Finetune — Per Domain\")\n",
    "\n",
    "display(Markdown(\"## Per-Agent Results\"))\n",
    "display(Markdown(\"---\"))\n",
    "df_to_markdown(logreg_by_agent,  title=\"Logistic Regression — Per Agent\")\n",
    "df_to_markdown(xgb_by_agent,     title=\"XGBoost — Per Agent\")\n",
    "df_to_markdown(lgbm_by_agent,    title=\"LightGBM — Per Agent\")\n",
    "df_to_markdown(ro_by_agent,      title=\"RoBERTa+LR — Per Agent\")\n",
    "df_to_markdown(ro_ft_by_agent,      title=\"RoBERTa+Finetune — Per Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b2efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

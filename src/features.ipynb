{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pymongo\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv(\"../env\")\n",
    "client = MongoClient(os.environ.get(\"MONGO_DB_CONNECTION\"))\n",
    "db = client.get_database(\"prismai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_transition_scores = db.get_collection(\"test_swt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_dataset import Dataset\n",
    "from transition_scores.data import FeaturesDict, TransitionScores\n",
    "\n",
    "dataset = Dataset(\n",
    "    FeaturesDict.new(**document) for document in collection_transition_scores.find()\n",
    ")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply(TransitionScores.merge, \"transition_scores\")\n",
    "type(dataset[0][\"transition_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataset = dataset.map(\n",
    "    lambda doc: {\"doc_id\": doc[\"document\"][\"_id\"]} | doc,\n",
    "    in_place=False,\n",
    ").group_documents_by(\n",
    "    \"doc_id\",\n",
    "    remainder_into=\"documents\",\n",
    "    in_place=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(group[\"documents\"]) for group in grouped_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oracle_ts(\n",
    "    seq_len=1024,\n",
    "    vocab_size=128,\n",
    "    top_k=10,\n",
    ") -> TransitionScores:\n",
    "    target_ids = np.random.choice(vocab_size, seq_len, replace=True).tolist()\n",
    "    target_probs = (np.arange(seq_len) / seq_len).tolist()\n",
    "    target_ranks = np.random.choice(vocab_size, seq_len, replace=True).tolist()\n",
    "    top_k_ids = [\n",
    "        np.random.choice(vocab_size, top_k, replace=False) for _ in range(seq_len)\n",
    "    ]\n",
    "    top_k_probs = [\n",
    "        list(sorted((np.random.rand(top_k) / 10).tolist(), reverse=True))\n",
    "        for _ in range(seq_len)\n",
    "    ]\n",
    "    return TransitionScores(\n",
    "        target_ids,\n",
    "        target_probs,\n",
    "        target_ranks,\n",
    "        top_k_ids,\n",
    "        top_k_probs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz2d(features: np.array, vmin=None, vmax=None):\n",
    "    fig = plt.imshow(features, cmap=\"gray\", vmin=vmin, vmax=vmax)\n",
    "    fig.axes.set_axis_off()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2d_likelihood(\n",
    "    transition_scores: TransitionScores,\n",
    "    w: int,\n",
    "    h: int,\n",
    "    skip: int = 1,\n",
    "    no_overlap: bool = False,\n",
    "    take_first: bool = False,\n",
    "    sort_slices: bool = True,\n",
    "):\n",
    "    if no_overlap:\n",
    "        raise NotImplementedError(\"no_overlap not implemented\")\n",
    "\n",
    "    size = len(transition_scores) - skip - w\n",
    "    if size < w:\n",
    "        raise ValueError(\n",
    "            f\"Sequence is too short: ({len(transition_scores)} - {skip} - {w}) < {w}\"\n",
    "        )\n",
    "\n",
    "    if take_first:\n",
    "        offsets = np.arange(skip, size, w)[:h]\n",
    "        if not sort_slices:\n",
    "            offsets = np.random.permutation(offsets)\n",
    "    else:\n",
    "        offsets = np.random.choice(size, h, replace=False) + skip\n",
    "        if sort_slices:\n",
    "            offsets = np.sort(offsets)\n",
    "\n",
    "    features = np.zeros((h, w))\n",
    "    for i, offset in enumerate(offsets):\n",
    "        features[i] = transition_scores.target_probs[offset : offset + w]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_2d_likelihood(dataset[3][\"transition_scores\"], 32, 16, take_first=True)\n",
    "viz2d(features)\n",
    "plt.show()\n",
    "\n",
    "features = features_2d_likelihood(dataset[-1][\"transition_scores\"], 32, 16, take_first=True)\n",
    "viz2d(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz3d(features: np.array, vmin=None, vmax=None):\n",
    "    vmin = vmin or features.min()\n",
    "    vmax = vmax or features.max()\n",
    "\n",
    "    h, w, d = features.shape\n",
    "    d = int(np.ceil(np.sqrt(d)))\n",
    "    fig, axes = plt.subplots(d, d, figsize=(5, h / w * 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < features.shape[-1]:\n",
    "            ax.imshow(features[..., i], cmap=\"gray\", vmin=vmin, vmax=vmax)\n",
    "        ax.set_axis_off()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "\n",
    "def features_3d_liklihood_topk_stack(\n",
    "    transition_scores: TransitionScores,\n",
    "    w: int,\n",
    "    h: int,\n",
    "    d: int,\n",
    "    skip: int = 1,\n",
    "    take_first: bool = False,\n",
    "    no_overlap: bool = False,\n",
    "    remove_target: bool = True,\n",
    "    sort_slices: bool = True,\n",
    "    log: bool = False,\n",
    "):\n",
    "    if no_overlap:\n",
    "        raise NotImplementedError(\"no_overlap not implemented\")\n",
    "\n",
    "    size = len(transition_scores) - skip - w\n",
    "    if size < w:\n",
    "        raise ValueError(\n",
    "            f\"Sequence is too short: ({len(transition_scores)} - {skip} - {w}) < {w}\"\n",
    "        )\n",
    "    if size < w * h:\n",
    "        warn(\n",
    "            f\"Sequence very short, will produce overlapping offsets: ({len(transition_scores)} - {skip} - {w}) = {size} < {w * h}\"\n",
    "        )\n",
    "    ts = transition_scores[skip:]\n",
    "\n",
    "    if take_first:\n",
    "        offsets = np.arange(0, size, w)[:h]\n",
    "        if not sort_slices:\n",
    "            offsets = np.random.permutation(offsets)\n",
    "    else:\n",
    "        offsets = np.random.choice(size, h, replace=False) + skip\n",
    "        if sort_slices:\n",
    "            offsets = np.sort(offsets)\n",
    "\n",
    "    top_k_probs = [\n",
    "        list(\n",
    "            islice(\n",
    "                (\n",
    "                    prob\n",
    "                    for top_id, prob in zip(top_ids, top_probs)\n",
    "                    if not remove_target or top_id != tgt_id\n",
    "                ),\n",
    "                d - 1,\n",
    "            )\n",
    "        )\n",
    "        for tgt_id, top_ids, top_probs in zip(\n",
    "            ts.target_ids,\n",
    "            ts.top_k_indices,\n",
    "            ts.top_k_probs,\n",
    "        )\n",
    "    ]\n",
    "    top_k_probs = np.array(top_k_probs)\n",
    "\n",
    "    features = np.empty((h, w, d))\n",
    "    for i, offset in enumerate(offsets):\n",
    "        features[i, :, 0] = ts.target_probs[offset : offset + w]\n",
    "        features[i, :, 1:] = top_k_probs[offset : offset + w]  # [:, ::-1]\n",
    "    if log:\n",
    "        features = np.log1p(features)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = get_oracle_ts()\n",
    "features = features_3d_liklihood_topk_stack(ts, 16, 16, 4, take_first=True)\n",
    "viz3d(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_3d_liklihood_topk_stack(\n",
    "    dataset[3][\"transition_scores\"], 32, 16, 9, take_first=True,\n",
    "    log=\"log_ratio\"\n",
    ")\n",
    "viz3d(features)\n",
    "plt.show()\n",
    "\n",
    "mn, mx = features.min(), features.max()\n",
    "\n",
    "features = features_3d_liklihood_topk_stack(\n",
    "    dataset[-1][\"transition_scores\"], 32, 16, 9, take_first=True,\n",
    "    log=\"log_ratio\"\n",
    ")\n",
    "viz3d(features, vmin=mn, vmax=mx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def features_3d_likelihood_top_k_ratio(\n",
    "    transition_scores: TransitionScores,\n",
    "    w: int,\n",
    "    h: int,\n",
    "    d: int,\n",
    "    skip: int = 1,\n",
    "    take_first: bool = False,\n",
    "    no_overlap: bool = False,\n",
    "    sort_slices: bool = True,\n",
    "    log: None | Literal[\"log_ratio\", \"ratio_of_logs\"] = None,\n",
    "):\n",
    "    if no_overlap:\n",
    "        raise NotImplementedError(\"no_overlap not implemented\")\n",
    "\n",
    "    size = len(transition_scores) - skip - w\n",
    "    if size < w:\n",
    "        raise ValueError(\n",
    "            f\"Sequence is too short: ({len(transition_scores)} - {skip} - {w}) < {w}\"\n",
    "        )\n",
    "    if size < w * h:\n",
    "        warn(\n",
    "            f\"Sequence very short, will produce overlapping offsets: ({len(transition_scores)} - {skip} - {w}) = {size} < {w * h}\"\n",
    "        )\n",
    "    ts = transition_scores[skip:]\n",
    "\n",
    "    if take_first:\n",
    "        offsets = np.arange(0, size, w)[:h]\n",
    "        if not sort_slices:\n",
    "            offsets = np.random.permutation(offsets)\n",
    "    else:\n",
    "        offsets = np.random.choice(size, h, replace=False) + skip\n",
    "        if sort_slices:\n",
    "            offsets = np.sort(offsets)\n",
    "\n",
    "    top_k_probs = [\n",
    "        list(\n",
    "            islice(\n",
    "                (prob for top_id, prob in zip(top_ids, top_probs) if top_id != tgt_id),\n",
    "                d,\n",
    "            )\n",
    "        )\n",
    "        for tgt_id, top_ids, top_probs in zip(\n",
    "            ts.target_ids,\n",
    "            ts.top_k_indices,\n",
    "            ts.top_k_probs,\n",
    "        )\n",
    "    ]\n",
    "    top_k_probs = np.array(top_k_probs)\n",
    "    target_probs = np.array(ts.target_probs).reshape(-1, 1)\n",
    "\n",
    "    if log == \"ratio_of_logs\":\n",
    "        top_k_probs = np.log(top_k_probs)\n",
    "        target_probs = np.log(target_probs)\n",
    "\n",
    "    features = np.empty((h, w, d))\n",
    "    for i, offset in enumerate(offsets):\n",
    "        features[i, :, :] = np.true_divide(\n",
    "            target_probs[offset : offset + w],\n",
    "            top_k_probs[offset : offset + w],\n",
    "        )\n",
    "        if log == \"log_ratio\":\n",
    "            features[i, :, :] = np.log(features[i, :, :])\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = features_3d_likelihood_top_k_ratio(\n",
    "    dataset[3][\"transition_scores\"],\n",
    "    32,\n",
    "    16,\n",
    "    9,\n",
    "    take_first=True,\n",
    "    # log=\"log_ratio\",\n",
    ")\n",
    "viz3d(human)\n",
    "plt.show()\n",
    "\n",
    "ai = features_3d_likelihood_top_k_ratio(\n",
    "    dataset[-1][\"transition_scores\"],\n",
    "    32,\n",
    "    16,\n",
    "    9,\n",
    "    take_first=True,\n",
    "    # log=\"log_ratio\",\n",
    ")\n",
    "viz3d(ai, vmin=human.min(), vmax=human.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz2d(human.reshape(-1, 9).T)\n",
    "plt.show()\n",
    "\n",
    "viz2d(ai.reshape(-1, 9).T, vmin=human.min(), vmax=human.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_3d_likelihood_top_k_ratio(ts, 32, 16, 9, take_first=True)\n",
    "viz3d(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3d_likelihood_top_k_ratio(\n",
    "    dataset[3][\"transition_scores\"], 32, 16, 9, take_first=True\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3d_likelihood_top_k_ratio(\n",
    "    dataset[-1][\"transition_scores\"], 32, 16, 9, take_first=True\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_2d_log_likelihood_log_rank_ratio(\n",
    "    transition_scores: TransitionScores,\n",
    "    w: int,\n",
    "    h: int,\n",
    "    skip: int = 1,\n",
    "    take_first: bool = False,\n",
    "    no_overlap: bool = False,\n",
    "    sort_slices: bool = True,\n",
    "    epsilon=1e-8,\n",
    "):\n",
    "    if no_overlap:\n",
    "        raise NotImplementedError(\"no_overlap not implemented\")\n",
    "\n",
    "    size = len(transition_scores) - skip - w\n",
    "    if size < w:\n",
    "        raise ValueError(\n",
    "            f\"Sequence is too short: ({len(transition_scores)} - {skip} - {w}) < {w}\"\n",
    "        )\n",
    "\n",
    "    if take_first:\n",
    "        offsets = np.arange(skip, size, w)[:h]\n",
    "        if not sort_slices:\n",
    "            offsets = np.random.permutation(offsets)\n",
    "    else:\n",
    "        offsets = np.random.choice(size, h, replace=False) + skip\n",
    "        if sort_slices:\n",
    "            offsets = np.sort(offsets)\n",
    "\n",
    "    features = np.zeros((h, w))\n",
    "    target_probs = np.array(transition_scores.target_probs)\n",
    "    target_ranks = np.array(transition_scores.target_ranks)\n",
    "    for i, offset in enumerate(offsets):\n",
    "        features[i] = -np.true_divide(\n",
    "            # probs are generally small but never zero, so log(x) is safe\n",
    "            np.log(target_probs[offset : offset + w]),\n",
    "            # ranks, however, are 0-indexed, so we use log1p to avoid log(0)\n",
    "            # and add epsilon to avoid division by zero\n",
    "            np.log1p(target_ranks[offset : offset + w]) + epsilon,\n",
    "        )\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = features_2d_log_likelihood_log_rank_ratio(\n",
    "    dataset[2][\"transition_scores\"], 32, 16, take_first=False\n",
    ")\n",
    "viz2d(human)\n",
    "plt.show()\n",
    "\n",
    "ai = features_2d_log_likelihood_log_rank_ratio(\n",
    "    dataset[-1][\"transition_scores\"], 32, 16, take_first=False\n",
    ")\n",
    "viz2d(ai, vmin=human.min(), vmax=human.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_line(features: np.array, vmin=None, vmax=None):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(\n",
    "        features.flatten().reshape(-1, 1).repeat(32, 1).T,\n",
    "        cmap=\"gray\",\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = features_2d_log_likelihood_log_rank_ratio(\n",
    "    dataset[2][\"transition_scores\"],\n",
    "    32,\n",
    "    16,\n",
    "    take_first=False,\n",
    "    sort_slices=False,\n",
    ")\n",
    "viz_line(human)\n",
    "plt.show()\n",
    "\n",
    "ai = features_2d_log_likelihood_log_rank_ratio(\n",
    "    dataset[-1][\"transition_scores\"],\n",
    "    32,\n",
    "    16,\n",
    "    take_first=False,\n",
    "    sort_slices=False,\n",
    ")\n",
    "viz_line(ai, vmin=human.min(), vmax=human.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def log_likelihood_log_rank_ratio(\n",
    "    target_probs: np.array,\n",
    "    target_ranks: np.array,\n",
    "    epsilon: float = 1e-8,\n",
    ") -> np.array:\n",
    "    \"\"\"Compute the log-likelihood log-rank ratio.\n",
    "\n",
    "    Args:\n",
    "        target_probs (np.array): Target token probabilities.\n",
    "        target_ranks (np.array): Zero-indexed target token ranks.\n",
    "\n",
    "    Returns:\n",
    "        np.array: array of the same shape as target_probs and target_ranks.\n",
    "    \"\"\"\n",
    "    return -np.true_divide(\n",
    "        # probs are generally small but never zero, so log(x) is safe\n",
    "        np.log(target_probs),\n",
    "        # ranks, however, are 0-indexed, so we use log1p to avoid log(0)\n",
    "        # and add epsilon to avoid division by zero\n",
    "        np.log1p(target_ranks) + epsilon,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = dataset[2][\"transition_scores\"]\n",
    "l = len(ts.target_probs) - 256\n",
    "o = np.random.randint(1, l)\n",
    "o = 1\n",
    "human = log_likelihood_log_rank_ratio(\n",
    "    ts.target_probs[o : o + 256],\n",
    "    ts.target_ranks[o : o + 256],\n",
    ")\n",
    "viz_line(human)\n",
    "plt.show()\n",
    "\n",
    "ts = dataset[-1][\"transition_scores\"]\n",
    "l = len(ts.target_probs) - 256\n",
    "o = np.random.randint(1, l)\n",
    "o = 1\n",
    "ai = log_likelihood_log_rank_ratio(\n",
    "    ts.target_probs[o : o + 256],\n",
    "    ts.target_ranks[o : o + 256],\n",
    ")\n",
    "viz_line(ai, vmin=human.min(), vmax=human.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from dotenv import load_dotenv\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "load_dotenv(\"../env\")\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import luminar.document.data\n",
    "import luminar.features\n",
    "import luminar.mongo\n",
    "\n",
    "importlib.reload(luminar.document.data)\n",
    "importlib.reload(luminar.features)\n",
    "importlib.reload(luminar.mongo)\n",
    "\n",
    "DocumentClassificationDataModule = (\n",
    "    luminar.document.data.DocumentClassificationDataModule\n",
    ")\n",
    "MongoDBAdapter = luminar.mongo.MongoDBAdapter\n",
    "OneDimFeatures = luminar.features.OneDimFeatures\n",
    "TwoDimFeatures = luminar.features.TwoDimFeatures\n",
    "ThreeDimFeatures = luminar.features.ThreeDimFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = MongoDBAdapter(\n",
    "    os.environ.get(\"MONGO_DB_CONNECTION\"),\n",
    "    \"prismai\",\n",
    "    \"collected_items\",\n",
    "    \"synthesized_texts\",\n",
    "    \"transition_scores\",\n",
    "    domain=\"cnn_news\",\n",
    "    source_collection_limit=1500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = OneDimFeatures(256)\n",
    "dm = DocumentClassificationDataModule(db, feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3840018cfadd46efa9d545b2d59c677e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2905 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import luminar.document.model\n",
    "\n",
    "importlib.reload(luminar.document.model)\n",
    "\n",
    "DocumentClassficationModel = luminar.document.model.DocumentClassficationModel\n",
    "ConvolutionalLayerSpec = luminar.document.model.ConvolutionalLayerSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = DocumentClassficationModel(\n",
    "    feature_size,\n",
    "    projection_dim=128,\n",
    "    learning_rate=5e-5,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    logger=tb_logger,\n",
    "    # gradient_clip_val=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (2-3): 2 x Sequential(\n",
      "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/mastoeck/Projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/mastoeck/Projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:477: The total number of parameters detected may be inaccurate because the model contains an instance of `UninitializedParameter`. To get an accurate number, set `self.example_input_array` in your LightningModule.\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | conv_layers | ModuleList        | 0      | train\n",
      "1 | classifier  | Sequential        | 0      | train\n",
      "2 | criterion   | BCEWithLogitsLoss | 0      | train\n",
      "----------------------------------------------------------\n",
      "0         Trainable params\n",
      "0         Non-trainable params\n",
      "0         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "21        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e69642a51d46199a4ee1a0c119a74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mastoeck/Projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d87ac454d04c54ab81649065745b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8f0faf1e254f529f4cb2fa0d9e16e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed24226514114177a4b4deb22baeba24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8380460745cd4fda83b0ca064e2baf3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722944fbb4b0468fbd361cf2b37ea9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6354ad0069fb46dd96f18a03e5bf1775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfa25c3285e4de5962d130126e11189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfaa7a251af433ab23c534a5e3427ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1095b675f74b4590a654b4a723eb3c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f3fdb0007a4ce0a1d5630f447b8901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6213376e2aa84b79a25647e83699bd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c4962ea4494306b8a2180bf9414438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08b836c6327434ab8bacfe9c7438947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b982724bcb4fca98c73bb4a35b92d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652de6de302e46dfa205922a97b01f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06411ad38194d049c37a321f1c16052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcd5407f7b64f229e82568db70b34cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a384ccc62fbd41aba4cab7b8c50836eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8004c77948491bbd2371f7f177f515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce8a34781ec4ff6ad353395d1ddb156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e62b2ceca484c41a67ee7c139d0872c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=dm.train_dataloader(),\n",
    "    val_dataloaders=dm.val_dataloader(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/mastoeck/Projects/PrismAI/PrismAI/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944d232d86214236994b895a15b22bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            acc            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8241379261016846     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            f1             </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8241379261016846     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         precision         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8241379261016846     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          recall           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8241379261016846     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.38913244009017944    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m           acc           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8241379261016846    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m           f1            \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8241379261016846    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        precision        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8241379261016846    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         recall          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8241379261016846    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.38913244009017944   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.38913244009017944,\n",
       "  'acc': 0.8241379261016846,\n",
       "  'precision': 0.8241379261016846,\n",
       "  'recall': 0.8241379261016846,\n",
       "  'f1': 0.8241379261016846}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, dataloaders=[dm.val_dataloader()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

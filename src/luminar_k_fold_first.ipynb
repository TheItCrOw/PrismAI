{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from luminar.data import (\n",
    "    FeatureDataset,\n",
    "    PaddingDataloader,\n",
    "    n_way_split,\n",
    ")\n",
    "from luminar.model import CNNDocumentClassficationModel, ConvolutionalLayerSpec\n",
    "from luminar.features import FeatureExtractor, OneDimFeatures, Slicer, TwoDimFeatures\n",
    "from luminar.mongo import PrismaiDataset\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = {\n",
    "    \"Blog Authorship\": {\"domain\": \"blog_authorship_corpus\"},\n",
    "    \"Student Essays\": {\"domain\": \"student_essays\"},\n",
    "    \"CNN News\": {\"domain\": \"cnn_news\"},\n",
    "    \"Euro Court Cases\": {\"domain\": \"euro_court_cases\"},\n",
    "    \"House of Commons\": {\"domain\": \"house_of_commons\"},\n",
    "    \"ArXiv Papers\": {\"domain\": \"arxiv_papers\"},\n",
    "    \"Gutenberg\": {\"domain\": \"gutenberg\", \"lang\": \"en-EN\"},\n",
    "    \"Bundestag\": {\"domain\": \"bundestag\"},\n",
    "    \"Spiegel\": {\"domain\": \"spiegel_articles\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"eval_split\": 0.1,\n",
    "    \"test_split\": 0.2,\n",
    "    \"feature_model\": \"gpt2\",\n",
    "    \"synth_agent\": \"gpt-4o-mini\",\n",
    "    \"document_type\": \"fulltext\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dim = 256\n",
    "k = 13\n",
    "\n",
    "feature_dim = TwoDimFeatures(first_dim, k)\n",
    "featurizer = FeatureExtractor.IntermediateLikelihood(k)\n",
    "config[\"second_dim_as_channels\"] = True\n",
    "\n",
    "slicer = Slicer.First(first_dim)\n",
    "# multiple = 4\n",
    "# slicer = Slicer.RandomMultiple(first_dim // multiple, multiple=multiple, stride=16)\n",
    "\n",
    "config[\"feature_dim\"] = feature_dim\n",
    "config[\"featurizer\"] = repr(featurizer)\n",
    "config[\"slicer\"] = repr(slicer)\n",
    "\n",
    "config[\"num_samples\"] = None\n",
    "\n",
    "\n",
    "def featurize(dataset) -> FeatureDataset:\n",
    "    return FeatureDataset.from_prismai(\n",
    "        tqdm(dataset, position=1, leave=False, disable=True),\n",
    "        slicer,\n",
    "        featurizer,\n",
    "        num_samples=config[\"num_samples\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m domain, kwargs \u001b[38;5;129;01min\u001b[39;00m tqdm(domains\u001b[38;5;241m.\u001b[39mitems(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDomains\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      6\u001b[0m     seed_everything(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     splits[domain] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 8\u001b[0m         \u001b[43mfeaturize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m subset \u001b[38;5;129;01min\u001b[39;00m n_way_split(\n\u001b[1;32m     10\u001b[0m             PrismaiDataset(\n\u001b[1;32m     11\u001b[0m                 mongo_db_connection\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMONGO_DB_CONNECTION\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     12\u001b[0m                 database\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprismai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m                 collection\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures_prismai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m                 feature_model\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_model\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m                 synth_agent\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynth_agent\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m                 document_type\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_type\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m                 additional_match_conditions\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     18\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_match_conditions\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}\n\u001b[1;32m     19\u001b[0m                 ),\n\u001b[1;32m     20\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     21\u001b[0m                 \u001b[38;5;66;03m# update_cache=True,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m             )\u001b[38;5;241m.\u001b[39mload(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;241m*\u001b[39msizes,\n\u001b[1;32m     24\u001b[0m         )\n\u001b[1;32m     25\u001b[0m     ]\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mfeaturize\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeaturize\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FeatureDataset:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFeatureDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_prismai\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mslicer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeaturizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_samples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/src/luminar/data.py:96\u001b[0m, in \u001b[0;36mFeatureDataset.from_prismai\u001b[0;34m(cls, dataset, slicer, featurizer, num_samples, label_fn)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_prismai\u001b[39m(\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     label_fn: Callable[[\u001b[38;5;28mdict\u001b[39m], \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m default_label_fn,\n\u001b[1;32m     83\u001b[0m ):\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a dataset with features for CNN training from a list of documents.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    Each document is expected to have a list of samples.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    Adapt `label_fn` to infer the label from a sample.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m        FeatureDataset: A dataset with features and labels.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43mSampleDict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFeatureValues\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msamples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslicer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeaturizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeaturizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/src/luminar/data.py:67\u001b[0m, in \u001b[0;36mFeatureDataset.from_samples\u001b[0;34m(cls, data, slicer, featurizer, num_samples)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_samples\u001b[39m(\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     num_samples: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     52\u001b[0m ):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    Create a dataset for CNN training.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    Uses the given `slicer` to extract feature (potentially multiple) slices from each sample.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m        num_samples (int, optional): _description_. Defaults to 1.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m     65\u001b[0m         [\n\u001b[1;32m     66\u001b[0m             {\n\u001b[0;32m---> 67\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfeaturizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeaturize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     68\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     69\u001b[0m             }\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;129;01min\u001b[39;00m slicer\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mlen\u001b[39m(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]), num_samples)\n\u001b[1;32m     72\u001b[0m         ]\n\u001b[1;32m     73\u001b[0m     )\n",
      "File \u001b[0;32m/nvme/projects/PrismAI/PrismAI/src/luminar/features.py:371\u001b[0m, in \u001b[0;36mIntermediateLikelihood.featurize\u001b[0;34m(self, ts, slices)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeaturize\u001b[39m(\u001b[38;5;28mself\u001b[39m, ts: FeatureValues, slices: \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mslice\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 371\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_likelihoods\u001b[49m\u001b[43m)\u001b[49m[slices]\n\u001b[1;32m    372\u001b[0m     last_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_n \u001b[38;5;129;01mor\u001b[39;00m logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    373\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39mlast_n:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, last_n)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config[\"seed\"] = 42\n",
    "\n",
    "sizes = [0.1] * 10\n",
    "splits = {}\n",
    "for domain, kwargs in tqdm(domains.items(), desc=\"Domains\", position=0):\n",
    "    seed_everything(config[\"seed\"], verbose=False)\n",
    "    splits[domain] = [\n",
    "        featurize(subset)\n",
    "        for subset in n_way_split(\n",
    "            PrismaiDataset(\n",
    "                mongo_db_connection=os.getenv(\"MONGO_DB_CONNECTION\"),\n",
    "                database=\"prismai\",\n",
    "                collection=\"features_prismai\",\n",
    "                feature_model=config[\"feature_model\"],\n",
    "                synth_agent=config[\"synth_agent\"],\n",
    "                document_type=config[\"document_type\"],\n",
    "                additional_match_conditions=config.get(\n",
    "                    \"additional_match_conditions\", {}\n",
    "                ),\n",
    "                **kwargs,\n",
    "                # update_cache=True,\n",
    "            ).load(verbose=False),\n",
    "            *sizes,\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config |= {\n",
    "    \"projection_dim\": 32,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"warmup_steps\": 66,\n",
    "    \"max_epochs\": 25,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "# SeqXGPT Layer Configuration\n",
    "config[\"conv_layer_shapes\"] = [\n",
    "    ConvolutionalLayerSpec(64, 5),\n",
    "    *[ConvolutionalLayerSpec(128, 3)] * 3,\n",
    "    ConvolutionalLayerSpec(64, 3),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "# capturing config from \"closure\"\n",
    "def get_dataloader(*dataset, **kwargs) -> PaddingDataloader:\n",
    "    if len(dataset) == 1:\n",
    "        dataset = dataset[0]\n",
    "    else:\n",
    "        dataset = ConcatDataset(dataset)\n",
    "    return PaddingDataloader(\n",
    "        dataset,\n",
    "        feature_dim=config[\"feature_dim\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Domain Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "metrics_in_domain = defaultdict(list)\n",
    "for domain, subsets in tqdm(splits.items()):\n",
    "    for _ in trange(5, desc=domain, position=1):\n",
    "        seed_everything(config[\"seed\"], verbose=False)\n",
    "        # cycle through splits for cross-validation\n",
    "        eval_dataset = subsets.pop(0)\n",
    "        test_dataloader = get_dataloader(*subsets[:2])\n",
    "        train_dataloader = get_dataloader(*subsets[2:], shuffle=True)\n",
    "        eval_dataloader = get_dataloader(eval_dataset)\n",
    "        subsets.append(eval_dataset)\n",
    "\n",
    "        model = CNNDocumentClassficationModel(**config)\n",
    "        trainer = Trainer(\n",
    "            max_epochs=config[\"max_epochs\"],\n",
    "            logger=pl_loggers.TensorBoardLogger(\n",
    "                save_dir=f\"logs/in_domain/{type(featurizer).__name__}\",\n",
    "                name=domain,\n",
    "            ),\n",
    "            gradient_clip_val=config[\"gradient_clip_val\"],\n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)],\n",
    "            deterministic=True,\n",
    "        )\n",
    "        trainer.progress_bar_callback.disable()\n",
    "\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=eval_dataloader,\n",
    "        )\n",
    "        (metrics,) = trainer.test(model, test_dataloader, verbose=False)\n",
    "        metrics_in_domain[domain].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"domain\": domain,\n",
    "            **{\n",
    "                \"test_auroc\": metric[\"test_auroc\"],\n",
    "                \"test_f1@0.5\": metric[\"test_f1@0.5\"],\n",
    "            },\n",
    "        }\n",
    "        for domain in domains\n",
    "        for metric in metrics_in_domain[domain]\n",
    "    ]\n",
    ")\n",
    "df = (\n",
    "    df.groupby(\"domain\")\n",
    "    .mean()\n",
    "    .sort_index(key=lambda i: list(map(list(domains.keys()).index, i)))\n",
    ")\n",
    "print(\n",
    "    df.to_latex(\n",
    "        float_format=\"%.3f\",\n",
    "        index=False,\n",
    "    )\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "metrics_out_of_domain = defaultdict(list)\n",
    "for domain in tqdm(splits.keys()):\n",
    "    for _ in trange(5, desc=domain, position=1):\n",
    "        seed_everything(config[\"seed\"], verbose=False)\n",
    "        train_subsets = []\n",
    "        eval_subsets = []\n",
    "        for other, subsets in splits.items():\n",
    "            if other == domain:\n",
    "                subsets.append(subsets.pop(0))\n",
    "                test_dataset = subsets[:2]\n",
    "            else:\n",
    "                eval_dataset = subsets.pop(0)\n",
    "                eval_subsets.append(eval_dataset)\n",
    "                train_subsets.extend(subsets[2:])\n",
    "                subsets.append(eval_dataset)\n",
    "\n",
    "        train_dataloader = get_dataloader(*train_subsets, shuffle=True)\n",
    "        eval_dataloader = get_dataloader(*eval_subsets)\n",
    "        test_dataloader = get_dataloader(*test_dataset)\n",
    "\n",
    "        model = CNNDocumentClassficationModel(**config)\n",
    "        trainer = Trainer(\n",
    "            max_epochs=config[\"max_epochs\"],\n",
    "            logger=pl_loggers.TensorBoardLogger(\n",
    "                save_dir=f\"logs/in_domain/{type(featurizer).__name__}\",\n",
    "                name=domain,\n",
    "            ),\n",
    "            gradient_clip_val=config[\"gradient_clip_val\"],\n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)],\n",
    "            deterministic=True,\n",
    "        )\n",
    "        trainer.progress_bar_callback.disable()\n",
    "\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=eval_dataloader,\n",
    "        )\n",
    "        (metrics,) = trainer.test(model, test_dataloader, verbose=False)\n",
    "        metrics_out_of_domain[domain].append(metrics)\n",
    "\n",
    "        print(domain, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"domain\": domain,\n",
    "            **{\n",
    "                \"test_auroc\": metric[\"test_auroc\"],\n",
    "                \"test_f1@0.5\": metric[\"test_f1@0.5\"],\n",
    "            },\n",
    "        }\n",
    "        for domain in domains\n",
    "        for metric in metrics_out_of_domain[domain]\n",
    "    ]\n",
    ")\n",
    "df = (\n",
    "    df.groupby(\"domain\")\n",
    "    .mean()\n",
    "    .sort_index(key=lambda i: list(map(list(domains.keys()).index, i)))\n",
    ")\n",
    "print(\n",
    "    df.to_latex(\n",
    "        float_format=\"%.3f\",\n",
    "        index=False,\n",
    "    )\n",
    ")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

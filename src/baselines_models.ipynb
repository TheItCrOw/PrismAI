{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=r\".*Please note that with a fast tokenizer.*\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*Using the `WANDB_DISABLED` environment variable is deprecated.*\",\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*Was asked to gather along dimension \\d+, but all input tensors were scalars.*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from luminar.baselines.core import run_detector, run_detector_tokenized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines: Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from luminar.utils import get_matched_datasets\n",
    "\n",
    "HF_TOKEN = (Path.home() / \".hf_token\").read_text().strip()\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    agent=\"gpt_4o_mini\",\n",
    "    other_agents=\"gemma2_9b\",\n",
    "    domains=(\n",
    "        \"blog_authorship_corpus\",\n",
    "        \"student_essays\",\n",
    "        \"cnn_news\",\n",
    "        \"euro_court_cases\",\n",
    "        \"house_of_commons\",\n",
    "        \"arxiv_papers\",\n",
    "        \"gutenberg_en\",\n",
    "        \"bundestag\",\n",
    "        \"spiegel_articles\",\n",
    "        # \"gutenberg_de\",\n",
    "        \"en\",\n",
    "        \"de\",\n",
    "    ),\n",
    "    num_proc: int = 32,\n",
    "):\n",
    "    datasets = {}\n",
    "    for domain in tqdm(domains):\n",
    "        datset_config_name = f\"{domain}-fulltext\"\n",
    "        dataset_split_name = f\"human+{agent}+{other_agents}\"\n",
    "        dataset: Dataset = (\n",
    "            load_dataset(\n",
    "                \"liberi-luminaris/PrismAI\",\n",
    "                datset_config_name,\n",
    "                split=dataset_split_name,\n",
    "                token=HF_TOKEN,\n",
    "            )  # type: ignore\n",
    "            .rename_column(\"label\", \"labels\")\n",
    "            .filter(\n",
    "                lambda text: len(text.strip()) > 0,\n",
    "                input_columns=[\"text\"],\n",
    "                num_proc=num_proc,  # type: ignore\n",
    "            )\n",
    "        )\n",
    "        datasets_matched, dataset_unmatched = get_matched_datasets(\n",
    "            dataset, agent, num_proc=num_proc\n",
    "        )\n",
    "        datasets_matched[\"unmatched\"] = dataset_unmatched\n",
    "        datasets[domain] = datasets_matched\n",
    "    return datasets\n",
    "\n",
    "\n",
    "datasets = load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RADAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luminar.baselines.radar import Radar\n",
    "\n",
    "\n",
    "def run_radar():\n",
    "    detector = Radar(device=\"cuda\")\n",
    "    try:\n",
    "        return run_detector(detector, datasets, sigmoid=False)\n",
    "    finally:\n",
    "        detector.model.to(\"cpu\")\n",
    "        del detector\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "scores_radar = run_radar()\n",
    "print(json.dumps(scores_radar, indent=4))\n",
    "with open(\"../logs/radar.json\", \"w\") as fp:\n",
    "    json.dump(scores_radar, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binoculars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luminar.baselines.binoculars import GLOBAL_BINOCULARS_THRESHOLD, Binoculars\n",
    "\n",
    "\n",
    "def run_binoculars():\n",
    "    detector = Binoculars(\"tiiuae/falcon-7b\", \"tiiuae/falcon-7b-instruct\")\n",
    "    try:\n",
    "        return run_detector(\n",
    "            detector,\n",
    "            datasets,\n",
    "            batch_size=16,\n",
    "            threshold=GLOBAL_BINOCULARS_THRESHOLD,\n",
    "            # lower scores indicate AI generated texts\n",
    "            less_than=True,\n",
    "        )\n",
    "    finally:\n",
    "        detector.observer_model.to(\"cpu\")\n",
    "        detector.performer_model.to(\"cpu\")\n",
    "        del detector\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "scores_binoculars = run_binoculars()\n",
    "print(json.dumps(scores_binoculars, indent=4))\n",
    "with open(\"../logs/binoculars.json\", \"w\") as fp:\n",
    "    json.dump(scores_binoculars, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E5-Small LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luminar.baselines.trainable.e5_lora import E5Lora\n",
    "\n",
    "\n",
    "def run_e5_small_lora():\n",
    "    detector = E5Lora(device=\"cuda:0\")\n",
    "    try:\n",
    "        return run_detector(detector, datasets)\n",
    "    finally:\n",
    "        detector.model.to(\"cpu\")\n",
    "        del detector\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "scores_e5 = run_e5_small_lora()\n",
    "\n",
    "print(json.dumps(scores_e5, indent=4))\n",
    "with open(\"../logs/e5-small-lora.json\", \"w\") as fp:\n",
    "    json.dump(scores_e5, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\n",
    "    \"Web Blogs\",\n",
    "    \"Essays\",\n",
    "    \"CNN\",\n",
    "    \"ECHR\",\n",
    "    \"HoC\",\n",
    "    \"arXiv\",\n",
    "    \"Gutenberg$_{en}$\",\n",
    "    \"Bundestag$_{de}$\",\n",
    "    \"Spiegel$_{de}$\",\n",
    "    \"Gutenberg$_{de}$\",\n",
    "    \"All$_{en}$\",\n",
    "    \"All$_{de}$\",\n",
    "]\n",
    "\n",
    "name_map = {\n",
    "    \"blog_authorship_corpus\": \"Web Blogs\",\n",
    "    \"student_essays\": \"Essays\",\n",
    "    \"cnn_news\": \"CNN\",\n",
    "    \"euro_court_cases\": \"ECHR\",\n",
    "    \"house_of_commons\": \"HoC\",\n",
    "    \"arxiv_papers\": \"arXiv\",\n",
    "    \"gutenberg_en\": \"Gutenberg$_{en}$\",\n",
    "    \"bundestag\": \"Bundestag$_{de}$\",\n",
    "    \"spiegel_articles\": \"Spiegel$_{de}$\",\n",
    "    # \"gutenberg_de\": \"Gutenberg$_{de}$\",\n",
    "    \"en\": \"All$_{en}$\",\n",
    "    \"de\": \"All$_{de}$\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = defaultdict(dict)\n",
    "for logs_path in Path(\"../logs/\").iterdir():\n",
    "    if logs_path.suffix == \".json\":\n",
    "        with logs_path.open(\"r\") as fp:\n",
    "            data = json.load(fp)\n",
    "        model_name = logs_path.stem\n",
    "        for domain, scores in data.items():\n",
    "            results[name_map[domain]].update(\n",
    "                {\n",
    "                    model_name + \"_f1_score\": scores[\"f1_score_fpr\"],\n",
    "                    # model_name + \"_accuracy\": scores[\"accuracy_fpr\"],\n",
    "                    model_name + \"_tpr\": scores[\"tpr_fpr\"],\n",
    "                    model_name + \"_roc_auc\": scores[\"roc_auc_fpr\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "model_name = \"roberta-base-ft\"\n",
    "for domain, name in name_map.items():\n",
    "    logs_path = Path(\"../logs/roberta-ft/roberta-base/\") / (domain + \".json\")\n",
    "    with (logs_path).open(\"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    results[name].update(\n",
    "        {\n",
    "            model_name + \"_f1_score\": data[domain][\"f1_score_fpr\"],\n",
    "            # model_name + \"_accuracy\": data[domain][\"accuracy_fpr\"],\n",
    "            model_name + \"_tpr\": data[domain][\"tpr_fpr\"],\n",
    "            model_name + \"_roc_auc\": data[domain][\"roc_auc_fpr\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "metric_df = (\n",
    "    pd.DataFrame([{\"domain\": domain} | dd for domain, dd in results.items()])\n",
    "    .set_index(\"domain\")\n",
    "    .sort_index(key=lambda x: list(map(domains.index, x)))\n",
    ")\n",
    "print(metric_df.to_latex(float_format=\"%.3f\", index=True))\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = defaultdict(dict)\n",
    "for logs_path in Path(\"../logs/\").iterdir():\n",
    "    if logs_path.suffix == \".json\":\n",
    "        with logs_path.open(\"r\") as fp:\n",
    "            data = json.load(fp)\n",
    "        model_name = logs_path.stem\n",
    "        for domain, scores in data.items():\n",
    "            results[name_map[domain]].update(\n",
    "                {\n",
    "                    model_name + \"_f1_score\": scores[\"f1_score\"],\n",
    "                    model_name + \"_accuracy\": scores[\"accuracy\"],\n",
    "                    model_name + \"_roc_auc\": scores[\"roc_auc\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "model_name = \"roberta-base-ft\"\n",
    "for domain, name in name_map.items():\n",
    "    logs_path = Path(\"../logs/roberta-ft/roberta-base/\") / (domain + \".json\")\n",
    "    with (logs_path).open(\"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    results[name].update(\n",
    "        {\n",
    "            model_name + \"_f1_score\": data[domain][\"f1_score\"],\n",
    "            model_name + \"_accuracy\": data[domain][\"accuracy\"],\n",
    "            model_name + \"_roc_auc\": data[domain][\"roc_auc\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "metric_df = (\n",
    "    pd.DataFrame([{\"domain\": domain} | dd for domain, dd in results.items()])\n",
    "    .set_index(\"domain\")\n",
    "    .sort_index(key=lambda x: list(map(domains.index, x)))\n",
    ")\n",
    "print(metric_df.to_latex(float_format=\"%.3f\", index=True))\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = defaultdict(dict)\n",
    "for logs_path in Path(\"../logs/\").iterdir():\n",
    "    if logs_path.suffix == \".json\":\n",
    "        with logs_path.open(\"r\") as fp:\n",
    "            data = json.load(fp)\n",
    "        model_name = logs_path.stem\n",
    "        for domain, scores in data.items():\n",
    "            results[name_map[domain]].update(\n",
    "                {\n",
    "                    model_name + \"_f1_score\": scores[\"f1_score_median\"],\n",
    "                    model_name + \"_accuracy\": scores[\"accuracy_median\"],\n",
    "                    model_name + \"_roc_auc\": scores[\"roc_auc_median\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "model_name = \"roberta-base-ft\"\n",
    "for domain, name in name_map.items():\n",
    "    logs_path = Path(\"../logs/roberta-ft/roberta-base/\") / (domain + \".json\")\n",
    "    with (logs_path).open(\"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    results[name].update(\n",
    "        {\n",
    "            model_name + \"_f1_score\": data[domain][\"f1_score_median\"],\n",
    "            model_name + \"_accuracy\": data[domain][\"accuracy_median\"],\n",
    "            model_name + \"_roc_auc\": data[domain][\"roc_auc_median\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "metric_df = (\n",
    "    pd.DataFrame([{\"domain\": domain} | dd for domain, dd in results.items()])\n",
    "    .set_index(\"domain\")\n",
    "    .sort_index(key=lambda x: list(map(domains.index, x)))\n",
    ")\n",
    "print(metric_df.to_latex(float_format=\"%.3f\", index=True))\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = defaultdict(dict)\n",
    "for logs_path in Path(\"../logs/\").iterdir():\n",
    "    if logs_path.suffix == \".json\":\n",
    "        with logs_path.open(\"r\") as fp:\n",
    "            data = json.load(fp)\n",
    "        model_name = logs_path.stem\n",
    "        for domain, scores in data.items():\n",
    "            results[name_map[domain]].update(\n",
    "                {\n",
    "                    model_name + \"_f1_score\": scores[\"f1_score_mean\"],\n",
    "                    model_name + \"_accuracy\": scores[\"accuracy_mean\"],\n",
    "                    model_name + \"_roc_auc\": scores[\"roc_auc_mean\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "model_name = \"roberta-base-ft\"\n",
    "for domain, name in name_map.items():\n",
    "    logs_path = Path(\"../logs/roberta-ft/roberta-base/\") / (domain + \".json\")\n",
    "    with (logs_path).open(\"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    results[name].update(\n",
    "        {\n",
    "            model_name + \"_f1_score\": data[domain][\"f1_score_mean\"],\n",
    "            model_name + \"_accuracy\": data[domain][\"accuracy_mean\"],\n",
    "            model_name + \"_roc_auc\": data[domain][\"roc_auc_mean\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "metric_df = (\n",
    "    pd.DataFrame([{\"domain\": domain} | dd for domain, dd in results.items()])\n",
    "    .set_index(\"domain\")\n",
    "    .sort_index(key=lambda x: list(map(domains.index, x)))\n",
    ")\n",
    "print(metric_df.to_latex(float_format=\"%.3f\", index=True))\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(dict)\n",
    "\n",
    "model_name = \"roberta-ft\"\n",
    "for domain, name in name_map.items():\n",
    "    logs_path = Path(\"../logs/chatgpt-detector-roberta/\") / (domain + \".json\")\n",
    "    with (logs_path).open(\"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    results[name].update(\n",
    "        {\n",
    "            model_name + \"_f1\": data[domain][\"f1\"],\n",
    "            # model_name + \"_accuracy\": data[domain][\"accuracy\"],\n",
    "            model_name + \"_auroc\": data[domain][\"auroc\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "metric_df = (\n",
    "    pd.DataFrame([{\"domain\": domain} | dd for domain, dd in results.items()])\n",
    "    .set_index(\"domain\")\n",
    "    .sort_index(key=lambda x: list(map(domains.index, x)))\n",
    ")\n",
    "print(metric_df.to_latex(float_format=\"%.3f\", index=True))\n",
    "metric_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luminar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines: Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Source: Ghostbuster, Verma et al. (2024)\n",
    "def get_scores(labels, probabilities, calibrated=False, precision=6):\n",
    "    assert len(labels) == len(probabilities)\n",
    "\n",
    "    if calibrated:\n",
    "        threshold = sorted(probabilities)[len(labels) - sum(labels) - 1]\n",
    "    else:\n",
    "        threshold = 0.5\n",
    "\n",
    "    acc = round(float(accuracy_score(labels, probabilities > threshold)), precision)\n",
    "    f1 = round(float(f1_score(labels, probabilities > threshold)), precision)\n",
    "\n",
    "    if sum(labels) == 0 or sum(labels) == len(labels):\n",
    "        auroc = -1\n",
    "    else:\n",
    "        auroc = round(float(roc_auc_score(labels, probabilities)), precision)\n",
    "\n",
    "    return acc, f1, auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from luminar.utils import get_matched_datasets\n",
    "\n",
    "HF_TOKEN = (Path.home() / \".hf_token\").read_text().strip()\n",
    "\n",
    "agent = \"gpt_4o_mini\"\n",
    "other_agents = \"gemma2_9b\"\n",
    "datasets = {}\n",
    "for domain in tqdm(\n",
    "    [\n",
    "        \"blog_authorship_corpus\",\n",
    "        \"student_essays\",\n",
    "        \"cnn_news\",\n",
    "        \"euro_court_cases\",\n",
    "        \"house_of_commons\",\n",
    "        \"arxiv_papers\",\n",
    "        \"gutenberg_en\",\n",
    "        \"bundestag\",\n",
    "        \"spiegel_articles\",\n",
    "        # \"gutenberg_de\",\n",
    "        \"en\",\n",
    "        \"de\",\n",
    "    ]\n",
    "):\n",
    "    datset_config_name = f\"{domain}-fulltext\"\n",
    "    dataset_split_name = f\"human+{agent}+{other_agents}\"\n",
    "    dataset: Dataset = (\n",
    "        load_dataset(\n",
    "            \"liberi-luminaris/PrismAI\",\n",
    "            datset_config_name,\n",
    "            split=dataset_split_name,\n",
    "            token=HF_TOKEN,\n",
    "        )  # type: ignore\n",
    "        .rename_column(\"label\", \"labels\")\n",
    "        .filter(\n",
    "            lambda text: len(text.strip()) > 0,\n",
    "            input_columns=[\"text\"],\n",
    "            num_proc=8,\n",
    "        )\n",
    "    )\n",
    "    datasets_matched, dataset_unmatched = get_matched_datasets(dataset, agent)\n",
    "    datasets_matched[\"unmatched\"] = dataset_unmatched\n",
    "    datasets[domain] = datasets_matched\n",
    "del dataset\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from luminar.utils import compute_metrics\n",
    "\n",
    "\n",
    "def run_detector(detector, datasets: dict[str, DatasetDict], batch_size=32):\n",
    "    scores = {}\n",
    "    for config_name, dataset in tqdm(datasets.items(), desc=\"Predicting on Datasets\"):\n",
    "        dataset: Dataset = dataset[\"test\"].map(\n",
    "            detector.tokenize,\n",
    "            input_columns=[\"text\"],\n",
    "            batched=True,\n",
    "            batch_size=1024,\n",
    "            desc=\"Tokenizing\",\n",
    "        )\n",
    "        dataset = dataset.sort(\"length\")\n",
    "\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for batch in dataset.batch(batch_size):\n",
    "            labels.extend(batch[\"labels\"])\n",
    "            predictions.extend(detector.process(batch)[\"prediction\"])\n",
    "\n",
    "        metrics = compute_metrics((np.array(predictions), np.array(labels)))\n",
    "        scores[config_name] = {k: float(v) for k, v in metrics.items()}\n",
    "    return scores\n",
    "\n",
    "\n",
    "def run_detector_tokenized(detector, datasets: dict[str, DatasetDict], batch_size=32):\n",
    "    scores = {}\n",
    "    for config_name, dataset in tqdm(datasets.items(), desc=\"Predicting on Datasets\"):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for batch in dataset[\"test\"].batch(batch_size):\n",
    "            labels.extend(batch[\"labels\"])\n",
    "            predictions.extend(detector.process(batch)[\"prediction\"])\n",
    "\n",
    "        metrics = compute_metrics((np.array(predictions), np.array(labels)))\n",
    "        scores[config_name] = {k: float(v) for k, v in metrics.items()}\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from: RAID, Dugan et al. 2024\n",
    "# > https://github.com/liamdugan/raid/blob/main/detectors/models/chatgpt_roberta_detector/chatgpt_detector.py\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics_acc(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "class ChatGPTDetector:\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = torch.device(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"Hello-SimpleAI/chatgpt-detector-roberta\"\n",
    "        )\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"Hello-SimpleAI/chatgpt-detector-roberta\"\n",
    "        ).to(self.device)\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"Hello-SimpleAI/chatgpt-detector-roberta\"\n",
    "        ).to(self.device)\n",
    "\n",
    "    def tokenize(self, texts: list[str]) -> BatchEncoding:\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_length=True,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, inputs: dict) -> list[float]:\n",
    "        encoding = self.tokenizer.pad(inputs, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model(**encoding)\n",
    "        probs = outputs.logits\n",
    "        return probs[:, 1].detach().cpu().flatten().tolist()\n",
    "\n",
    "    def process(self, inputs: dict) -> dict[str, list[float]]:\n",
    "        return {\n",
    "            \"prediction\": self.predict(\n",
    "                {\n",
    "                    \"input_ids\": inputs[\"input_ids\"],\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def inference(self, texts: list) -> list:\n",
    "        predictions = []\n",
    "        for text in tqdm(texts):\n",
    "            inputs = self.tokenizer(text, truncation=True, return_tensors=\"pt\").to(\n",
    "                self.device\n",
    "            )\n",
    "            outputs = self.model(**inputs)\n",
    "            probs = outputs.logits.softmax(dim=-1)\n",
    "            real, fake = probs.detach().cpu().flatten().numpy().tolist()\n",
    "            predictions.append(fake)\n",
    "        return predictions\n",
    "\n",
    "    def train(self, dataset: DatasetDict, training_args: TrainingArguments):\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        trainer = Trainer(\n",
    "            self.model,\n",
    "            training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"eval\"],\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics_acc,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self.model = trainer.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets:   0%|                                                                                                                                                             | 0/11 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Predicting on Datasets:   9%|█████████████▌                                                                                                                                       | 1/11 [00:04<00:46,  4.66s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f894711166f4a4290f4b324512c8ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/14496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e0866f261e40b288cb57b1d16edc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/14496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets:  18%|███████████████████████████                                                                                                                          | 2/11 [01:12<06:16, 41.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160af951e20c43b0a038d11fe48a0087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/4726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd868095571e411f9a7cf88d14f9549c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/4726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets:  27%|████████████████████████████████████████▋                                                                                                            | 3/11 [01:42<04:51, 36.42s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb13934ab6f347c9a3e0a5042716fdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1506 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ac75bde2734c06b1de88cbf4092931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/1506 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets:  36%|██████████████████████████████████████████████████████▏                                                                                              | 4/11 [01:55<03:09, 27.03s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0015ade489446ecad1272b901f48a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80fbc416bdd4a7db8759577a649c7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets:  45%|███████████████████████████████████████████████████████████████████▋                                                                                 | 5/11 [02:19<02:35, 25.91s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d48ba4eaa7498eaae0c30344684739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fadd8fd2aa43a9b78a14a0202a16b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/2870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets:  55%|█████████████████████████████████████████████████████████████████████████████████▎                                                                   | 6/11 [03:12<02:56, 35.25s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7df7126b534042893167ff1057e1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a75dd3f1404929874513e30eb33286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets:  64%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                      | 7/11 [04:06<02:45, 41.35s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b324e8175a0412fb9a375bf7564a622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/4024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a72511bcbd149648667868d1c71a378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/4024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets:  73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 8/11 [04:32<01:49, 36.57s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840e0073ff764060ad3ee918c22dd393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/4902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1787690a9c4345caaeab45b75d7a49cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/4902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets:  82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                           | 9/11 [05:02<01:08, 34.44s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874e08e7456b43fd87c2a9add57cac30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/28404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bce7d1fbd943b484d1d1f81578e7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/28404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 10/11 [08:51<01:34, 94.47s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc32957aaa1b41b1854b18079a255c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/8964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370aac1947f74cfbbc9ebdfbb1d11d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/8964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [09:51<00:00, 53.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"blog_authorship_corpus\": {\n",
      "        \"f1_human\": 0.7663951993141878,\n",
      "        \"f1_ai\": 0.5905334335086401,\n",
      "        \"f1_weighted\": 0.6784643164114139,\n",
      "        \"accuracy\": 0.7025109170305677,\n",
      "        \"roc_auc\": 0.7025109170305676,\n",
      "        \"f1_human_median\": 0.8231441048034934,\n",
      "        \"f1_ai_median\": 0.8231441048034934,\n",
      "        \"f1_weighted_median\": 0.8231441048034934,\n",
      "        \"accuracy_median\": 0.8231441048034934,\n",
      "        \"roc_auc_median\": 0.8231441048034934,\n",
      "        \"threshold_median\": 0.1145414453929881,\n",
      "        \"ground_truth_human\": 916.0,\n",
      "        \"ground_truth_ai\": 916.0\n",
      "    },\n",
      "    \"student_essays\": {\n",
      "        \"f1_human\": 0.6941830139471595,\n",
      "        \"f1_ai\": 0.5249251365157653,\n",
      "        \"f1_weighted\": 0.6095540752314623,\n",
      "        \"accuracy\": 0.6278973509933775,\n",
      "        \"roc_auc\": 0.6278973509933774,\n",
      "        \"f1_human_median\": 0.6826710816777042,\n",
      "        \"f1_ai_median\": 0.6826710816777042,\n",
      "        \"f1_weighted_median\": 0.6826710816777042,\n",
      "        \"accuracy_median\": 0.6826710816777042,\n",
      "        \"roc_auc_median\": 0.6826710816777042,\n",
      "        \"threshold_median\": 0.23121643172701614,\n",
      "        \"ground_truth_human\": 7248.0,\n",
      "        \"ground_truth_ai\": 7248.0\n",
      "    },\n",
      "    \"cnn_news\": {\n",
      "        \"f1_human\": 0.7458975634012929,\n",
      "        \"f1_ai\": 0.5516232816613045,\n",
      "        \"f1_weighted\": 0.6487604225312986,\n",
      "        \"accuracy\": 0.6756242065171393,\n",
      "        \"roc_auc\": 0.6756242065171392,\n",
      "        \"f1_human_median\": 0.8645789250952179,\n",
      "        \"f1_ai_median\": 0.8645789250952179,\n",
      "        \"f1_weighted_median\": 0.8645789250952179,\n",
      "        \"accuracy_median\": 0.8645789250952179,\n",
      "        \"roc_auc_median\": 0.864578925095218,\n",
      "        \"threshold_median\": 0.1136957041701503,\n",
      "        \"ground_truth_human\": 2363.0,\n",
      "        \"ground_truth_ai\": 2363.0\n",
      "    },\n",
      "    \"euro_court_cases\": {\n",
      "        \"f1_human\": 0.6920955882352942,\n",
      "        \"f1_ai\": 0.19856459330143542,\n",
      "        \"f1_weighted\": 0.4453300907683648,\n",
      "        \"accuracy\": 0.5551128818061088,\n",
      "        \"roc_auc\": 0.5551128818061088,\n",
      "        \"f1_human_median\": 0.7569721115537849,\n",
      "        \"f1_ai_median\": 0.7569721115537849,\n",
      "        \"f1_weighted_median\": 0.7569721115537849,\n",
      "        \"accuracy_median\": 0.7569721115537849,\n",
      "        \"roc_auc_median\": 0.7569721115537849,\n",
      "        \"threshold_median\": 0.06573456669695277,\n",
      "        \"ground_truth_human\": 753.0,\n",
      "        \"ground_truth_ai\": 753.0\n",
      "    },\n",
      "    \"house_of_commons\": {\n",
      "        \"f1_human\": 0.7187878787878788,\n",
      "        \"f1_ai\": 0.3695652173913043,\n",
      "        \"f1_weighted\": 0.5441765480895915,\n",
      "        \"accuracy\": 0.6110645431684828,\n",
      "        \"roc_auc\": 0.6110645431684828,\n",
      "        \"f1_human_median\": 0.8575020955574183,\n",
      "        \"f1_ai_median\": 0.8575020955574183,\n",
      "        \"f1_weighted_median\": 0.8575020955574183,\n",
      "        \"accuracy_median\": 0.8575020955574183,\n",
      "        \"roc_auc_median\": 0.8575020955574183,\n",
      "        \"threshold_median\": 0.0909201964070054,\n",
      "        \"ground_truth_human\": 1193.0,\n",
      "        \"ground_truth_ai\": 1193.0\n",
      "    },\n",
      "    \"arxiv_papers\": {\n",
      "        \"f1_human\": 0.6836588851834207,\n",
      "        \"f1_ai\": 0.13878080415045396,\n",
      "        \"f1_weighted\": 0.41121984466693734,\n",
      "        \"accuracy\": 0.5372822299651568,\n",
      "        \"roc_auc\": 0.5372822299651568,\n",
      "        \"f1_human_median\": 0.8020905923344948,\n",
      "        \"f1_ai_median\": 0.8020905923344948,\n",
      "        \"f1_weighted_median\": 0.8020905923344948,\n",
      "        \"accuracy_median\": 0.8020905923344948,\n",
      "        \"roc_auc_median\": 0.8020905923344948,\n",
      "        \"threshold_median\": 0.031737218157109344,\n",
      "        \"ground_truth_human\": 1435.0,\n",
      "        \"ground_truth_ai\": 1435.0\n",
      "    },\n",
      "    \"gutenberg_en\": {\n",
      "        \"f1_human\": 0.6956521739130435,\n",
      "        \"f1_ai\": 0.2314540059347181,\n",
      "        \"f1_weighted\": 0.46355308992388083,\n",
      "        \"accuracy\": 0.563973063973064,\n",
      "        \"roc_auc\": 0.5639730639730639,\n",
      "        \"f1_human_median\": 0.8787878787878788,\n",
      "        \"f1_ai_median\": 0.8787878787878788,\n",
      "        \"f1_weighted_median\": 0.8787878787878788,\n",
      "        \"accuracy_median\": 0.8787878787878788,\n",
      "        \"roc_auc_median\": 0.8787878787878789,\n",
      "        \"threshold_median\": 0.04526158798330267,\n",
      "        \"ground_truth_human\": 297.0,\n",
      "        \"ground_truth_ai\": 297.0\n",
      "    },\n",
      "    \"bundestag\": {\n",
      "        \"f1_human\": 0.6666666666666666,\n",
      "        \"f1_ai\": 0.0,\n",
      "        \"f1_weighted\": 0.3333333333333333,\n",
      "        \"accuracy\": 0.5,\n",
      "        \"roc_auc\": 0.5,\n",
      "        \"f1_human_median\": 0.6973161033797217,\n",
      "        \"f1_ai_median\": 0.6973161033797217,\n",
      "        \"f1_weighted_median\": 0.6973161033797217,\n",
      "        \"accuracy_median\": 0.6973161033797217,\n",
      "        \"roc_auc_median\": 0.6973161033797218,\n",
      "        \"threshold_median\": 0.028939342608573743,\n",
      "        \"ground_truth_human\": 2012.0,\n",
      "        \"ground_truth_ai\": 2012.0\n",
      "    },\n",
      "    \"spiegel_articles\": {\n",
      "        \"f1_human\": 0.6669387755102041,\n",
      "        \"f1_ai\": 0.0024449877750611247,\n",
      "        \"f1_weighted\": 0.33469188164263264,\n",
      "        \"accuracy\": 0.5006119951040392,\n",
      "        \"roc_auc\": 0.5006119951040392,\n",
      "        \"f1_human_median\": 0.5560995512035903,\n",
      "        \"f1_ai_median\": 0.5560995512035903,\n",
      "        \"f1_weighted_median\": 0.5560995512035903,\n",
      "        \"accuracy_median\": 0.5560995512035903,\n",
      "        \"roc_auc_median\": 0.5560995512035904,\n",
      "        \"threshold_median\": 0.03132947315877875,\n",
      "        \"ground_truth_human\": 2451.0,\n",
      "        \"ground_truth_ai\": 2451.0\n",
      "    },\n",
      "    \"en\": {\n",
      "        \"f1_human\": 0.7073150662387888,\n",
      "        \"f1_ai\": 0.47560076662243844,\n",
      "        \"f1_weighted\": 0.5914579164306136,\n",
      "        \"accuracy\": 0.6243134769750739,\n",
      "        \"roc_auc\": 0.6243134769750739,\n",
      "        \"f1_human_median\": 0.7251795521757499,\n",
      "        \"f1_ai_median\": 0.7251795521757499,\n",
      "        \"f1_weighted_median\": 0.7251795521757499,\n",
      "        \"accuracy_median\": 0.7251795521757499,\n",
      "        \"roc_auc_median\": 0.7251795521757499,\n",
      "        \"threshold_median\": 0.12768262990973359,\n",
      "        \"ground_truth_human\": 14202.0,\n",
      "        \"ground_truth_ai\": 14202.0\n",
      "    },\n",
      "    \"de\": {\n",
      "        \"f1_human\": 0.6671132946255769,\n",
      "        \"f1_ai\": 0.004895416110369382,\n",
      "        \"f1_weighted\": 0.33600435536797313,\n",
      "        \"accuracy\": 0.50111557340473,\n",
      "        \"roc_auc\": 0.5011155734047301,\n",
      "        \"f1_human_median\": 0.5910307898259706,\n",
      "        \"f1_ai_median\": 0.5910307898259706,\n",
      "        \"f1_weighted_median\": 0.5910307898259706,\n",
      "        \"accuracy_median\": 0.5910307898259706,\n",
      "        \"roc_auc_median\": 0.5910307898259706,\n",
      "        \"threshold_median\": 0.030052531307817612,\n",
      "        \"ground_truth_human\": 4482.0,\n",
      "        \"ground_truth_ai\": 4482.0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    results = run_detector(ChatGPTDetector(device=\"cuda:3\"), datasets)\n",
    "    gc.collect()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "\n",
    "scores_roberta = f()\n",
    "print(json.dumps(scores_roberta, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bbce7c20f440e9bdabb1b1780fc32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/6406 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1051be84ff644496914d2d9a9db7e821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/914 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca6c9fb885c4c0ea6cf521223f48cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f47706e246c40c988bcf44926b7a4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/24280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4404a8db307476ea49d22ddce0fa7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/50734 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ab6663002f41a6aa49ffad37bf9acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/7248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe91e76fe0a4c5e833fe0342a13cd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/14496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad94231b78e42248d2af21f919c5039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/11222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7de2c6af0b248fa9a7bf509fbda41dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/16538 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f09f24ca1f4c34a9d85a4df04f0cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35c8d1d020f473297c14f2a9abc0716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/4726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5c26fc7bbe4deb8bf8c69fb96d29e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/12901 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfa22a2a27b446fb0ba150749e52762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/5264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72061cdc7bf4953b2f57a44257a5dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/752 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2597830a0c4ab0b2064ff0b9b3c103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1506 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26913b01d5064a01a6016c3ccd999884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/10732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1193a2905c704377b03852399956c386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/8344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf1bbab3b3b42a1b6450b1cc41be980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297e196739964c13979db3c873050f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b595e467374e12a78c1fea0fa8da30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/15237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34e8731c3584eccb6305a658d936cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/10042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e4a68348d04b2cbfffe958b4151d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1434 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0944d60fc074471db3a912de91a8f810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c8e2e2c12b433d84c484f3b2688309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2497 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b99b05ccd24ef580eb6fa064e0d85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb93b16558c4a52b88acb9a27ef97da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/298 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5489b55861b6402f915c9d018f9dca80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b017055590b412398e15b853eb46899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/10552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d05276c2e7140d98e438b916e504927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/14078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b1249bbd4f47a8a1018db879068053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5844112d2b024bdbbca1b0dfcb08dd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/4024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3666194eb14f9aae9aa2ffc6cc663a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/16165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506840f9b1204705bcaa33925f3969b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/17154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acc1e89cbeb42088673a74b66e1ebe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/2450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f96560c1d4644b7a20001e841636a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/4902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ee918935d84268a4a3948b52a40e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/7524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577f17c224ba47928a8e2de0fdd80b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/99412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d147487b8574f0682088f42e438972f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/14202 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27a3c4cb11046a48899f620f5d99f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/28404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808478d1688d4501875596e40951e86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/87421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebbd3248fd646ed9dfcc468cbfaf61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/31372 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70fa274d81d43f8945f0997de6c4940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/4484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0d23b1503349efa5709b8a79c33c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/8964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae66ed11bf57452eab3480bf16b31a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/24224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetuning:   0%|                                                                                                                                                 | 0/11 [00:00<?, ?it/s, blog_authorship_corpus]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='107' max='107' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [107/107 01:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223450</td>\n",
       "      <td>0.936543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.196102</td>\n",
       "      <td>0.933260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5c071d0b58438a92d963e45f67e8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/1832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Predicting on Datasets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.02s/it]\n",
      "Finetuning:   9%|█████████████▏                                                                                                                                   | 1/11 [01:10<11:47, 70.74s/it, student_essays]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 06:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.269518</td>\n",
       "      <td>0.916391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.090071</td>\n",
       "      <td>0.980822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.090734</td>\n",
       "      <td>0.980132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.055652</td>\n",
       "      <td>0.989514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.034602</td>\n",
       "      <td>0.993791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.998206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.008040</td>\n",
       "      <td>0.998482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.054871</td>\n",
       "      <td>0.984685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.012667</td>\n",
       "      <td>0.996137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.048985</td>\n",
       "      <td>0.987583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>0.995999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.031946</td>\n",
       "      <td>0.991722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.033703</td>\n",
       "      <td>0.991722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.027454</td>\n",
       "      <td>0.993240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.011649</td>\n",
       "      <td>0.996275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.028482</td>\n",
       "      <td>0.993515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e5f793e85c44e88f384f62eb13bc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/14496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:55<00:00, 55.58s/it]\n",
      "Finetuning:  18%|███████████████████████████▎                                                                                                                          | 2/11 [08:53<45:10, 301.21s/it, cnn_news]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='276' max='276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [276/276 02:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.067120</td>\n",
       "      <td>0.983503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.044294</td>\n",
       "      <td>0.989425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.033669</td>\n",
       "      <td>0.989425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.034233</td>\n",
       "      <td>0.991540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>0.991540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec32951e4bb4dd48f6ae380261ece92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/4726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:21<00:00, 21.30s/it]\n",
      "Finetuning:  27%|██████████████████████████████████████▋                                                                                                       | 3/11 [11:23<30:56, 232.06s/it, euro_court_cases]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [88/88 01:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9744fe429f46d0b81c93e887546c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/1506 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.91s/it]\n",
      "Finetuning:  36%|███████████████████████████████████████████████████▋                                                                                          | 4/11 [12:32<19:35, 167.96s/it, house_of_commons]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 01:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.099287</td>\n",
       "      <td>0.979027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.053376</td>\n",
       "      <td>0.984899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3930be9c6b154b3aaf4fdf1a7af2ce09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.36s/it]\n",
      "Finetuning:  45%|██████████████████████████████████████████████████████████████████▎                                                                               | 5/11 [14:01<13:56, 139.41s/it, arxiv_papers]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [168/168 01:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313431</td>\n",
       "      <td>0.849372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.983264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.057966</td>\n",
       "      <td>0.989540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7811a1ee494d90b6b1b39ce4a8e155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/2870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.10s/it]\n",
      "Finetuning:  55%|███████████████████████████████████████████████████████████████████████████████▋                                                                  | 6/11 [15:46<10:37, 127.53s/it, gutenberg_en]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d015e015f0e74119b6e59b1623231269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.82s/it]\n",
      "Finetuning:  64%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                      | 7/11 [16:39<06:53, 103.47s/it, bundestag]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='235' max='235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [235/235 01:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.457674</td>\n",
       "      <td>0.842445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.113712</td>\n",
       "      <td>0.969682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.185112</td>\n",
       "      <td>0.951292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.209413</td>\n",
       "      <td>0.938867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2cb3dd64e54cd18094fe65d3ac28d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/4024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.03s/it]\n",
      "Finetuning:  73%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎                                      | 8/11 [18:50<05:36, 112.07s/it, spiegel_articles]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='286' max='286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [286/286 02:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.707816</td>\n",
       "      <td>0.673878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.645539</td>\n",
       "      <td>0.802449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.433282</td>\n",
       "      <td>0.888571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270944</td>\n",
       "      <td>0.920408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.264392</td>\n",
       "      <td>0.922857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6636093ea1ae462b98a77102b5e1f754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/4902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:22<00:00, 22.13s/it]\n",
      "Finetuning:  82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                            | 9/11 [21:25<04:10, 125.43s/it, en]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1657' max='1657' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1657/1657 17:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.234703</td>\n",
       "      <td>0.923039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.109619</td>\n",
       "      <td>0.973807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.099607</td>\n",
       "      <td>0.977257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.981270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.140027</td>\n",
       "      <td>0.971905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.043529</td>\n",
       "      <td>0.990353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.033531</td>\n",
       "      <td>0.992748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.048332</td>\n",
       "      <td>0.989720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.050249</td>\n",
       "      <td>0.987607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.057815</td>\n",
       "      <td>0.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.071681</td>\n",
       "      <td>0.980848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.033153</td>\n",
       "      <td>0.991762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.038891</td>\n",
       "      <td>0.989016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.020550</td>\n",
       "      <td>0.995564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.026993</td>\n",
       "      <td>0.993945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.024639</td>\n",
       "      <td>0.993452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.028219</td>\n",
       "      <td>0.993522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.012255</td>\n",
       "      <td>0.996550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.023811</td>\n",
       "      <td>0.994437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.023344</td>\n",
       "      <td>0.994719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.035716</td>\n",
       "      <td>0.992748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>0.997183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.044811</td>\n",
       "      <td>0.990635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.016477</td>\n",
       "      <td>0.995986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.013784</td>\n",
       "      <td>0.996479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.014897</td>\n",
       "      <td>0.996057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.013717</td>\n",
       "      <td>0.996479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.993240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.023491</td>\n",
       "      <td>0.994015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.017877</td>\n",
       "      <td>0.995282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.024882</td>\n",
       "      <td>0.993874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.023293</td>\n",
       "      <td>0.994437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.994297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce18f2ad14024e29a05133e91b67e973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/28404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:57<00:00, 117.23s/it]\n",
      "Finetuning:  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 10/11 [41:18<07:35, 455.24s/it, de]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='523' max='523' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [523/523 03:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.979816</td>\n",
       "      <td>0.691347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.291511</td>\n",
       "      <td>0.890946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424480</td>\n",
       "      <td>0.875558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.344510</td>\n",
       "      <td>0.883140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.122489</td>\n",
       "      <td>0.960749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.340472</td>\n",
       "      <td>0.917930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.287332</td>\n",
       "      <td>0.902542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.429497</td>\n",
       "      <td>0.875112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.320671</td>\n",
       "      <td>0.911463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.307393</td>\n",
       "      <td>0.916592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/staff_homes/mastoeck/Projects/PrismAI/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff64d146ecd4647b00c319f3003215a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/8964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on Datasets: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:40<00:00, 40.43s/it]\n",
      "Finetuning: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [46:01<00:00, 251.02s/it, de]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"blog_authorship_corpus\": {\n",
      "        \"blog_authorship_corpus\": {\n",
      "            \"f1_human\": 0.9453758788534343,\n",
      "            \"f1_ai\": 0.9443526170798898,\n",
      "            \"f1_weighted\": 0.944864247966662,\n",
      "            \"accuracy\": 0.9448689956331878,\n",
      "            \"roc_auc\": 0.9448689956331878,\n",
      "            \"f1_human_median\": 0.9421397379912664,\n",
      "            \"f1_ai_median\": 0.9421397379912664,\n",
      "            \"f1_weighted_median\": 0.9421397379912664,\n",
      "            \"accuracy_median\": 0.9421397379912664,\n",
      "            \"roc_auc_median\": 0.9421397379912664,\n",
      "            \"threshold_median\": 0.4127888748756461,\n",
      "            \"ground_truth_human\": 916.0,\n",
      "            \"ground_truth_ai\": 916.0\n",
      "        }\n",
      "    },\n",
      "    \"student_essays\": {\n",
      "        \"student_essays\": {\n",
      "            \"f1_human\": 0.9997930320800276,\n",
      "            \"f1_ai\": 0.9997930606332345,\n",
      "            \"f1_weighted\": 0.999793046356631,\n",
      "            \"accuracy\": 0.9997930463576159,\n",
      "            \"roc_auc\": 0.999793046357616,\n",
      "            \"f1_human_median\": 0.9997240618101545,\n",
      "            \"f1_ai_median\": 0.9997240618101545,\n",
      "            \"f1_weighted_median\": 0.9997240618101545,\n",
      "            \"accuracy_median\": 0.9997240618101545,\n",
      "            \"roc_auc_median\": 0.9997240618101544,\n",
      "            \"threshold_median\": 0.5467840819717229,\n",
      "            \"ground_truth_human\": 7248.0,\n",
      "            \"ground_truth_ai\": 7248.0\n",
      "        }\n",
      "    },\n",
      "    \"cnn_news\": {\n",
      "        \"cnn_news\": {\n",
      "            \"f1_human\": 0.9987304274227676,\n",
      "            \"f1_ai\": 0.9987304274227676,\n",
      "            \"f1_weighted\": 0.9987304274227676,\n",
      "            \"accuracy\": 0.9987304274227676,\n",
      "            \"roc_auc\": 0.9987304274227676,\n",
      "            \"f1_human_median\": 0.9987304274227676,\n",
      "            \"f1_ai_median\": 0.9987304274227676,\n",
      "            \"f1_weighted_median\": 0.9987304274227676,\n",
      "            \"accuracy_median\": 0.9987304274227676,\n",
      "            \"roc_auc_median\": 0.9987304274227676,\n",
      "            \"threshold_median\": 0.42889294748601925,\n",
      "            \"ground_truth_human\": 2363.0,\n",
      "            \"ground_truth_ai\": 2363.0\n",
      "        }\n",
      "    },\n",
      "    \"euro_court_cases\": {\n",
      "        \"euro_court_cases\": {\n",
      "            \"f1_human\": 0.9986737400530504,\n",
      "            \"f1_ai\": 0.9986702127659575,\n",
      "            \"f1_weighted\": 0.998671976409504,\n",
      "            \"accuracy\": 0.99867197875166,\n",
      "            \"roc_auc\": 0.99867197875166,\n",
      "            \"f1_human_median\": 0.99867197875166,\n",
      "            \"f1_ai_median\": 0.99867197875166,\n",
      "            \"f1_weighted_median\": 0.99867197875166,\n",
      "            \"accuracy_median\": 0.99867197875166,\n",
      "            \"roc_auc_median\": 0.99867197875166,\n",
      "            \"threshold_median\": 0.31296760725208583,\n",
      "            \"ground_truth_human\": 753.0,\n",
      "            \"ground_truth_ai\": 753.0\n",
      "        }\n",
      "    },\n",
      "    \"house_of_commons\": {\n",
      "        \"house_of_commons\": {\n",
      "            \"f1_human\": 0.9887359198998749,\n",
      "            \"f1_ai\": 0.9886315789473684,\n",
      "            \"f1_weighted\": 0.9886837494236216,\n",
      "            \"accuracy\": 0.9886839899413243,\n",
      "            \"roc_auc\": 0.9886839899413243,\n",
      "            \"f1_human_median\": 0.9899413243922883,\n",
      "            \"f1_ai_median\": 0.9899413243922883,\n",
      "            \"f1_weighted_median\": 0.9899413243922883,\n",
      "            \"accuracy_median\": 0.9899413243922883,\n",
      "            \"roc_auc_median\": 0.9899413243922883,\n",
      "            \"threshold_median\": 0.4611669087790502,\n",
      "            \"ground_truth_human\": 1193.0,\n",
      "            \"ground_truth_ai\": 1193.0\n",
      "        }\n",
      "    },\n",
      "    \"arxiv_papers\": {\n",
      "        \"arxiv_papers\": {\n",
      "            \"f1_human\": 0.9948006932409013,\n",
      "            \"f1_ai\": 0.9947460595446584,\n",
      "            \"f1_weighted\": 0.9947733763927799,\n",
      "            \"accuracy\": 0.9947735191637631,\n",
      "            \"roc_auc\": 0.994773519163763,\n",
      "            \"f1_human_median\": 0.9923344947735192,\n",
      "            \"f1_ai_median\": 0.9923344947735192,\n",
      "            \"f1_weighted_median\": 0.9923344947735192,\n",
      "            \"accuracy_median\": 0.9923344947735192,\n",
      "            \"roc_auc_median\": 0.9923344947735192,\n",
      "            \"threshold_median\": 0.11878554601876505,\n",
      "            \"ground_truth_human\": 1435.0,\n",
      "            \"ground_truth_ai\": 1435.0\n",
      "        }\n",
      "    },\n",
      "    \"gutenberg_en\": {\n",
      "        \"gutenberg_en\": {\n",
      "            \"f1_human\": 0.9635761589403974,\n",
      "            \"f1_ai\": 0.9623287671232876,\n",
      "            \"f1_weighted\": 0.9629524630318426,\n",
      "            \"accuracy\": 0.9629629629629629,\n",
      "            \"roc_auc\": 0.962962962962963,\n",
      "            \"f1_human_median\": 0.9595959595959596,\n",
      "            \"f1_ai_median\": 0.9595959595959596,\n",
      "            \"f1_weighted_median\": 0.9595959595959596,\n",
      "            \"accuracy_median\": 0.9595959595959596,\n",
      "            \"roc_auc_median\": 0.9595959595959596,\n",
      "            \"threshold_median\": 0.43650609973865506,\n",
      "            \"ground_truth_human\": 297.0,\n",
      "            \"ground_truth_ai\": 297.0\n",
      "        }\n",
      "    },\n",
      "    \"bundestag\": {\n",
      "        \"bundestag\": {\n",
      "            \"f1_human\": 0.9823690091879811,\n",
      "            \"f1_ai\": 0.9823427008206914,\n",
      "            \"f1_weighted\": 0.9823558550043363,\n",
      "            \"accuracy\": 0.9823558648111332,\n",
      "            \"roc_auc\": 0.9823558648111332,\n",
      "            \"f1_human_median\": 0.9826043737574552,\n",
      "            \"f1_ai_median\": 0.9826043737574552,\n",
      "            \"f1_weighted_median\": 0.9826043737574552,\n",
      "            \"accuracy_median\": 0.9826043737574552,\n",
      "            \"roc_auc_median\": 0.9826043737574552,\n",
      "            \"threshold_median\": 0.40509988439296896,\n",
      "            \"ground_truth_human\": 2012.0,\n",
      "            \"ground_truth_ai\": 2012.0\n",
      "        }\n",
      "    },\n",
      "    \"spiegel_articles\": {\n",
      "        \"spiegel_articles\": {\n",
      "            \"f1_human\": 0.9744845886915697,\n",
      "            \"f1_ai\": 0.9745158002038736,\n",
      "            \"f1_weighted\": 0.9745001944477216,\n",
      "            \"accuracy\": 0.974500203998368,\n",
      "            \"roc_auc\": 0.974500203998368,\n",
      "            \"f1_human_median\": 0.9747042023663811,\n",
      "            \"f1_ai_median\": 0.9747042023663811,\n",
      "            \"f1_weighted_median\": 0.9747042023663811,\n",
      "            \"accuracy_median\": 0.9747042023663811,\n",
      "            \"roc_auc_median\": 0.9747042023663811,\n",
      "            \"threshold_median\": 0.5103538094189195,\n",
      "            \"ground_truth_human\": 2451.0,\n",
      "            \"ground_truth_ai\": 2451.0\n",
      "        }\n",
      "    },\n",
      "    \"en\": {\n",
      "        \"en\": {\n",
      "            \"f1_human\": 0.9982055522325042,\n",
      "            \"f1_ai\": 0.9982034029661465,\n",
      "            \"f1_weighted\": 0.9982044775993253,\n",
      "            \"accuracy\": 0.998204478242501,\n",
      "            \"roc_auc\": 0.998204478242501,\n",
      "            \"f1_human_median\": 0.9981692719335304,\n",
      "            \"f1_ai_median\": 0.9981692719335304,\n",
      "            \"f1_weighted_median\": 0.9981692719335304,\n",
      "            \"accuracy_median\": 0.9981692719335304,\n",
      "            \"roc_auc_median\": 0.9981692719335304,\n",
      "            \"threshold_median\": 0.39438111386498187,\n",
      "            \"ground_truth_human\": 14202.0,\n",
      "            \"ground_truth_ai\": 14202.0\n",
      "        }\n",
      "    },\n",
      "    \"de\": {\n",
      "        \"de\": {\n",
      "            \"f1_human\": 0.9812368158099256,\n",
      "            \"f1_ai\": 0.9810559354332474,\n",
      "            \"f1_weighted\": 0.9811463756215865,\n",
      "            \"accuracy\": 0.9811468094600625,\n",
      "            \"roc_auc\": 0.9811468094600625,\n",
      "            \"f1_human_median\": 0.9799196787148594,\n",
      "            \"f1_ai_median\": 0.9799196787148594,\n",
      "            \"f1_weighted_median\": 0.9799196787148594,\n",
      "            \"accuracy_median\": 0.9799196787148594,\n",
      "            \"roc_auc_median\": 0.9799196787148594,\n",
      "            \"threshold_median\": 0.27477505647432704,\n",
      "            \"ground_truth_human\": 4482.0,\n",
      "            \"ground_truth_ai\": 4482.0\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    scores_roberta_ft = {}\n",
    "\n",
    "    model = ChatGPTDetector(device=\"cuda:0\")\n",
    "    dataset_items = [\n",
    "        (\n",
    "            k,\n",
    "            dataset.map(\n",
    "                model.tokenize,\n",
    "                input_columns=[\"text\"],\n",
    "                batched=True,\n",
    "                batch_size=1024,\n",
    "                desc=\"Tokenizing\",\n",
    "            ).sort(\"length\"),\n",
    "        )\n",
    "        for k, dataset in datasets.items()\n",
    "    ]\n",
    "\n",
    "    tq = tqdm(dataset_items, desc=\"Finetuning\")\n",
    "    for config, dataset in tq:\n",
    "        tq.set_postfix_str(config)\n",
    "        model.reset()\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"../models/chatgpt-detector-roberta/{config}\",\n",
    "            seed=42,\n",
    "            num_train_epochs=1,\n",
    "            per_device_train_batch_size=15,\n",
    "            per_device_eval_batch_size=30,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=1e-5,\n",
    "        )\n",
    "        model.train(dataset, training_args)\n",
    "\n",
    "        scores_roberta_ft[config] = run_detector_tokenized(model, {config: dataset})\n",
    "\n",
    "        path = Path(\"../logs/chatgpt-detector-roberta/\")\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        with (path / f\"{config}.json\").open(\"w\") as fp:\n",
    "            json.dump(scores_roberta_ft[config], fp, indent=4)\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    return scores_roberta_ft\n",
    "\n",
    "\n",
    "scores_roberta_ft = f()\n",
    "print(json.dumps(scores_roberta_ft, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RADAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from: RAID, Dugan et al. 2024\n",
    "# > https://github.com/liamdugan/raid/blob/main/detectors/models/radar/radar.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "\n",
    "class Radar:\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = torch.device(device)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"TrustSafeAI/RADAR-Vicuna-7B\",  # cache_dir=os.environ[\"CACHE_DIR\"]\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"TrustSafeAI/RADAR-Vicuna-7B\")\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def tokenize(self, texts: list[str]) -> BatchEncoding:\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_length=True,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, inputs: dict) -> list[float]:\n",
    "        encoding = self.tokenizer.pad(inputs, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model(**encoding)\n",
    "        output_probs = F.log_softmax(outputs.logits, -1)[:, 0].exp().tolist()\n",
    "        return output_probs\n",
    "\n",
    "    def process(self, inputs: dict) -> dict[str, list[float]]:\n",
    "        return {\n",
    "            \"prediction\": self.predict(\n",
    "                {\n",
    "                    \"input_ids\": inputs[\"input_ids\"],\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def inference(self, texts: list) -> list:\n",
    "        predictions = []\n",
    "        for text in tqdm(texts):\n",
    "            with torch.no_grad():\n",
    "                inputs = self.tokenizer(\n",
    "                    [text],\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                output_probs = (\n",
    "                    F.log_softmax(self.model(**inputs).logits, -1)[:, 0].exp().tolist()\n",
    "                )\n",
    "            predictions.append(output_probs[0])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f():\n",
    "    results = run_detector(Radar(device=\"cuda:3\"), datasets)\n",
    "    gc.collect()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "\n",
    "scores_radar = f()\n",
    "scores_radar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binoculars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: RAID, Dugan et al. 2024\n",
    "# > https://github.com/liamdugan/raid/blob/main/detectors/models/binoculars/utils/metrics.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "ce_loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "softmax_fn = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "def perplexity(\n",
    "    encoding: transformers.BatchEncoding,\n",
    "    logits: torch.Tensor,\n",
    "    median: bool = False,\n",
    "    temperature: float = 1.0,\n",
    "):\n",
    "    shifted_logits = logits[..., :-1, :].contiguous() / temperature\n",
    "    shifted_labels = encoding.input_ids[..., 1:].contiguous()\n",
    "    shifted_attention_mask = encoding.attention_mask[..., 1:].contiguous()\n",
    "\n",
    "    if median:\n",
    "        ce_nan = ce_loss_fn(shifted_logits.transpose(1, 2), shifted_labels).masked_fill(\n",
    "            ~shifted_attention_mask.bool(), float(\"nan\")\n",
    "        )\n",
    "        ppl = np.nanmedian(ce_nan.cpu().float().numpy(), 1)\n",
    "\n",
    "    else:\n",
    "        ppl = (\n",
    "            ce_loss_fn(shifted_logits.transpose(1, 2), shifted_labels)\n",
    "            * shifted_attention_mask\n",
    "        ).sum(1) / shifted_attention_mask.sum(1)\n",
    "        ppl = ppl.to(\"cpu\").float().numpy()\n",
    "\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def entropy(\n",
    "    p_logits: torch.Tensor,\n",
    "    q_logits: torch.Tensor,\n",
    "    encoding: transformers.BatchEncoding,\n",
    "    pad_token_id: int,\n",
    "    median: bool = False,\n",
    "    sample_p: bool = False,\n",
    "    temperature: float = 1.0,\n",
    "):\n",
    "    vocab_size = p_logits.shape[-1]\n",
    "    total_tokens_available = q_logits.shape[-2]\n",
    "    p_scores, q_scores = p_logits / temperature, q_logits / temperature\n",
    "\n",
    "    p_proba = softmax_fn(p_scores).view(-1, vocab_size)\n",
    "\n",
    "    if sample_p:\n",
    "        p_proba = torch.multinomial(\n",
    "            p_proba.view(-1, vocab_size), replacement=True, num_samples=1\n",
    "        ).view(-1)\n",
    "\n",
    "    q_scores = q_scores.view(-1, vocab_size)\n",
    "\n",
    "    ce = ce_loss_fn(input=q_scores, target=p_proba).view(-1, total_tokens_available)\n",
    "    padding_mask = (encoding.input_ids != pad_token_id).type(torch.uint8)\n",
    "\n",
    "    if median:\n",
    "        ce_nan = ce.masked_fill(~padding_mask.bool(), float(\"nan\"))\n",
    "        agg_ce = np.nanmedian(ce_nan.cpu().float().numpy(), 1)\n",
    "    else:\n",
    "        agg_ce = (\n",
    "            ((ce * padding_mask).sum(1) / padding_mask.sum(1)).to(\"cpu\").float().numpy()\n",
    "        )\n",
    "\n",
    "    return agg_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from: RAID, Dugan et al. 2024\n",
    "# > https://github.com/liamdugan/raid/blob/main/detectors/models/binoculars/binoculars.py\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "GLOBAL_BINOCULARS_THRESHOLD = (\n",
    "    0.9015310749276843  # selected using Falcon-7B and Falcon-7B-Instruct at bfloat16\n",
    ")\n",
    "# DEVICE_1 = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# DEVICE_2 = \"cuda:1\" if torch.cuda.device_count() > 1 else DEVICE_1\n",
    "DEVICE_1 = \"cuda:2\"\n",
    "DEVICE_2 = \"cuda:3\"\n",
    "\n",
    "\n",
    "class Binoculars(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observer_name_or_path: str = \"tiiuae/falcon-7b\",\n",
    "        performer_name_or_path: str = \"tiiuae/falcon-7b-instruct\",\n",
    "        use_bfloat16: bool = True,\n",
    "        max_token_observed: int = 512,\n",
    "    ) -> None:\n",
    "        # assert_tokenizer_consistency(observer_name_or_path, performer_name_or_path)\n",
    "\n",
    "        self.observer_model = AutoModelForCausalLM.from_pretrained(\n",
    "            observer_name_or_path,\n",
    "            device_map={\"\": DEVICE_1},\n",
    "            trust_remote_code=True,\n",
    "            # cache_dir=os.environ[\"CACHE_DIR\"],\n",
    "            torch_dtype=torch.bfloat16 if use_bfloat16 else torch.float32,\n",
    "        )\n",
    "        self.performer_model = AutoModelForCausalLM.from_pretrained(\n",
    "            performer_name_or_path,\n",
    "            device_map={\"\": DEVICE_2},\n",
    "            trust_remote_code=True,\n",
    "            # cache_dir=os.environ[\"CACHE_DIR\"],\n",
    "            torch_dtype=torch.bfloat16 if use_bfloat16 else torch.float32,\n",
    "        )\n",
    "\n",
    "        self.observer_model.eval()\n",
    "        self.performer_model.eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(observer_name_or_path)\n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.max_token_observed = max_token_observed\n",
    "\n",
    "    def tokenize(self, texts: list[str]) -> BatchEncoding:\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_token_observed,\n",
    "            return_length=True,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def _get_logits(\n",
    "        self, encodings: transformers.BatchEncoding\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        observer_logits = self.observer_model(\n",
    "            **encodings.to(self.observer_model.device)\n",
    "        ).logits\n",
    "        performer_logits = self.performer_model(\n",
    "            **encodings.to(self.performer_model.device)\n",
    "        ).logits\n",
    "        if DEVICE_1 != \"cpu\":\n",
    "            torch.cuda.synchronize()\n",
    "        return observer_logits, performer_logits\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, inputs: dict) -> list[float]:\n",
    "        encodings = self.tokenizer.pad(inputs, return_tensors=\"pt\")\n",
    "        observer_logits, performer_logits = self._get_logits(encodings)\n",
    "        ppl = perplexity(encodings, performer_logits)\n",
    "        x_ppl = entropy(\n",
    "            observer_logits.to(DEVICE_1),\n",
    "            performer_logits.to(DEVICE_1),\n",
    "            encodings.to(DEVICE_1),\n",
    "            self.tokenizer.pad_token_id,\n",
    "        )\n",
    "        binoculars_scores = ppl / x_ppl\n",
    "        return binoculars_scores.tolist()\n",
    "\n",
    "    def process(self, inputs: dict) -> dict[str, list[float]]:\n",
    "        return {\n",
    "            \"prediction\": self.predict(\n",
    "                {\n",
    "                    \"input_ids\": inputs[\"input_ids\"],\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                }\n",
    "            )\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching\n",
    "\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f():\n",
    "    results = run_detector(Binoculars(), datasets, batch_size=16)\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "\n",
    "scores_binoculars = f()\n",
    "scores_binoculars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E5-Small LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from: RAID, Dugan et al. 2024\n",
    "# > https://github.com/liamdugan/raid/blob/main/detectors/models/radar/radar.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "\n",
    "class E5Lora:\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = torch.device(device)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"MayZhou/e5-small-lora-ai-generated-detector\"\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"MayZhou/e5-small-lora-ai-generated-detector\"\n",
    "        )\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def tokenize(self, texts: list[str]) -> BatchEncoding:\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_length=True,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, inputs: dict) -> list[float]:\n",
    "        encoding = self.tokenizer.pad(inputs, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model(**encoding)\n",
    "        output_probs = F.log_softmax(outputs.logits, -1)[:, 0].exp().tolist()\n",
    "        return output_probs\n",
    "\n",
    "    def process(self, inputs: dict) -> dict[str, list[float]]:\n",
    "        return {\n",
    "            \"prediction\": self.predict(\n",
    "                {\n",
    "                    \"input_ids\": inputs[\"input_ids\"],\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def inference(self, texts: list) -> list:\n",
    "        predictions = []\n",
    "        for text in tqdm(texts):\n",
    "            with torch.no_grad():\n",
    "                inputs = self.tokenizer(\n",
    "                    [text],\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                output_probs = (\n",
    "                    F.log_softmax(self.model(**inputs).logits, -1)[:, 0].exp().tolist()\n",
    "                )\n",
    "            predictions.append(output_probs[0])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def f():\n",
    "    results = run_detector(E5Lora(device=\"cuda:0\"), datasets)\n",
    "    gc.collect()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "\n",
    "scores_e5 = f()\n",
    "\n",
    "with open(\"../logs/e5-small-lora.json\", \"w\") as fp:\n",
    "    json.dump(scores_e5, fp, indent=4)\n",
    "\n",
    "print(json.dumps(scores_e5, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\n",
    "    \"Web Blogs\",\n",
    "    \"Essays\",\n",
    "    \"CNN\",\n",
    "    \"ECHR\",\n",
    "    \"HoC\",\n",
    "    \"arXiv\",\n",
    "    \"Gutenberg$_{en}$\",\n",
    "    \"Bundestag$_{de}$\",\n",
    "    \"Spiegel$_{de}$\",\n",
    "    \"Gutenberg$_{de}$\",\n",
    "    \"All$_{en}$\",\n",
    "    \"All$_{de}$\",\n",
    "]\n",
    "\n",
    "name_map = {\n",
    "    \"blog_authorship_corpus\": \"Web Blogs\",\n",
    "    \"student_essays\": \"Essays\",\n",
    "    \"cnn_news\": \"CNN\",\n",
    "    \"euro_court_cases\": \"ECHR\",\n",
    "    \"house_of_commons\": \"HoC\",\n",
    "    \"arxiv_papers\": \"arXiv\",\n",
    "    \"gutenberg_en\": \"Gutenberg$_{en}$\",\n",
    "    \"bundestag\": \"Bundestag$_{de}$\",\n",
    "    \"spiegel_articles\": \"Spiegel$_{de}$\",\n",
    "    \"gutenberg_de\": \"Gutenberg$_{de}$\",\n",
    "    \"en\": \"All$_{en}$\",\n",
    "    \"de\": \"All$_{de}$\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = defaultdict(dict)\n",
    "for path in Path(\"../logs/\").iterdir():\n",
    "    if path.suffix == \".json\":\n",
    "        with path.open(\"r\") as fp:\n",
    "            data = json.load(fp)\n",
    "        model_name = path.stem\n",
    "        for domain, scores in data.items():\n",
    "            results[name_map[domain]].update(\n",
    "                {\n",
    "                    model_name + \"_f1\": scores[\"f1\"],\n",
    "                    model_name + \"_accuracy\": scores[\"accuracy\"],\n",
    "                    model_name + \"_auroc\": scores[\"auroc\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "model_name = \"roberta-ft\"\n",
    "for domain, name in name_map.items():\n",
    "    path = Path(\"../logs/chatgpt-detector-roberta/\") / (domain + \".json\")\n",
    "    with (path).open(\"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    results[name].update(\n",
    "        {\n",
    "            model_name + \"_f1\": data[domain][\"test_f1_\"],\n",
    "            model_name + \"_accuracy\": data[domain][\"accuracy\"],\n",
    "            model_name + \"_auroc\": data[domain][\"auroc\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "metric_df = (\n",
    "    pd.DataFrame([{\"domain\": domain} | dd for domain, dd in results.items()])\n",
    "    .set_index(\"domain\")\n",
    "    .sort_index(key=lambda x: list(map(domains.index, x)))\n",
    ")\n",
    "print(metric_df.to_latex(float_format=\"%.3f\", index=True))\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(dict)\n",
    "\n",
    "model_name = \"roberta-ft\"\n",
    "for domain, name in name_map.items():\n",
    "    path = Path(\"../logs/chatgpt-detector-roberta/\") / (domain + \".json\")\n",
    "    with (path).open(\"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    results[name].update(\n",
    "        {\n",
    "            model_name + \"_f1\": data[domain][\"f1\"],\n",
    "            # model_name + \"_accuracy\": data[domain][\"accuracy\"],\n",
    "            model_name + \"_auroc\": data[domain][\"auroc\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "metric_df = (\n",
    "    pd.DataFrame([{\"domain\": domain} | dd for domain, dd in results.items()])\n",
    "    .set_index(\"domain\")\n",
    "    .sort_index(key=lambda x: list(map(domains.index, x)))\n",
    ")\n",
    "print(metric_df.to_latex(float_format=\"%.3f\", index=True))\n",
    "metric_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# About\n",
    "\n",
    "Training of LuminarSequenceClassifier on the PrismAI dataset."
   ],
   "id": "f1cac2df18338774"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:28:26.762715Z",
     "start_time": "2025-07-29T08:28:25.576944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "66a5cfd8c7d2051e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T11:36:41.189363Z",
     "start_time": "2025-07-29T11:36:40.007702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "from typing import Final, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from IPython.display import display, Markdown\n",
    "from datasets import load_dataset\n",
    "from numpy._typing import NDArray\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from datasets import DatasetDict\n",
    "from src.luminar.utils.data import get_pad_to_fixed_length_fn, get_matched_datasets\n",
    "from src.luminar.utils.training import ConvolutionalLayerSpec\n",
    "from src.luminar.encoder import LuminarEncoder\n",
    "\n",
    "import numpy as np\n",
    "import glob"
   ],
   "id": "5a16e47050282fde",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:28:27.010928Z",
     "start_time": "2025-07-29T08:28:26.959082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    HF_TOKEN: Final[str] = (Path.home() / \".hf_token\").read_text().strip()\n",
    "    #DATASET_PATH: Final[str] = \"liberi-luminaris/PrismAI-encoded-gpt2\"\n",
    "    DATASET_ROOT_PATH: Final[str] = \"/storage/projects/stoeckel/prismai/encoded/fulltext/\"\n",
    "    #DATASET_ROOT_PATH: Final[str] = \"/mnt/c/home/projects/prismAI/data/encoded/fulltext/\"\n",
    "    NUM_INTERMEDIATE_LIKELIHOODS: Final[int] = 13\n",
    "    FEATURE_LEN = 512\n",
    "    SEED = 42\n"
   ],
   "id": "a9172b7cf1e56309",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading & Preprocessing the datasets\n",
    "\n",
    "Load the datasets for the training."
   ],
   "id": "7a319dd3c792f5ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:28:27.116580Z",
     "start_time": "2025-07-29T08:28:27.065977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We need that to sentence tokenize the text\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "id": "c8c1f57c27d96d42",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/staff_homes/kboenisc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T13:23:52.985584Z",
     "start_time": "2025-07-29T13:23:51.860274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sentence_to_token_spans(text: str, span_min_length : int = -1) -> list[tuple[int, int]]:\n",
    "    \"\"\"Return a list of (start_token_idx, end_token_idx) for each sentence in the text.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    spans = []\n",
    "    current_token_idx = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        tokens = luminar_encoder.tokenize(sent)[\"input_ids\"]\n",
    "        token_count = len(tokens)\n",
    "\n",
    "        start = current_token_idx\n",
    "        end = min(current_token_idx + token_count, Config.FEATURE_LEN)\n",
    "\n",
    "        if start >= Config.FEATURE_LEN:\n",
    "            break  # Stop if padding region or overflow\n",
    "        # Testing: only take spans of a minimal length\n",
    "        if end - start > span_min_length:\n",
    "            spans.append((start, end))\n",
    "        current_token_idx = end\n",
    "\n",
    "    return spans\n"
   ],
   "id": "5bfc20f0f811508",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T16:36:42.537922Z",
     "start_time": "2025-07-29T16:36:41.512138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "domains = ['bundestag', 'student_essays']\n",
    "agents = ['gpt_4o_mini_gemma2_9b']\n",
    "feature_agents = ['gpt2_512']\n",
    "\n",
    "luminar_encoder = LuminarEncoder(max_len=Config.FEATURE_LEN)"
   ],
   "id": "7ae3dbbad3613837",
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T16:42:47.459712Z",
     "start_time": "2025-07-29T16:36:44.665665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = {}\n",
    "\n",
    "pad_to_fixed_length: Callable[[NDArray], NDArray] = get_pad_to_fixed_length_fn(Config.FEATURE_LEN)\n",
    "\n",
    "for domain in domains:\n",
    "    for agent in agents:\n",
    "        for feature_agent in feature_agents:\n",
    "            dataset_path = Path(Config.DATASET_ROOT_PATH) / agent / feature_agent / domain\n",
    "\n",
    "            if not dataset_path.exists():\n",
    "                raise FileNotFoundError(f\"Dataset path {dataset_path} does not exist.\")\n",
    "\n",
    "            data_files = {\n",
    "                \"train\": sorted(str(f) for f in dataset_path.glob(\"train/*.arrow\")),\n",
    "                \"test\": sorted(str(f) for f in dataset_path.glob(\"test/*.arrow\")),\n",
    "                \"eval\": sorted(str(f) for f in dataset_path.glob(\"eval/*.arrow\")),\n",
    "            }\n",
    "            data_files = {k: v for k, v in data_files.items() if v}\n",
    "\n",
    "            # These datasets are already matched, so we can load them directly\n",
    "            # We need the tokenized text since we label sequences based on sentences.\n",
    "            dataset_dict = (\n",
    "                load_dataset(\n",
    "                    \"arrow\",\n",
    "                    data_files=data_files,\n",
    "                )\n",
    "                .map(\n",
    "                    lambda batch: {\n",
    "                        \"tokenized_text\": [\n",
    "                            pad_to_fixed_length(\n",
    "                                np.array(luminar_encoder.tokenize(t)[\"input_ids\"]).reshape(-1, 1)\n",
    "                            )\n",
    "                            for t in batch[\"text\"]\n",
    "                        ],\n",
    "                        \"sentence_token_spans\": [\n",
    "                            sentence_to_token_spans(t)\n",
    "                            for t in batch[\"text\"]\n",
    "                        ],\n",
    "                    },\n",
    "                    batched=True,\n",
    "                    desc=\"Tokenizing, padding, and aligning sentences\"\n",
    "                )\n",
    "                .map(\n",
    "                    lambda batch: {\n",
    "                        \"span_labels\": [\n",
    "                            [label] * len(spans)\n",
    "                            for label, spans in zip(batch[\"labels\"], batch[\"sentence_token_spans\"])\n",
    "                        ]\n",
    "                    },\n",
    "                    batched=True,\n",
    "                    desc=\"Assigning labels to sentence spans\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            datasets.setdefault(domain, {}).setdefault(agent, {})[feature_agent] = dataset_dict\n",
    "            print(f\"Loaded dataset for domain '{domain}' with agent '{agent}' and feature agent '{feature_agent}'\")\n",
    "\n",
    "datasets"
   ],
   "id": "d6f5d8ba7e620bec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d983be714c7b4a789945c0677fbf2ab3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38721a395d554f8b892d127a5b4e65dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating eval split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f057fe25008c4950b097cf6bbeb7617f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/14078 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0552d277fef4a61b042a312c36e84a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/4024 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf14c126a2ab4bcd9431dc5d6ccd0b99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/2012 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1827268808d5465e9a327c2e0fc471a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/14078 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93dce34c85624054a8698db54aa841c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/4024 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2194cc8ad0d04ae490a4499a8b728214"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/2012 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e935fc3622e44e7926c1e08534eb4d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset for domain 'bundestag' with agent 'gpt_4o_mini_gemma2_9b' and feature agent 'gpt2_512'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/50734 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bbba06f025f43dab3363b362d5e50f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/14496 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aaf83137dd4b4e2e90ccb9fa5abddbad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/7248 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e51be3f9f9d4c7c893ea02b0dd78d13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/50734 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bc0fbb68f2349f49ef6d07b74ff6cb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/14496 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bb44bf015854efa8163537271aba214"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/7248 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c470ca8f09004395820e50e80c8ef402"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset for domain 'student_essays' with agent 'gpt_4o_mini_gemma2_9b' and feature agent 'gpt2_512'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bundestag': {'gpt_4o_mini_gemma2_9b': {'gpt2_512': DatasetDict({\n",
       "       train: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 14078\n",
       "       })\n",
       "       test: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 4024\n",
       "       })\n",
       "       eval: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 2012\n",
       "       })\n",
       "   })}},\n",
       " 'student_essays': {'gpt_4o_mini_gemma2_9b': {'gpt2_512': DatasetDict({\n",
       "       train: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 50734\n",
       "       })\n",
       "       test: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 14496\n",
       "       })\n",
       "       eval: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 7248\n",
       "       })\n",
       "   })}}}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T16:44:27.544084Z",
     "start_time": "2025-07-29T16:44:26.433043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sanity check\n",
    "idx = 0\n",
    "for domain, agents_dict in datasets.items():\n",
    "    for agent, feature_agents_dict in agents_dict.items():\n",
    "        for feature_agent, dataset in feature_agents_dict.items():\n",
    "            md = f\"\"\"\n",
    "**Domain:** `{domain}`\n",
    "**Agent:** `{agent}`\n",
    "**Feature Agent:** `{feature_agent}`\n",
    "\n",
    "**Train:** {len(dataset['train'])}\n",
    "**Test:** {len(dataset['test'])}\n",
    "**Eval:** {len(dataset['eval'])}\n",
    "\n",
    "**Example-Features:**\n",
    "`{dataset['train'][idx]['features'][:2]}...`\n",
    "\n",
    "**Feature-Shape:**\n",
    "`{np.asarray(dataset['train'][idx]['features']).shape}`\n",
    "\n",
    "**Example text:**\n",
    "`{dataset['train'][idx]['text']}`\n",
    "\n",
    "**Example-Tokenized Text:**\n",
    "`{dataset['train'][idx]['tokenized_text'][:10]}...`\n",
    "\n",
    "**Tokenized Text Shape:**\n",
    "`{np.asarray(dataset['train'][idx]['tokenized_text']).shape}`\n",
    "\n",
    "**Sentence-Token-Spans:**\n",
    "`{dataset['train'][idx]['sentence_token_spans']}`\n",
    "\n",
    "**Example Sentence-Token Span decoded:**\n",
    "`{luminar_encoder.tokenizer.decode(np.asarray(dataset['train'][idx]['tokenized_text'])[dataset['train'][idx]['sentence_token_spans'][0][0]:dataset['train'][idx]['sentence_token_spans'][0][1]].flatten().tolist())}`\n",
    "\n",
    "**Example span labels:**\n",
    "`{dataset['train'][idx]['span_labels']}`\n",
    "\n",
    "**Example label:**\n",
    "`{dataset['train'][idx]['labels']}`\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "            display(Markdown(md))"
   ],
   "id": "98f45ae307339095",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n**Domain:** `bundestag`\n**Agent:** `gpt_4o_mini_gemma2_9b`\n**Feature Agent:** `gpt2_512`\n\n**Train:** 14078\n**Test:** 4024\n**Eval:** 2012\n\n**Example-Features:**\n`[[1.8125965652870946e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00021406936866696924], [1.058023485711601e-06, 6.257003803966654e-08, 1.0760977176005326e-07, 1.8288755398998546e-08, 5.383418155702202e-10, 4.788487661944174e-11, 1.5573458321538336e-13, 2.799126584192487e-17, 1.2352709805983406e-22, 1.2212475851755014e-28, 6.798245356840774e-40, 0.0, 0.00011042043479392305]]...`\n\n**Feature-Shape:**\n`(512, 13)`\n\n**Example text:**\n`Sehr geehrte Damen und Herren, heute stehen wir hier im Deutschen Bundestag, um über ein Thema zu diskutieren, das für unsere Gesellschaft von entscheidender Bedeutung ist: die Gesundheitspolitik in Deutschland. Die Herausforderungen, vor denen unser Gesundheitssystem steht, sind nicht nur vielschichtig, sondern auch von drängender Aktualität, insbesondere im Kontext der COVID-19-Pandemie und ihrer weitreichenden Folgen. Es ist an der Zeit, dass wir uns den Fragen der Finanzierung und der strukturellen Reformen stellen, die notwendig sind, um ein Gesundheitssystem zu gewährleisten, das allen Bürgerinnen und Bürgern zugutekommt. Die Pandemie hat nicht nur die Schwächen unseres Gesundheitssystems offengelegt, sondern auch die Dringlichkeit von Reformen unterstrichen. Wir haben erlebt, wie überlastet unsere Krankenhäuser waren, wie das Pflegepersonal am Limit seiner Belastbarkeit operierte und wie die fehlende Ausstattung in vielen Einrichtungen zu einem echten Problem wurde. Diese Herausforderungen sind nicht neu, sie sind jedoch durch die Pandemie in den Fokus gerückt worden. Wir können nicht länger ignorieren, dass wir in der Vergangenheit versäumt haben, in die Zukunft unserer Gesundheitsversorgung zu investieren. Ein zentraler Punkt in dieser Debatte ist die Finanzierung des Gesundheitssystems. Der Gesundheitsfonds, der ursprünglich geschaffen wurde, um die Finanzierung der gesetzlichen Krankenversicherung zu stabilisieren, steht heute vor ernsthaften Herausforderungen. Die steigenden Kosten im Gesundheitswesen, die durch die COVID-19-Pandemie noch verstärkt wurden, erfordern ein Umdenken in der Finanzierungsstrategie. Wir müssen sicherstellen, dass die Mittel dort ankommen, wo sie am dringendsten benötigt werden – in den Kliniken, bei den Pflegekräften und in der Prävention. Die Bundesregierung hat in der Vergangenheit zahlreiche Maßnahmen ergriffen, um die finanziellen Belastungen während der Pandemie abzufedern. Doch ich frage Sie: Reichen diese Maßnahmen aus? Sind wir bereit, die Lehren aus dieser Krise zu ziehen und die notwendigen Schritte zu unternehmen, um unser Gesundheitssystem nachhaltig zu stärken? Ich bin der Überzeugung, dass wir nicht nur kurzfristige Lösungen suchen dürfen, sondern auch langfristige Strategien entwickeln müssen, die die strukturellen Probleme angehen. Ein weiterer Aspekt, den wir nicht außer Acht lassen dürfen, ist die Digitalisierung im Gesundheitswesen. Die Pandemie hat uns gezeigt, wie wichtig digitale Lösungen sind, um die Patientenversorgung zu verbessern und den Austausch von Informationen zu erleichtern. Dennoch stehen wir vor der Herausforderung, dass viele Einrichtungen nicht ausreichend digitalisiert sind. Hier müssen wir investieren, um die digitale Infrastruktur auszubauen und sicherzustellen, dass alle Bürgerinnen und Bürger von den Vorteilen der Digitalisierung profitieren können. Es ist auch unerlässlich, dass wir die Rolle der Pflegekräfte in unserer Gesellschaft neu bewerten. Die Pandemie hat uns eindringlich vor Augen geführt, wie wichtig das Pflegepersonal für die Funktionsfähigkeit unseres Gesundheitssystems ist. Dennoch sehen wir, dass die Arbeitsbedingungen und die Bezahlung in vielen Bereichen unzureichend sind. Wir müssen die Wertschätzung für diese Berufe erhöhen und die Rahmenbedingungen so gestalten, dass Pflegekräfte ihre wichtige Arbeit unter würdigen Bedingungen leisten können. Darüber hinaus müssen wir auch die Prävention in den Fokus rücken. Die COVID-19-Pandemie hat uns gelehrt, wie wichtig es ist, frühzeitig zu handeln, um Gesundheitsrisiken zu minimieren. Investitionen in präventive Maßnahmen sind nicht nur eine Frage der Gerechtigkeit, sondern auch eine ökonomische Notwendigkeit. Wir sollten uns nicht nur auf die Behandlung von Krankheiten konzentrieren, sondern auch auf deren Verhinderung. Hierzu gehört auch die Förderung von gesundheitsbewusstem Verhalten in der Bevölkerung und der Zugang zu Informationen über gesunde Lebensweisen. Ich möchte auch die Bedeutung der Zusammenarbeit zwischen Bund, Ländern und Kommunen betonen. Nur wenn wir gemeinsam an einem Strang ziehen, können wir die Herausforderungen, vor denen unser Gesundheitssystem steht, bewältigen. Es ist wichtig, dass wir die verschiedenen Akteure im Gesundheitswesen – von den Ärzten über die Krankenhäuser bis hin zu den Pflegeeinrichtungen – in den Reformprozess einbeziehen. Nur so können wir sicherstellen, dass die Lösungen, die wir entwickeln, auch tatsächlich den Bedürfnissen der Menschen entsprechen. Abschließend möchte ich betonen, dass wir jetzt die Chance haben, die Weichen für die Zukunft unseres Gesundheitssystems zu stellen. Lassen Sie uns diese Gelegenheit nutzen, um die notwendigen Reformen einzuleiten und ein Gesundheitssystem zu schaffen, das nicht nur auf Krisen reagiert, sondern proaktiv handelt. Wir müssen die Herausforderungen annehmen und gemeinsam für ein Gesundheitssystem kämpfen, das allen Bürgerinnen und Bürgern gerecht wird. Ich fordere Sie alle auf, sich an dieser wichtigen Debatte zu beteiligen. Lassen Sie uns gemeinsam für eine bessere Gesundheitspolitik in Deutschland eintreten, die den Menschen in den Mittelpunkt stellt und die Herausforderungen der Zukunft aktiv angeht. Vielen Dank für Ihre Aufmerksamkeit.`\n\n**Example-Tokenized Text:**\n`[[4653], [11840], [308], [1453], [11840], [660], [5245], [268], [3318], [2332]]...`\n\n**Tokenized Text Shape:**\n`(512, 1)`\n\n**Sentence-Token-Spans:**\n`[[0, 78], [78, 158], [158, 237], [237, 283], [283, 360], [360, 401], [401, 455], [455, 484], [484, 512]]`\n\n**Example Sentence-Token Span decoded:**\n`Sehr geehrte Damen und Herren, heute stehen wir hier im Deutschen Bundestag, um über ein Thema zu diskutieren, das für unsere Gesellschaft von entscheidender Bedeutung ist: die Gesundheitspolitik in Deutschland.`\n\n**Example span labels:**\n`[1, 1, 1, 1, 1, 1, 1, 1, 1]`\n\n**Example label:**\n`1`\n\n---\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n**Domain:** `student_essays`\n**Agent:** `gpt_4o_mini_gemma2_9b`\n**Feature Agent:** `gpt2_512`\n\n**Train:** 50734\n**Test:** 14496\n**Eval:** 7248\n\n**Example-Features:**\n`[[4.4132913899375126e-05, 4.203895392974451e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00015184479707386345], [9.98157247522613e-06, 2.268475629563227e-09, 2.4706817147723825e-10, 2.598772308459729e-10, 4.608947598572222e-11, 2.177385494128714e-10, 7.711160811448015e-13, 6.366168985201537e-13, 2.281021511211531e-17, 9.533206545892253e-25, 7.987888483894126e-31, 0.0, 0.029878340661525726]]...`\n\n**Feature-Shape:**\n`(512, 13)`\n\n**Example text:**\n`[Your Name] [Your Address] [City, State, Zip Code] [Email Address] [Date] The Honorable [Senator's Name] [Senator's Office Address] [City, State, Zip Code] Dear Senator [Senator's Last Name], I hope this letter finds you well. I’m writing to you today as a concerned citizen who’s really worried about the way our presidential elections work. Honestly, I can’t wrap my head around the Electoral College anymore. It feels like an outdated system that just doesn’t represent us, the people. I mean, how is it fair that a candidate can win the presidency without winning the popular vote? It’s like saying my vote doesn’t count just because I live in a state that leans one way or another. That doesn’t seem right, does it? We should all have an equal say in who leads our country, and the popular vote is the only way to truly reflect the will of the people. The Electoral College creates a situation where some votes are more valuable than others, and that’s just plain wrong. It leads to candidates ignoring vast swathes of the country because they know they won’t win those states. Isn’t it time we changed that? We need a system that encourages candidates to engage with all of us, not just the folks in swing states. I urge you to consider advocating for a popular vote system. It’s time for our elections to truly represent the voice of the people. Thank you for your time, and I hope you’ll take my concerns to heart.`\n\n**Example-Tokenized Text:**\n`[[58], [7120], [6530], [60], [685], [7120], [17917], [60], [685], [14941]]...`\n\n**Tokenized Text Shape:**\n`(512, 1)`\n\n**Sentence-Token-Spans:**\n`[[0, 61], [61, 87], [87, 103], [103, 121], [121, 141], [141, 168], [168, 179], [179, 209], [209, 233], [233, 256], [256, 266], [266, 288], [288, 300], [300, 317], [317, 336]]`\n\n**Example Sentence-Token Span decoded:**\n`[Your Name] [Your Address] [City, State, Zip Code] [Email Address] [Date] The Honorable [Senator's Name] [Senator's Office Address] [City, State, Zip Code] Dear Senator [Senator's Last Name], I hope this letter finds you well.`\n\n**Example span labels:**\n`[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]`\n\n**Example label:**\n`1`\n\n---\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 211
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Loaders\n",
    "\n",
    "Create the data loaders for training and evaluation."
   ],
   "id": "51b81189b4cd0958"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T16:44:32.437851Z",
     "start_time": "2025-07-29T16:44:32.389216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LuminarSequenceDataset(Dataset):\n",
    "    def __init__(self, dataset, feature_key=\"features\"):\n",
    "        self.samples = []\n",
    "        for example in dataset:\n",
    "            spans = example[\"sentence_token_spans\"]\n",
    "            features = torch.tensor(example[feature_key])  # (seq_len, feature_dim)\n",
    "            labels = example[\"span_labels\"]\n",
    "            self.samples.append({\n",
    "                \"features\": features,\n",
    "                \"sentence_spans\": spans,\n",
    "                \"span_labels\": labels\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features = [item[\"features\"] for item in batch]\n",
    "    sentence_spans = [item[\"sentence_spans\"] for item in batch]\n",
    "    span_labels = [item[\"span_labels\"] for item in batch]\n",
    "    features = torch.stack(features)\n",
    "    return {\n",
    "        \"features\": features,\n",
    "        \"sentence_spans\": sentence_spans,\n",
    "        \"span_labels\": span_labels\n",
    "    }"
   ],
   "id": "d50cd8ab23c00195",
   "outputs": [],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T16:50:17.350627Z",
     "start_time": "2025-07-29T16:44:32.485926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_datasets = []\n",
    "test_loaders = []\n",
    "\n",
    "all_domains_train_datasets = Dataset()\n",
    "all_domains_test_loader = Dataset()\n",
    "for domain in domains:\n",
    "    print(f\"Creating datasets for domain: {domain}\")\n",
    "    # Since we got CV, we can merge the eval and train datasets\n",
    "    train_dataset = LuminarSequenceDataset(\n",
    "        ConcatDataset([datasets[domain][\"gpt_4o_mini_gemma2_9b\"][\"gpt2_512\"][\"train\"],\n",
    "                       datasets[domain][\"gpt_4o_mini_gemma2_9b\"][\"gpt2_512\"][\"eval\"]]))\n",
    "    print(f\"Train Dataset: {len(train_dataset)}\")\n",
    "    train_datasets.append((domain, train_dataset))\n",
    "    ConcatDataset([all_domains_train_datasets, train_dataset])\n",
    "\n",
    "    test_dataset = LuminarSequenceDataset(datasets[domain][\"gpt_4o_mini_gemma2_9b\"][\"gpt2_512\"][\"test\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "    print(f\"Test Dataset: {len(test_dataset)}\")\n",
    "    test_loaders.append((domain, test_loader))\n",
    "    ConcatDataset([all_domains_test_loader, test_dataset])\n",
    "\n",
    "train_datasets.append((\"all\", all_domains_train_datasets))\n",
    "test_loaders.append((\"all\", all_domains_test_loader))"
   ],
   "id": "ebcb886877bdde06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets for domain: bundestag\n",
      "Train Dataset: 16090\n",
      "Test Dataset: 4024\n",
      "Creating datasets for domain: student_essays\n",
      "Train Dataset: 57982\n",
      "Test Dataset: 14496\n"
     ]
    }
   ],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T16:50:18.533112Z",
     "start_time": "2025-07-29T16:50:17.495692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            features = batch[\"features\"].to(device)\n",
    "            sentence_spans = batch[\"sentence_spans\"]\n",
    "            span_labels = batch[\"span_labels\"]\n",
    "\n",
    "            output = model(features, sentence_spans)\n",
    "            probs = torch.sigmoid(output.logits).view(-1).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            labels = torch.cat([torch.tensor(lbl, dtype=torch.int) for lbl in span_labels]).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            features = batch[\"features\"].to(device)\n",
    "            sentence_spans = batch[\"sentence_spans\"]\n",
    "            span_labels = batch[\"span_labels\"]\n",
    "\n",
    "            output = model(features, sentence_spans, span_labels=span_labels)\n",
    "            loss = output.loss\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_loader, eval_loader, optimizer, device, epochs, patience=3):\n",
    "    best_eval_loss = float(\"inf\")\n",
    "    best_train_loss = None\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features = batch[\"features\"].to(device)\n",
    "            sentence_spans = batch[\"sentence_spans\"]\n",
    "            span_labels = batch[\"span_labels\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features, sentence_spans, span_labels=span_labels)\n",
    "            loss = output.loss\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Evaluation\n",
    "        avg_eval_loss = evaluate(model, eval_loader, device)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Eval Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping & Checkpoint\n",
    "        if avg_eval_loss < best_eval_loss:\n",
    "            best_eval_loss = avg_eval_loss\n",
    "            best_train_loss = avg_train_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping after {epoch + 1} epochs.\")\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    print(f\"\\nBest Eval Loss: {best_eval_loss:.4f} | Best Train Loss: {best_train_loss:.4f}\")\n",
    "    return model"
   ],
   "id": "91e1d2c24f9a1755",
   "outputs": [],
   "execution_count": 214
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "Train the model using K-Fold Cross Validation on the training dataset."
   ],
   "id": "b29e02e97c1f008"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T16:50:18.648299Z",
     "start_time": "2025-07-29T16:50:18.599764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#from luminar.sequence_classifier import LuminarSequence\n",
    "from src.luminar.sequence_classifier import LuminarSequence\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    \"feature_dim\": (Config.FEATURE_LEN, Config.NUM_INTERMEDIATE_LIKELIHOODS),\n",
    "    \"feature_type\": \"intermediate_likelihoods\",\n",
    "    \"feature_selection\": \"first\",\n",
    "    \"conv_layer_shapes\": (\n",
    "        ConvolutionalLayerSpec(32, 5),\n",
    "        ConvolutionalLayerSpec(64, 5),\n",
    "        ConvolutionalLayerSpec(32, 3),\n",
    "    ),\n",
    "    \"lstm_hidden_dim\": 128,\n",
    "    \"lstm_layers\": 2,\n",
    "    #\"projection_dim\": 32,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"max_epochs\": 40,\n",
    "    \"seed\": Config.SEED,\n",
    "    \"rescale_features\": False,\n",
    "    \"stack_spans\": 3\n",
    "}"
   ],
   "id": "2828fd3277e84424",
   "outputs": [],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T16:50:18.738163Z",
     "start_time": "2025-07-29T16:50:18.687486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_kfold_training(\n",
    "        dataset, test_loader, model_config, num_folds=5, epochs=10, batch_size=32, patience=3, device=\"cpu\"\n",
    "):\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_metrics = []\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    best_models = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\n========== Fold {fold + 1}/{num_folds} ==========\")\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        eval_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        model = LuminarSequence(**model_config).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=model_config[\"learning_rate\"])\n",
    "\n",
    "        model = train_and_evaluate(model, train_loader, eval_loader, optimizer, device, epochs, patience)\n",
    "        best_models.append((fold, model))\n",
    "\n",
    "        # Evaluate after training on the validation set\n",
    "        acc, f1 = evaluate_metrics(model, eval_loader, device)\n",
    "        print(f\"[Fold {fold + 1}] Final Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
    "        fold_metrics.append({\"accuracy\": acc, \"f1\": f1})\n",
    "\n",
    "    # Summarize results\n",
    "    avg_acc = sum(m[\"accuracy\"] for m in fold_metrics) / num_folds\n",
    "    avg_f1 = sum(m[\"f1\"] for m in fold_metrics) / num_folds\n",
    "    print(f\"\\n========== K-Fold Cross Validation Results ==========\")\n",
    "    for i, m in enumerate(fold_metrics):\n",
    "        print(f\"Fold {i + 1}: Accuracy = {m['accuracy']:.4f}, F1 = {m['f1']:.4f}\")\n",
    "    print(f\"\\nAverage Accuracy: {avg_acc:.4f} | Average F1: {avg_f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"\\n========== Test Evaluation of all K-Fold Cross Model ==========\")\n",
    "    for (fold, model) in best_models:\n",
    "        test_acc, test_f1 = evaluate_metrics(model, test_loader, device)\n",
    "        print(f\"K-Fold {fold}: Test Accuracy = {test_acc:.4f}, Test F1 = {test_f1:.4f}\")\n"
   ],
   "id": "105a3383df0d35de",
   "outputs": [],
   "execution_count": 216
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-29T16:50:18.792468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for train_dataset, test_loader in zip(train_datasets, test_loaders):\n",
    "    print(f\"Training on domain: {train_dataset[0]}\")\n",
    "\n",
    "    # Reset the random seed for reproducibility\n",
    "    torch.manual_seed(Config.SEED)\n",
    "    np.random.seed(Config.SEED)\n",
    "\n",
    "    # Unpack the datasets\n",
    "    train_dataset = train_dataset[1]\n",
    "    test_loader = test_loader[1]\n",
    "\n",
    "    print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "    print(f\"Test Dataset Size: {len(test_loader.dataset)}\")\n",
    "\n",
    "    # Run K-Fold training\n",
    "    run_kfold_training(train_dataset, test_loader, config, num_folds=5, epochs=80, batch_size=512, patience=6, device=device)"
   ],
   "id": "93d3186525396281",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on domain: bundestag\n",
      "Train Dataset Size: 16090\n",
      "Test Dataset Size: 4024\n",
      "\n",
      "========== Fold 1/5 ==========\n",
      "LuminarTrainingConfig(feature_dim=(512, 13), feature_type='intermediate_likelihoods', feature_selection='first', conv_layer_shapes=((32, 5, 1), (64, 5, 1), (32, 3, 1)), lstm_hidden_dim=64, lstm_layers=1, projection_dim=32, early_stopping_patience=3, learning_rate=0.0003, max_epochs=40, gradient_clip_val=1.0, train_batch_size=32, eval_batch_size=1024, warmup_ratio=1.0, seed=42, rescale_features=False, stack_spans=1)\n",
      "\n",
      "Epoch 1/60\n",
      "Train Loss: 0.6754 | Eval Loss: 0.6554\n",
      "\n",
      "Epoch 2/60\n",
      "Train Loss: 0.6296 | Eval Loss: 0.6041\n",
      "\n",
      "Epoch 3/60\n",
      "Train Loss: 0.5817 | Eval Loss: 0.5873\n",
      "\n",
      "Epoch 4/60\n",
      "Train Loss: 0.5486 | Eval Loss: 0.5502\n",
      "\n",
      "Epoch 5/60\n",
      "Train Loss: 0.5381 | Eval Loss: 0.5286\n",
      "\n",
      "Epoch 6/60\n",
      "Train Loss: 0.5266 | Eval Loss: 0.5293\n",
      "\n",
      "Epoch 7/60\n",
      "Train Loss: 0.5197 | Eval Loss: 0.5154\n",
      "\n",
      "Epoch 8/60\n",
      "Train Loss: 0.5048 | Eval Loss: 0.4899\n",
      "\n",
      "Epoch 9/60\n",
      "Train Loss: 0.4898 | Eval Loss: 0.5122\n",
      "\n",
      "Epoch 10/60\n",
      "Train Loss: 0.4715 | Eval Loss: 0.4576\n",
      "\n",
      "Epoch 11/60\n",
      "Train Loss: 0.4575 | Eval Loss: 0.4487\n",
      "\n",
      "Epoch 12/60\n",
      "Train Loss: 0.4505 | Eval Loss: 0.4306\n",
      "\n",
      "Epoch 13/60\n",
      "Train Loss: 0.4327 | Eval Loss: 0.4308\n",
      "\n",
      "Epoch 14/60\n",
      "Train Loss: 0.4268 | Eval Loss: 0.4012\n",
      "\n",
      "Epoch 15/60\n",
      "Train Loss: 0.4119 | Eval Loss: 0.3990\n",
      "\n",
      "Epoch 16/60\n",
      "Train Loss: 0.4080 | Eval Loss: 0.3875\n",
      "\n",
      "Epoch 17/60\n",
      "Train Loss: 0.4011 | Eval Loss: 0.4344\n",
      "\n",
      "Epoch 18/60\n",
      "Train Loss: 0.3954 | Eval Loss: 0.3692\n",
      "\n",
      "Epoch 19/60\n",
      "Train Loss: 0.3974 | Eval Loss: 0.3644\n",
      "\n",
      "Epoch 20/60\n",
      "Train Loss: 0.3882 | Eval Loss: 0.3729\n",
      "\n",
      "Epoch 21/60\n",
      "Train Loss: 0.3967 | Eval Loss: 0.4106\n",
      "\n",
      "Epoch 22/60\n",
      "Train Loss: 0.3776 | Eval Loss: 0.3576\n",
      "\n",
      "Epoch 23/60\n",
      "Train Loss: 0.3591 | Eval Loss: 0.3606\n",
      "\n",
      "Epoch 24/60\n",
      "Train Loss: 0.3569 | Eval Loss: 0.3621\n",
      "\n",
      "Epoch 25/60\n",
      "Train Loss: 0.3522 | Eval Loss: 0.3453\n",
      "\n",
      "Epoch 26/60\n",
      "Train Loss: 0.3558 | Eval Loss: 0.3659\n",
      "\n",
      "Epoch 27/60\n",
      "Train Loss: 0.3495 | Eval Loss: 0.3271\n",
      "\n",
      "Epoch 28/60\n",
      "Train Loss: 0.3521 | Eval Loss: 0.3646\n",
      "\n",
      "Epoch 29/60\n",
      "Train Loss: 0.3327 | Eval Loss: 0.3180\n",
      "\n",
      "Epoch 30/60\n",
      "Train Loss: 0.3312 | Eval Loss: 0.3350\n",
      "\n",
      "Epoch 31/60\n",
      "Train Loss: 0.3432 | Eval Loss: 0.3142\n",
      "\n",
      "Epoch 32/60\n",
      "Train Loss: 0.3350 | Eval Loss: 0.3165\n",
      "\n",
      "Epoch 33/60\n",
      "Train Loss: 0.3283 | Eval Loss: 0.3060\n",
      "\n",
      "Epoch 34/60\n",
      "Train Loss: 0.3261 | Eval Loss: 0.3647\n",
      "\n",
      "Epoch 35/60\n",
      "Train Loss: 0.3112 | Eval Loss: 0.3745\n",
      "\n",
      "Epoch 36/60\n",
      "Train Loss: 0.3272 | Eval Loss: 0.3024\n",
      "\n",
      "Epoch 37/60\n",
      "Train Loss: 0.3124 | Eval Loss: 0.2951\n",
      "\n",
      "Epoch 38/60\n",
      "Train Loss: 0.3130 | Eval Loss: 0.3046\n",
      "\n",
      "Epoch 39/60\n",
      "Train Loss: 0.3035 | Eval Loss: 0.3018\n",
      "\n",
      "Epoch 40/60\n",
      "Train Loss: 0.3063 | Eval Loss: 0.2921\n",
      "\n",
      "Epoch 41/60\n",
      "Train Loss: 0.3123 | Eval Loss: 0.3313\n",
      "\n",
      "Epoch 42/60\n",
      "Train Loss: 0.3028 | Eval Loss: 0.3626\n",
      "\n",
      "Epoch 43/60\n",
      "Train Loss: 0.3067 | Eval Loss: 0.2868\n",
      "\n",
      "Epoch 44/60\n",
      "Train Loss: 0.2913 | Eval Loss: 0.2753\n",
      "\n",
      "Epoch 45/60\n",
      "Train Loss: 0.2929 | Eval Loss: 0.2774\n",
      "\n",
      "Epoch 46/60\n",
      "Train Loss: 0.2855 | Eval Loss: 0.2888\n",
      "\n",
      "Epoch 47/60\n",
      "Train Loss: 0.3049 | Eval Loss: 0.3307\n",
      "\n",
      "Epoch 48/60\n",
      "Train Loss: 0.2942 | Eval Loss: 0.2877\n",
      "\n",
      "Epoch 49/60\n",
      "Train Loss: 0.3093 | Eval Loss: 0.3452\n",
      "Early stopping after 49 epochs.\n",
      "\n",
      "Best Eval Loss: 0.2753 | Best Train Loss: 0.2913\n",
      "[Fold 1] Final Accuracy: 0.8531 | F1: 0.8086\n",
      "\n",
      "========== Fold 2/5 ==========\n",
      "LuminarTrainingConfig(feature_dim=(512, 13), feature_type='intermediate_likelihoods', feature_selection='first', conv_layer_shapes=((32, 5, 1), (64, 5, 1), (32, 3, 1)), lstm_hidden_dim=64, lstm_layers=1, projection_dim=32, early_stopping_patience=3, learning_rate=0.0003, max_epochs=40, gradient_clip_val=1.0, train_batch_size=32, eval_batch_size=1024, warmup_ratio=1.0, seed=42, rescale_features=False, stack_spans=1)\n",
      "\n",
      "Epoch 1/60\n",
      "Train Loss: 0.6820 | Eval Loss: 0.6649\n",
      "\n",
      "Epoch 2/60\n",
      "Train Loss: 0.6348 | Eval Loss: 0.5892\n",
      "\n",
      "Epoch 3/60\n",
      "Train Loss: 0.5723 | Eval Loss: 0.5713\n",
      "\n",
      "Epoch 4/60\n",
      "Train Loss: 0.5503 | Eval Loss: 0.5551\n",
      "\n",
      "Epoch 5/60\n",
      "Train Loss: 0.5310 | Eval Loss: 0.5143\n",
      "\n",
      "Epoch 6/60\n",
      "Train Loss: 0.5088 | Eval Loss: 0.5035\n",
      "\n",
      "Epoch 7/60\n",
      "Train Loss: 0.5071 | Eval Loss: 0.5117\n",
      "\n",
      "Epoch 8/60\n",
      "Train Loss: 0.4960 | Eval Loss: 0.5122\n",
      "\n",
      "Epoch 9/60\n",
      "Train Loss: 0.4760 | Eval Loss: 0.4979\n",
      "\n",
      "Epoch 10/60\n",
      "Train Loss: 0.4713 | Eval Loss: 0.4976\n",
      "\n",
      "Epoch 11/60\n",
      "Train Loss: 0.4605 | Eval Loss: 0.4514\n",
      "\n",
      "Epoch 12/60\n",
      "Train Loss: 0.4568 | Eval Loss: 0.5097\n",
      "\n",
      "Epoch 13/60\n",
      "Train Loss: 0.4518 | Eval Loss: 0.4376\n",
      "\n",
      "Epoch 14/60\n",
      "Train Loss: 0.4315 | Eval Loss: 0.4170\n",
      "\n",
      "Epoch 15/60\n",
      "Train Loss: 0.4283 | Eval Loss: 0.4494\n",
      "\n",
      "Epoch 16/60\n",
      "Train Loss: 0.4190 | Eval Loss: 0.4237\n",
      "\n",
      "Epoch 17/60\n",
      "Train Loss: 0.4140 | Eval Loss: 0.4150\n",
      "\n",
      "Epoch 18/60\n",
      "Train Loss: 0.3971 | Eval Loss: 0.3894\n",
      "\n",
      "Epoch 19/60\n",
      "Train Loss: 0.4039 | Eval Loss: 0.3988\n",
      "\n",
      "Epoch 20/60\n",
      "Train Loss: 0.3985 | Eval Loss: 0.4062\n",
      "\n",
      "Epoch 21/60\n",
      "Train Loss: 0.3855 | Eval Loss: 0.3634\n",
      "\n",
      "Epoch 22/60\n",
      "Train Loss: 0.3702 | Eval Loss: 0.3757\n",
      "\n",
      "Epoch 23/60\n",
      "Train Loss: 0.3743 | Eval Loss: 0.3701\n",
      "\n",
      "Epoch 24/60\n",
      "Train Loss: 0.3716 | Eval Loss: 0.3963\n",
      "\n",
      "Epoch 25/60\n",
      "Train Loss: 0.3722 | Eval Loss: 0.3847\n",
      "\n",
      "Epoch 26/60\n",
      "Train Loss: 0.3536 | Eval Loss: 0.3522\n",
      "\n",
      "Epoch 27/60\n",
      "Train Loss: 0.3615 | Eval Loss: 0.3692\n",
      "\n",
      "Epoch 28/60\n",
      "Train Loss: 0.3508 | Eval Loss: 0.3914\n",
      "\n",
      "Epoch 29/60\n",
      "Train Loss: 0.3411 | Eval Loss: 0.3311\n",
      "\n",
      "Epoch 30/60\n",
      "Train Loss: 0.3477 | Eval Loss: 0.3790\n",
      "\n",
      "Epoch 31/60\n",
      "Train Loss: 0.3512 | Eval Loss: 0.3787\n",
      "\n",
      "Epoch 32/60\n",
      "Train Loss: 0.3569 | Eval Loss: 0.3926\n",
      "\n",
      "Epoch 33/60\n",
      "Train Loss: 0.3313 | Eval Loss: 0.4435\n",
      "\n",
      "Epoch 34/60\n",
      "Train Loss: 0.3574 | Eval Loss: 0.3179\n",
      "\n",
      "Epoch 35/60\n",
      "Train Loss: 0.3245 | Eval Loss: 0.3470\n",
      "\n",
      "Epoch 36/60\n",
      "Train Loss: 0.3192 | Eval Loss: 0.3215\n",
      "\n",
      "Epoch 37/60\n",
      "Train Loss: 0.3138 | Eval Loss: 0.3412\n",
      "\n",
      "Epoch 38/60\n",
      "Train Loss: 0.3030 | Eval Loss: 0.3042\n",
      "\n",
      "Epoch 39/60\n",
      "Train Loss: 0.3263 | Eval Loss: 0.3377\n",
      "\n",
      "Epoch 40/60\n",
      "Train Loss: 0.3120 | Eval Loss: 0.3671\n",
      "\n",
      "Epoch 41/60\n",
      "Train Loss: 0.3031 | Eval Loss: 0.3099\n",
      "\n",
      "Epoch 42/60\n",
      "Train Loss: 0.3083 | Eval Loss: 0.3049\n",
      "\n",
      "Epoch 43/60\n",
      "Train Loss: 0.2992 | Eval Loss: 0.3224\n",
      "Early stopping after 43 epochs.\n",
      "\n",
      "Best Eval Loss: 0.3042 | Best Train Loss: 0.3030\n",
      "[Fold 2] Final Accuracy: 0.8594 | F1: 0.8468\n",
      "\n",
      "========== Fold 3/5 ==========\n",
      "LuminarTrainingConfig(feature_dim=(512, 13), feature_type='intermediate_likelihoods', feature_selection='first', conv_layer_shapes=((32, 5, 1), (64, 5, 1), (32, 3, 1)), lstm_hidden_dim=64, lstm_layers=1, projection_dim=32, early_stopping_patience=3, learning_rate=0.0003, max_epochs=40, gradient_clip_val=1.0, train_batch_size=32, eval_batch_size=1024, warmup_ratio=1.0, seed=42, rescale_features=False, stack_spans=1)\n",
      "\n",
      "Epoch 1/60\n",
      "Train Loss: 0.6713 | Eval Loss: 0.6506\n",
      "\n",
      "Epoch 2/60\n",
      "Train Loss: 0.6072 | Eval Loss: 0.5803\n",
      "\n",
      "Epoch 3/60\n",
      "Train Loss: 0.5631 | Eval Loss: 0.5507\n",
      "\n",
      "Epoch 4/60\n",
      "Train Loss: 0.5452 | Eval Loss: 0.5324\n",
      "\n",
      "Epoch 5/60\n",
      "Train Loss: 0.5353 | Eval Loss: 0.5329\n",
      "\n",
      "Epoch 6/60\n",
      "Train Loss: 0.5179 | Eval Loss: 0.5033\n",
      "\n",
      "Epoch 7/60\n",
      "Train Loss: 0.5197 | Eval Loss: 0.4841\n",
      "\n",
      "Epoch 8/60\n",
      "Train Loss: 0.4748 | Eval Loss: 0.5327\n",
      "\n",
      "Epoch 9/60\n",
      "Train Loss: 0.4669 | Eval Loss: 0.4773\n",
      "\n",
      "Epoch 10/60\n",
      "Train Loss: 0.4563 | Eval Loss: 0.4913\n",
      "\n",
      "Epoch 11/60\n",
      "Train Loss: 0.4582 | Eval Loss: 0.4372\n",
      "\n",
      "Epoch 12/60\n",
      "Train Loss: 0.4323 | Eval Loss: 0.4440\n",
      "\n",
      "Epoch 13/60\n",
      "Train Loss: 0.4267 | Eval Loss: 0.4731\n",
      "\n",
      "Epoch 14/60\n",
      "Train Loss: 0.4207 | Eval Loss: 0.4056\n",
      "\n",
      "Epoch 15/60\n",
      "Train Loss: 0.4030 | Eval Loss: 0.3893\n",
      "\n",
      "Epoch 16/60\n",
      "Train Loss: 0.3980 | Eval Loss: 0.3882\n",
      "\n",
      "Epoch 17/60\n",
      "Train Loss: 0.4019 | Eval Loss: 0.5076\n",
      "\n",
      "Epoch 18/60\n",
      "Train Loss: 0.4012 | Eval Loss: 0.4104\n",
      "\n",
      "Epoch 19/60\n",
      "Train Loss: 0.3872 | Eval Loss: 0.3682\n",
      "\n",
      "Epoch 20/60\n",
      "Train Loss: 0.3840 | Eval Loss: 0.3745\n",
      "\n",
      "Epoch 21/60\n",
      "Train Loss: 0.3689 | Eval Loss: 0.3698\n",
      "\n",
      "Epoch 22/60\n",
      "Train Loss: 0.3793 | Eval Loss: 0.3557\n",
      "\n",
      "Epoch 23/60\n",
      "Train Loss: 0.3563 | Eval Loss: 0.4500\n",
      "\n",
      "Epoch 24/60\n",
      "Train Loss: 0.3577 | Eval Loss: 0.3461\n",
      "\n",
      "Epoch 25/60\n",
      "Train Loss: 0.3483 | Eval Loss: 0.3787\n",
      "\n",
      "Epoch 26/60\n",
      "Train Loss: 0.3521 | Eval Loss: 0.4045\n",
      "\n",
      "Epoch 27/60\n",
      "Train Loss: 0.3513 | Eval Loss: 0.3473\n",
      "\n",
      "Epoch 28/60\n",
      "Train Loss: 0.3454 | Eval Loss: 0.3576\n",
      "\n",
      "Epoch 29/60\n",
      "Train Loss: 0.3288 | Eval Loss: 0.3289\n",
      "\n",
      "Epoch 30/60\n",
      "Train Loss: 0.3256 | Eval Loss: 0.3183\n",
      "\n",
      "Epoch 31/60\n",
      "Train Loss: 0.3229 | Eval Loss: 0.3165\n",
      "\n",
      "Epoch 32/60\n",
      "Train Loss: 0.3153 | Eval Loss: 0.4604\n",
      "\n",
      "Epoch 33/60\n",
      "Train Loss: 0.3140 | Eval Loss: 0.4186\n",
      "\n",
      "Epoch 34/60\n",
      "Train Loss: 0.3232 | Eval Loss: 0.3763\n",
      "\n",
      "Epoch 35/60\n",
      "Train Loss: 0.3012 | Eval Loss: 0.3327\n",
      "\n",
      "Epoch 36/60\n",
      "Train Loss: 0.3059 | Eval Loss: 0.3821\n",
      "Early stopping after 36 epochs.\n",
      "\n",
      "Best Eval Loss: 0.3165 | Best Train Loss: 0.3229\n",
      "[Fold 3] Final Accuracy: 0.8396 | F1: 0.7735\n",
      "\n",
      "========== Fold 4/5 ==========\n",
      "LuminarTrainingConfig(feature_dim=(512, 13), feature_type='intermediate_likelihoods', feature_selection='first', conv_layer_shapes=((32, 5, 1), (64, 5, 1), (32, 3, 1)), lstm_hidden_dim=64, lstm_layers=1, projection_dim=32, early_stopping_patience=3, learning_rate=0.0003, max_epochs=40, gradient_clip_val=1.0, train_batch_size=32, eval_batch_size=1024, warmup_ratio=1.0, seed=42, rescale_features=False, stack_spans=1)\n",
      "\n",
      "Epoch 1/60\n",
      "Train Loss: 0.6773 | Eval Loss: 0.6543\n",
      "\n",
      "Epoch 2/60\n",
      "Train Loss: 0.6350 | Eval Loss: 0.6302\n",
      "\n",
      "Epoch 3/60\n",
      "Train Loss: 0.6182 | Eval Loss: 0.6277\n",
      "\n",
      "Epoch 4/60\n",
      "Train Loss: 0.5658 | Eval Loss: 0.5593\n",
      "\n",
      "Epoch 5/60\n",
      "Train Loss: 0.5384 | Eval Loss: 0.5558\n",
      "\n",
      "Epoch 6/60\n",
      "Train Loss: 0.5232 | Eval Loss: 0.5212\n",
      "\n",
      "Epoch 7/60\n",
      "Train Loss: 0.5165 | Eval Loss: 0.5211\n",
      "\n",
      "Epoch 8/60\n",
      "Train Loss: 0.5095 | Eval Loss: 0.5176\n",
      "\n",
      "Epoch 9/60\n",
      "Train Loss: 0.5000 | Eval Loss: 0.4858\n",
      "\n",
      "Epoch 10/60\n",
      "Train Loss: 0.5019 | Eval Loss: 0.4978\n",
      "\n",
      "Epoch 11/60\n",
      "Train Loss: 0.4801 | Eval Loss: 0.4898\n",
      "\n",
      "Epoch 12/60\n",
      "Train Loss: 0.4601 | Eval Loss: 0.5147\n",
      "\n",
      "Epoch 13/60\n",
      "Train Loss: 0.4470 | Eval Loss: 0.4764\n",
      "\n",
      "Epoch 14/60\n",
      "Train Loss: 0.4341 | Eval Loss: 0.4225\n",
      "\n",
      "Epoch 15/60\n",
      "Train Loss: 0.4324 | Eval Loss: 0.4407\n",
      "\n",
      "Epoch 16/60\n",
      "Train Loss: 0.4271 | Eval Loss: 0.4785\n",
      "\n",
      "Epoch 17/60\n",
      "Train Loss: 0.4040 | Eval Loss: 0.4236\n",
      "\n",
      "Epoch 18/60\n",
      "Train Loss: 0.3927 | Eval Loss: 0.4156\n",
      "\n",
      "Epoch 19/60\n",
      "Train Loss: 0.3955 | Eval Loss: 0.4458\n",
      "\n",
      "Epoch 20/60\n",
      "Train Loss: 0.3861 | Eval Loss: 0.4335\n",
      "\n",
      "Epoch 21/60\n",
      "Train Loss: 0.3869 | Eval Loss: 0.3874\n",
      "\n",
      "Epoch 22/60\n",
      "Train Loss: 0.3751 | Eval Loss: 0.3902\n",
      "\n",
      "Epoch 23/60\n",
      "Train Loss: 0.3789 | Eval Loss: 0.3620\n",
      "\n",
      "Epoch 24/60\n",
      "Train Loss: 0.3623 | Eval Loss: 0.3544\n",
      "\n",
      "Epoch 25/60\n",
      "Train Loss: 0.3818 | Eval Loss: 0.4109\n",
      "\n",
      "Epoch 26/60\n",
      "Train Loss: 0.3695 | Eval Loss: 0.4295\n",
      "\n",
      "Epoch 27/60\n",
      "Train Loss: 0.3629 | Eval Loss: 0.3477\n",
      "\n",
      "Epoch 28/60\n",
      "Train Loss: 0.3544 | Eval Loss: 0.3610\n",
      "\n",
      "Epoch 29/60\n",
      "Train Loss: 0.3477 | Eval Loss: 0.3393\n",
      "\n",
      "Epoch 30/60\n",
      "Train Loss: 0.3430 | Eval Loss: 0.3379\n",
      "\n",
      "Epoch 31/60\n",
      "Train Loss: 0.3350 | Eval Loss: 0.3263\n",
      "\n",
      "Epoch 32/60\n",
      "Train Loss: 0.3269 | Eval Loss: 0.3618\n",
      "\n",
      "Epoch 33/60\n",
      "Train Loss: 0.3313 | Eval Loss: 0.3250\n",
      "\n",
      "Epoch 34/60\n",
      "Train Loss: 0.3275 | Eval Loss: 0.3195\n",
      "\n",
      "Epoch 35/60\n",
      "Train Loss: 0.3210 | Eval Loss: 0.3130\n",
      "\n",
      "Epoch 36/60\n",
      "Train Loss: 0.3188 | Eval Loss: 0.3173\n",
      "\n",
      "Epoch 37/60\n",
      "Train Loss: 0.3128 | Eval Loss: 0.3616\n",
      "\n",
      "Epoch 38/60\n",
      "Train Loss: 0.3156 | Eval Loss: 0.3226\n",
      "\n",
      "Epoch 39/60\n",
      "Train Loss: 0.3036 | Eval Loss: 0.3083\n",
      "\n",
      "Epoch 40/60\n",
      "Train Loss: 0.3176 | Eval Loss: 0.3769\n",
      "\n",
      "Epoch 41/60\n",
      "Train Loss: 0.3037 | Eval Loss: 0.3297\n",
      "\n",
      "Epoch 42/60\n",
      "Train Loss: 0.3068 | Eval Loss: 0.2976\n",
      "\n",
      "Epoch 43/60\n",
      "Train Loss: 0.3063 | Eval Loss: 0.3038\n",
      "\n",
      "Epoch 44/60\n",
      "Train Loss: 0.3124 | Eval Loss: 0.3103\n",
      "\n",
      "Epoch 45/60\n",
      "Train Loss: 0.3024 | Eval Loss: 0.2935\n",
      "\n",
      "Epoch 46/60\n",
      "Train Loss: 0.2907 | Eval Loss: 0.3593\n",
      "\n",
      "Epoch 47/60\n",
      "Train Loss: 0.2881 | Eval Loss: 0.2958\n",
      "\n",
      "Epoch 48/60\n",
      "Train Loss: 0.2963 | Eval Loss: 0.2965\n",
      "\n",
      "Epoch 49/60\n",
      "Train Loss: 0.2921 | Eval Loss: 0.2859\n",
      "\n",
      "Epoch 50/60\n",
      "Train Loss: 0.2929 | Eval Loss: 0.2777\n",
      "\n",
      "Epoch 51/60\n",
      "Train Loss: 0.2994 | Eval Loss: 0.2845\n",
      "\n",
      "Epoch 52/60\n",
      "Train Loss: 0.2861 | Eval Loss: 0.2809\n",
      "\n",
      "Epoch 53/60\n",
      "Train Loss: 0.2835 | Eval Loss: 0.3881\n",
      "\n",
      "Epoch 54/60\n",
      "Train Loss: 0.2755 | Eval Loss: 0.2856\n",
      "\n",
      "Epoch 55/60\n",
      "Train Loss: 0.2758 | Eval Loss: 0.2852\n",
      "Early stopping after 55 epochs.\n",
      "\n",
      "Best Eval Loss: 0.2777 | Best Train Loss: 0.2929\n",
      "[Fold 4] Final Accuracy: 0.8788 | F1: 0.8470\n",
      "\n",
      "========== Fold 5/5 ==========\n",
      "LuminarTrainingConfig(feature_dim=(512, 13), feature_type='intermediate_likelihoods', feature_selection='first', conv_layer_shapes=((32, 5, 1), (64, 5, 1), (32, 3, 1)), lstm_hidden_dim=64, lstm_layers=1, projection_dim=32, early_stopping_patience=3, learning_rate=0.0003, max_epochs=40, gradient_clip_val=1.0, train_batch_size=32, eval_batch_size=1024, warmup_ratio=1.0, seed=42, rescale_features=False, stack_spans=1)\n",
      "\n",
      "Epoch 1/60\n",
      "Train Loss: 0.6776 | Eval Loss: 0.6686\n",
      "\n",
      "Epoch 2/60\n",
      "Train Loss: 0.6320 | Eval Loss: 0.5992\n",
      "\n",
      "Epoch 3/60\n",
      "Train Loss: 0.5863 | Eval Loss: 0.5702\n",
      "\n",
      "Epoch 4/60\n",
      "Train Loss: 0.5650 | Eval Loss: 0.5670\n",
      "\n",
      "Epoch 5/60\n",
      "Train Loss: 0.5470 | Eval Loss: 0.5442\n",
      "\n",
      "Epoch 6/60\n",
      "Train Loss: 0.5351 | Eval Loss: 0.5384\n",
      "\n",
      "Epoch 7/60\n",
      "Train Loss: 0.5265 | Eval Loss: 0.5379\n",
      "\n",
      "Epoch 8/60\n",
      "Train Loss: 0.5252 | Eval Loss: 0.5571\n",
      "\n",
      "Epoch 9/60\n",
      "Train Loss: 0.5139 | Eval Loss: 0.5039\n",
      "\n",
      "Epoch 10/60\n",
      "Train Loss: 0.5058 | Eval Loss: 0.4951\n",
      "\n",
      "Epoch 11/60\n",
      "Train Loss: 0.4874 | Eval Loss: 0.5034\n",
      "\n",
      "Epoch 12/60\n",
      "Train Loss: 0.4817 | Eval Loss: 0.4735\n",
      "\n",
      "Epoch 13/60\n",
      "Train Loss: 0.4647 | Eval Loss: 0.4676\n",
      "\n",
      "Epoch 14/60\n",
      "Train Loss: 0.4500 | Eval Loss: 0.4914\n",
      "\n",
      "Epoch 15/60\n",
      "Train Loss: 0.4424 | Eval Loss: 0.4298\n",
      "\n",
      "Epoch 16/60\n",
      "Train Loss: 0.4361 | Eval Loss: 0.4784\n",
      "\n",
      "Epoch 17/60\n",
      "Train Loss: 0.4308 | Eval Loss: 0.4334\n",
      "\n",
      "Epoch 18/60\n",
      "Train Loss: 0.4246 | Eval Loss: 0.4838\n",
      "\n",
      "Epoch 19/60\n",
      "Train Loss: 0.4116 | Eval Loss: 0.3937\n",
      "\n",
      "Epoch 20/60\n",
      "Train Loss: 0.3948 | Eval Loss: 0.4210\n",
      "\n",
      "Epoch 21/60\n",
      "Train Loss: 0.3953 | Eval Loss: 0.4108\n",
      "\n",
      "Epoch 22/60\n",
      "Train Loss: 0.3918 | Eval Loss: 0.3818\n",
      "\n",
      "Epoch 23/60\n",
      "Train Loss: 0.3820 | Eval Loss: 0.3674\n",
      "\n",
      "Epoch 24/60\n",
      "Train Loss: 0.3795 | Eval Loss: 0.3954\n",
      "\n",
      "Epoch 25/60\n",
      "Train Loss: 0.3724 | Eval Loss: 0.4319\n",
      "\n",
      "Epoch 26/60\n",
      "Train Loss: 0.3656 | Eval Loss: 0.4464\n",
      "\n",
      "Epoch 27/60\n",
      "Train Loss: 0.3601 | Eval Loss: 0.3750\n",
      "\n",
      "Epoch 28/60\n",
      "Train Loss: 0.3501 | Eval Loss: 0.3450\n",
      "\n",
      "Epoch 29/60\n",
      "Train Loss: 0.3474 | Eval Loss: 0.3482\n",
      "\n",
      "Epoch 30/60\n",
      "Train Loss: 0.3499 | Eval Loss: 0.3517\n",
      "\n",
      "Epoch 31/60\n",
      "Train Loss: 0.3434 | Eval Loss: 0.3305\n",
      "\n",
      "Epoch 32/60\n",
      "Train Loss: 0.3469 | Eval Loss: 0.3292\n",
      "\n",
      "Epoch 33/60\n",
      "Train Loss: 0.3517 | Eval Loss: 0.3335\n",
      "\n",
      "Epoch 34/60\n",
      "Train Loss: 0.3337 | Eval Loss: 0.3320\n",
      "\n",
      "Epoch 35/60\n",
      "Train Loss: 0.3335 | Eval Loss: 0.4157\n",
      "\n",
      "Epoch 36/60\n",
      "Train Loss: 0.3249 | Eval Loss: 0.3155\n",
      "\n",
      "Epoch 37/60\n",
      "Train Loss: 0.3348 | Eval Loss: 0.3588\n",
      "\n",
      "Epoch 38/60\n",
      "Train Loss: 0.3301 | Eval Loss: 0.3521\n",
      "\n",
      "Epoch 39/60\n",
      "Train Loss: 0.3293 | Eval Loss: 0.3157\n",
      "\n",
      "Epoch 40/60\n",
      "Train Loss: 0.3159 | Eval Loss: 0.3042\n",
      "\n",
      "Epoch 41/60\n",
      "Train Loss: 0.3185 | Eval Loss: 0.3060\n",
      "\n",
      "Epoch 42/60\n",
      "Train Loss: 0.3247 | Eval Loss: 0.3338\n",
      "\n",
      "Epoch 43/60\n",
      "Train Loss: 0.3080 | Eval Loss: 0.4230\n",
      "\n",
      "Epoch 44/60\n",
      "Train Loss: 0.3149 | Eval Loss: 0.3029\n",
      "\n",
      "Epoch 45/60\n",
      "Train Loss: 0.3064 | Eval Loss: 0.2958\n",
      "\n",
      "Epoch 46/60\n",
      "Train Loss: 0.3035 | Eval Loss: 0.3067\n",
      "\n",
      "Epoch 47/60\n",
      "Train Loss: 0.3145 | Eval Loss: 0.3655\n",
      "\n",
      "Epoch 48/60\n",
      "Train Loss: 0.2962 | Eval Loss: 0.2873\n",
      "\n",
      "Epoch 49/60\n",
      "Train Loss: 0.2887 | Eval Loss: 0.2907\n",
      "\n",
      "Epoch 50/60\n",
      "Train Loss: 0.2946 | Eval Loss: 0.2867\n",
      "\n",
      "Epoch 51/60\n",
      "Train Loss: 0.3119 | Eval Loss: 0.3014\n",
      "\n",
      "Epoch 52/60\n",
      "Train Loss: 0.2976 | Eval Loss: 0.2823\n",
      "\n",
      "Epoch 53/60\n",
      "Train Loss: 0.2865 | Eval Loss: 0.3937\n",
      "\n",
      "Epoch 54/60\n",
      "Train Loss: 0.2855 | Eval Loss: 0.3180\n",
      "\n",
      "Epoch 55/60\n",
      "Train Loss: 0.2841 | Eval Loss: 0.3145\n",
      "\n",
      "Epoch 56/60\n",
      "Train Loss: 0.2866 | Eval Loss: 0.3123\n",
      "\n",
      "Epoch 57/60\n",
      "Train Loss: 0.2820 | Eval Loss: 0.2820\n",
      "\n",
      "Epoch 58/60\n",
      "Train Loss: 0.2737 | Eval Loss: 0.2727\n",
      "\n",
      "Epoch 59/60\n",
      "Train Loss: 0.2850 | Eval Loss: 0.3048\n",
      "\n",
      "Epoch 60/60\n",
      "Train Loss: 0.2640 | Eval Loss: 0.2761\n",
      "\n",
      "Best Eval Loss: 0.2727 | Best Train Loss: 0.2737\n",
      "[Fold 5] Final Accuracy: 0.8819 | F1: 0.8598\n",
      "\n",
      "========== K-Fold Cross Validation Results ==========\n",
      "Fold 1: Accuracy = 0.8531, F1 = 0.8086\n",
      "Fold 2: Accuracy = 0.8594, F1 = 0.8468\n",
      "Fold 3: Accuracy = 0.8396, F1 = 0.7735\n",
      "Fold 4: Accuracy = 0.8788, F1 = 0.8470\n",
      "Fold 5: Accuracy = 0.8819, F1 = 0.8598\n",
      "\n",
      "Average Accuracy: 0.8626 | Average F1: 0.8271\n",
      "\n",
      "\n",
      "========== Test Evaluation of all K-Fold Cross Model ==========\n",
      "K-Fold 0: Test Accuracy = 0.8550, Test F1 = 0.8051\n",
      "K-Fold 1: Test Accuracy = 0.8579, Test F1 = 0.8458\n",
      "K-Fold 2: Test Accuracy = 0.8358, Test F1 = 0.7739\n",
      "K-Fold 3: Test Accuracy = 0.8802, Test F1 = 0.8498\n",
      "K-Fold 4: Test Accuracy = 0.8864, Test F1 = 0.8616\n",
      "Training on domain: student_essays\n",
      "Train Dataset Size: 57982\n",
      "Test Dataset Size: 14496\n",
      "\n",
      "========== Fold 1/5 ==========\n",
      "LuminarTrainingConfig(feature_dim=(512, 13), feature_type='intermediate_likelihoods', feature_selection='first', conv_layer_shapes=((32, 5, 1), (64, 5, 1), (32, 3, 1)), lstm_hidden_dim=64, lstm_layers=1, projection_dim=32, early_stopping_patience=3, learning_rate=0.0003, max_epochs=40, gradient_clip_val=1.0, train_batch_size=32, eval_batch_size=1024, warmup_ratio=1.0, seed=42, rescale_features=False, stack_spans=1)\n",
      "\n",
      "Epoch 1/60\n",
      "Train Loss: 0.5180 | Eval Loss: 0.4445\n",
      "\n",
      "Epoch 2/60\n",
      "Train Loss: 0.4202 | Eval Loss: 0.4510\n",
      "\n",
      "Epoch 3/60\n",
      "Train Loss: 0.3904 | Eval Loss: 0.3530\n",
      "\n",
      "Epoch 4/60\n",
      "Train Loss: 0.3743 | Eval Loss: 0.3726\n",
      "\n",
      "Epoch 5/60\n",
      "Train Loss: 0.3616 | Eval Loss: 0.4647\n",
      "\n",
      "Epoch 6/60\n",
      "Train Loss: 0.3353 | Eval Loss: 0.3398\n",
      "\n",
      "Epoch 7/60\n",
      "Train Loss: 0.3270 | Eval Loss: 0.2971\n",
      "\n",
      "Epoch 8/60\n",
      "Train Loss: 0.3201 | Eval Loss: 0.3235\n",
      "\n",
      "Epoch 9/60\n",
      "Train Loss: 0.3098 | Eval Loss: 0.2920\n",
      "\n",
      "Epoch 10/60\n",
      "Train Loss: 0.3025 | Eval Loss: 0.2725\n",
      "\n",
      "Epoch 11/60\n",
      "Train Loss: 0.2912 | Eval Loss: 0.2704\n",
      "\n",
      "Epoch 12/60\n",
      "Train Loss: 0.2844 | Eval Loss: 0.3200\n",
      "\n",
      "Epoch 13/60\n",
      "Train Loss: 0.2842 | Eval Loss: 0.2612\n",
      "\n",
      "Epoch 14/60\n",
      "Train Loss: 0.2731 | Eval Loss: 0.2831\n",
      "\n",
      "Epoch 15/60\n",
      "Train Loss: 0.2800 | Eval Loss: 0.4793\n",
      "\n",
      "Epoch 16/60\n",
      "Train Loss: 0.2688 | Eval Loss: 0.2622\n",
      "\n",
      "Epoch 17/60\n",
      "Train Loss: 0.2633 | Eval Loss: 0.3072\n",
      "\n",
      "Epoch 18/60\n",
      "Train Loss: 0.2542 | Eval Loss: 0.2624\n",
      "Early stopping after 18 epochs.\n",
      "\n",
      "Best Eval Loss: 0.2612 | Best Train Loss: 0.2842\n",
      "[Fold 1] Final Accuracy: 0.8851 | F1: 0.9007\n",
      "\n",
      "========== Fold 2/5 ==========\n",
      "LuminarTrainingConfig(feature_dim=(512, 13), feature_type='intermediate_likelihoods', feature_selection='first', conv_layer_shapes=((32, 5, 1), (64, 5, 1), (32, 3, 1)), lstm_hidden_dim=64, lstm_layers=1, projection_dim=32, early_stopping_patience=3, learning_rate=0.0003, max_epochs=40, gradient_clip_val=1.0, train_batch_size=32, eval_batch_size=1024, warmup_ratio=1.0, seed=42, rescale_features=False, stack_spans=1)\n",
      "\n",
      "Epoch 1/60\n",
      "Train Loss: 0.5097 | Eval Loss: 0.4175\n",
      "\n",
      "Epoch 2/60\n",
      "Train Loss: 0.4107 | Eval Loss: 0.4774\n",
      "\n",
      "Epoch 3/60\n",
      "Train Loss: 0.3632 | Eval Loss: 0.3511\n",
      "\n",
      "Epoch 4/60\n",
      "Train Loss: 0.3421 | Eval Loss: 0.3168\n",
      "\n",
      "Epoch 5/60\n",
      "Train Loss: 0.3346 | Eval Loss: 0.3741\n",
      "\n",
      "Epoch 6/60\n",
      "Train Loss: 0.3259 | Eval Loss: 0.3927\n",
      "\n",
      "Epoch 7/60\n",
      "Train Loss: 0.3099 | Eval Loss: 0.3118\n",
      "\n",
      "Epoch 8/60\n",
      "Train Loss: 0.3067 | Eval Loss: 0.2893\n",
      "\n",
      "Epoch 9/60\n",
      "Train Loss: 0.3028 | Eval Loss: 0.3280\n",
      "\n",
      "Epoch 10/60\n",
      "Train Loss: 0.2971 | Eval Loss: 0.2710\n",
      "\n",
      "Epoch 11/60\n",
      "Train Loss: 0.2871 | Eval Loss: 0.3046\n",
      "\n",
      "Epoch 12/60\n",
      "Train Loss: 0.2805 | Eval Loss: 0.3018\n",
      "\n",
      "Epoch 13/60\n",
      "Train Loss: 0.2773 | Eval Loss: 0.2593\n",
      "\n",
      "Epoch 14/60\n",
      "Train Loss: 0.2697 | Eval Loss: 0.2523\n",
      "\n",
      "Epoch 15/60\n",
      "Train Loss: 0.2658 | Eval Loss: 0.3031\n",
      "\n",
      "Epoch 16/60\n",
      "Train Loss: 0.2664 | Eval Loss: 0.2532\n",
      "\n",
      "Epoch 17/60\n",
      "Train Loss: 0.2594 | Eval Loss: 0.2552\n",
      "\n",
      "Epoch 18/60\n",
      "Train Loss: 0.2637 | Eval Loss: 0.2457\n",
      "\n",
      "Epoch 19/60\n",
      "Train Loss: 0.2532 | Eval Loss: 0.2411\n",
      "\n",
      "Epoch 20/60\n",
      "Train Loss: 0.2451 | Eval Loss: 0.2547\n",
      "\n",
      "Epoch 21/60\n",
      "Train Loss: 0.2449 | Eval Loss: 0.2407\n",
      "\n",
      "Epoch 22/60\n",
      "Train Loss: 0.2401 | Eval Loss: 0.2266\n",
      "\n",
      "Epoch 23/60\n",
      "Train Loss: 0.2382 | Eval Loss: 0.2265\n",
      "\n",
      "Epoch 24/60\n",
      "Train Loss: 0.2398 | Eval Loss: 0.2365\n",
      "\n",
      "Epoch 25/60\n",
      "Train Loss: 0.2368 | Eval Loss: 0.2319\n",
      "\n",
      "Epoch 26/60\n",
      "Train Loss: 0.2302 | Eval Loss: 0.2265\n",
      "\n",
      "Epoch 27/60\n",
      "Train Loss: 0.2316 | Eval Loss: 0.2413\n",
      "\n",
      "Epoch 28/60\n",
      "Train Loss: 0.2259 | Eval Loss: 0.2302\n",
      "Early stopping after 28 epochs.\n",
      "\n",
      "Best Eval Loss: 0.2265 | Best Train Loss: 0.2382\n",
      "[Fold 2] Final Accuracy: 0.8988 | F1: 0.9020\n",
      "\n",
      "========== Fold 3/5 ==========\n",
      "LuminarTrainingConfig(feature_dim=(512, 13), feature_type='intermediate_likelihoods', feature_selection='first', conv_layer_shapes=((32, 5, 1), (64, 5, 1), (32, 3, 1)), lstm_hidden_dim=64, lstm_layers=1, projection_dim=32, early_stopping_patience=3, learning_rate=0.0003, max_epochs=40, gradient_clip_val=1.0, train_batch_size=32, eval_batch_size=1024, warmup_ratio=1.0, seed=42, rescale_features=False, stack_spans=1)\n",
      "\n",
      "Epoch 1/60\n",
      "Train Loss: 0.5143 | Eval Loss: 0.4227\n",
      "\n",
      "Epoch 2/60\n",
      "Train Loss: 0.4247 | Eval Loss: 0.3840\n",
      "\n",
      "Epoch 3/60\n",
      "Train Loss: 0.3903 | Eval Loss: 0.5132\n",
      "\n",
      "Epoch 4/60\n",
      "Train Loss: 0.3689 | Eval Loss: 0.3433\n",
      "\n",
      "Epoch 5/60\n",
      "Train Loss: 0.3491 | Eval Loss: 0.3752\n",
      "\n",
      "Epoch 6/60\n",
      "Train Loss: 0.3375 | Eval Loss: 0.3157\n",
      "\n",
      "Epoch 7/60\n",
      "Train Loss: 0.3318 | Eval Loss: 0.3019\n",
      "\n",
      "Epoch 8/60\n",
      "Train Loss: 0.3237 | Eval Loss: 0.3265\n",
      "\n",
      "Epoch 9/60\n",
      "Train Loss: 0.3119 | Eval Loss: 0.2903\n",
      "\n",
      "Epoch 10/60\n",
      "Train Loss: 0.3069 | Eval Loss: 0.2853\n",
      "\n",
      "Epoch 11/60\n",
      "Train Loss: 0.3002 | Eval Loss: 0.2896\n",
      "\n",
      "Epoch 12/60\n",
      "Train Loss: 0.2939 | Eval Loss: 0.2887\n",
      "\n",
      "Epoch 13/60\n",
      "Train Loss: 0.2911 | Eval Loss: 0.2882\n",
      "\n",
      "Epoch 14/60\n",
      "Train Loss: 0.2901 | Eval Loss: 0.2813\n",
      "\n",
      "Epoch 15/60\n",
      "Train Loss: 0.2769 | Eval Loss: 0.2606\n",
      "\n",
      "Epoch 16/60\n",
      "Train Loss: 0.2829 | Eval Loss: 0.2581\n",
      "\n",
      "Epoch 17/60\n",
      "Train Loss: 0.2758 | Eval Loss: 0.2611\n",
      "\n",
      "Epoch 18/60\n",
      "Train Loss: 0.2696 | Eval Loss: 0.3490\n",
      "\n",
      "Epoch 19/60\n",
      "Train Loss: 0.2658 | Eval Loss: 0.2525\n",
      "\n",
      "Epoch 20/60\n",
      "Train Loss: 0.2638 | Eval Loss: 0.2685\n",
      "\n",
      "Epoch 21/60\n",
      "Train Loss: 0.2576 | Eval Loss: 0.2506\n",
      "\n",
      "Epoch 22/60\n",
      "Train Loss: 0.2536 | Eval Loss: 0.2424\n",
      "\n",
      "Epoch 23/60\n",
      "Train Loss: 0.2495 | Eval Loss: 0.2361\n",
      "\n",
      "Epoch 24/60\n",
      "Train Loss: 0.2492 | Eval Loss: 0.2516\n",
      "\n",
      "Epoch 25/60\n",
      "Train Loss: 0.2441 | Eval Loss: 0.2389\n",
      "\n",
      "Epoch 26/60\n",
      "Train Loss: 0.2397 | Eval Loss: 0.2302\n",
      "\n",
      "Epoch 27/60\n",
      "Train Loss: 0.2387 | Eval Loss: 0.2276\n",
      "\n",
      "Epoch 28/60\n",
      "Train Loss: 0.2395 | Eval Loss: 0.2234\n",
      "\n",
      "Epoch 29/60\n",
      "Train Loss: 0.2318 | Eval Loss: 0.2281\n",
      "\n",
      "Epoch 30/60\n",
      "Train Loss: 0.2311 | Eval Loss: 0.2205\n",
      "\n",
      "Epoch 31/60\n",
      "Train Loss: 0.2296 | Eval Loss: 0.2157\n",
      "\n",
      "Epoch 32/60\n",
      "Train Loss: 0.2264 | Eval Loss: 0.2348\n",
      "\n",
      "Epoch 33/60\n",
      "Train Loss: 0.2230 | Eval Loss: 0.2111\n",
      "\n",
      "Epoch 34/60\n",
      "Train Loss: 0.2221 | Eval Loss: 0.2108\n",
      "\n",
      "Epoch 35/60\n",
      "Train Loss: 0.2180 | Eval Loss: 0.2381\n",
      "\n",
      "Epoch 36/60\n",
      "Train Loss: 0.2166 | Eval Loss: 0.2266\n",
      "\n",
      "Epoch 37/60\n",
      "Train Loss: 0.2180 | Eval Loss: 0.2014\n",
      "\n",
      "Epoch 38/60\n",
      "Train Loss: 0.2117 | Eval Loss: 0.2080\n",
      "\n",
      "Epoch 39/60\n",
      "Train Loss: 0.2085 | Eval Loss: 0.2129\n",
      "\n",
      "Epoch 40/60\n",
      "Train Loss: 0.2123 | Eval Loss: 0.1987\n",
      "\n",
      "Epoch 41/60\n",
      "Train Loss: 0.2042 | Eval Loss: 0.2226\n",
      "\n",
      "Epoch 42/60\n",
      "Train Loss: 0.2024 | Eval Loss: 0.3014\n",
      "\n",
      "Epoch 43/60\n",
      "Train Loss: 0.2023 | Eval Loss: 0.1979\n",
      "\n",
      "Epoch 44/60\n",
      "Train Loss: 0.1976 | Eval Loss: 0.1996\n",
      "\n",
      "Epoch 45/60\n",
      "Train Loss: 0.1990 | Eval Loss: 0.1935\n",
      "\n",
      "Epoch 46/60\n",
      "Train Loss: 0.1938 | Eval Loss: 0.1969\n",
      "\n",
      "Epoch 47/60\n",
      "Train Loss: 0.1983 | Eval Loss: 0.1866\n",
      "\n",
      "Epoch 48/60\n",
      "Train Loss: 0.1927 | Eval Loss: 0.2072\n",
      "\n",
      "Epoch 49/60\n",
      "Train Loss: 0.1909 | Eval Loss: 0.2099\n",
      "\n",
      "Epoch 50/60\n",
      "Train Loss: 0.1875 | Eval Loss: 0.1863\n",
      "\n",
      "Epoch 51/60\n",
      "Train Loss: 0.1863 | Eval Loss: 0.1829\n",
      "\n",
      "Epoch 52/60\n",
      "Train Loss: 0.1851 | Eval Loss: 0.1833\n",
      "\n",
      "Epoch 53/60\n",
      "Train Loss: 0.1818 | Eval Loss: 0.1828\n",
      "\n",
      "Epoch 54/60\n",
      "Train Loss: 0.1799 | Eval Loss: 0.1773\n",
      "\n",
      "Epoch 55/60\n",
      "Train Loss: 0.1831 | Eval Loss: 0.2131\n",
      "\n",
      "Epoch 56/60\n",
      "Train Loss: 0.1772 | Eval Loss: 0.1745\n",
      "\n",
      "Epoch 57/60\n",
      "Train Loss: 0.1735 | Eval Loss: 0.1759\n",
      "\n",
      "Epoch 58/60\n",
      "Train Loss: 0.1771 | Eval Loss: 0.1731\n",
      "\n",
      "Epoch 59/60\n",
      "Train Loss: 0.1770 | Eval Loss: 0.2209\n",
      "\n",
      "Epoch 60/60\n",
      "Train Loss: 0.1743 | Eval Loss: 0.1739\n",
      "\n",
      "Best Eval Loss: 0.1731 | Best Train Loss: 0.1771\n",
      "[Fold 3] Final Accuracy: 0.9271 | F1: 0.9318\n",
      "\n",
      "========== Fold 4/5 ==========\n",
      "LuminarTrainingConfig(feature_dim=(512, 13), feature_type='intermediate_likelihoods', feature_selection='first', conv_layer_shapes=((32, 5, 1), (64, 5, 1), (32, 3, 1)), lstm_hidden_dim=64, lstm_layers=1, projection_dim=32, early_stopping_patience=3, learning_rate=0.0003, max_epochs=40, gradient_clip_val=1.0, train_batch_size=32, eval_batch_size=1024, warmup_ratio=1.0, seed=42, rescale_features=False, stack_spans=1)\n",
      "\n",
      "Epoch 1/60\n",
      "Train Loss: 0.5180 | Eval Loss: 0.4576\n",
      "\n",
      "Epoch 2/60\n",
      "Train Loss: 0.4233 | Eval Loss: 0.4098\n",
      "\n",
      "Epoch 3/60\n",
      "Train Loss: 0.3852 | Eval Loss: 0.3613\n",
      "\n",
      "Epoch 4/60\n",
      "Train Loss: 0.3624 | Eval Loss: 0.3528\n",
      "\n",
      "Epoch 5/60\n",
      "Train Loss: 0.3477 | Eval Loss: 0.3315\n",
      "\n",
      "Epoch 6/60\n",
      "Train Loss: 0.3427 | Eval Loss: 0.3131\n",
      "\n",
      "Epoch 7/60\n",
      "Train Loss: 0.3336 | Eval Loss: 0.3097\n",
      "\n",
      "Epoch 8/60\n",
      "Train Loss: 0.3264 | Eval Loss: 0.3553\n",
      "\n",
      "Epoch 9/60\n",
      "Train Loss: 0.3130 | Eval Loss: 0.3537\n",
      "\n",
      "Epoch 10/60\n",
      "Train Loss: 0.3110 | Eval Loss: 0.2842\n",
      "\n",
      "Epoch 11/60\n",
      "Train Loss: 0.3070 | Eval Loss: 0.2947\n",
      "\n",
      "Epoch 12/60\n",
      "Train Loss: 0.2986 | Eval Loss: 0.2919\n",
      "\n",
      "Epoch 13/60\n",
      "Train Loss: 0.2954 | Eval Loss: 0.2759\n",
      "\n",
      "Epoch 14/60\n",
      "Train Loss: 0.2947 | Eval Loss: 0.3087\n",
      "\n",
      "Epoch 15/60\n",
      "Train Loss: 0.2855 | Eval Loss: 0.2755\n",
      "\n",
      "Epoch 16/60\n",
      "Train Loss: 0.2822 | Eval Loss: 0.2601\n",
      "\n",
      "Epoch 17/60\n",
      "Train Loss: 0.2752 | Eval Loss: 0.2629\n",
      "\n",
      "Epoch 18/60\n",
      "Train Loss: 0.2689 | Eval Loss: 0.2765\n",
      "\n",
      "Epoch 19/60\n",
      "Train Loss: 0.2662 | Eval Loss: 0.2485\n",
      "\n",
      "Epoch 20/60\n",
      "Train Loss: 0.2612 | Eval Loss: 0.2625\n",
      "\n",
      "Epoch 21/60\n",
      "Train Loss: 0.2583 | Eval Loss: 0.2456\n",
      "\n",
      "Epoch 22/60\n",
      "Train Loss: 0.2531 | Eval Loss: 0.2367\n",
      "\n",
      "Epoch 23/60\n",
      "Train Loss: 0.2533 | Eval Loss: 0.2491\n",
      "\n",
      "Epoch 24/60\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T16:23:13.814764Z",
     "start_time": "2025-07-29T16:23:13.812364Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5e7a49225612a113",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

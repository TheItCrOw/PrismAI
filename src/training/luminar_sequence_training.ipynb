{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cac2df18338774",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Training of LuminarSequenceClassifier on the PrismAI dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "66a5cfd8c7d2051e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:12:09.006967Z",
     "start_time": "2025-08-05T09:12:07.115102Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "5a16e47050282fde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:22:42.715242Z",
     "start_time": "2025-08-01T11:22:35.091102Z"
    }
   },
   "source": [
    "import sys\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from typing import Final, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path().resolve().parent))\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from IPython.display import display, Markdown\n",
    "from datasets import load_dataset\n",
    "from numpy._typing import NDArray\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from datasets import DatasetDict\n",
    "from luminar.utils.sequential_data import SequentialDataService\n",
    "from luminar.utils.data import get_pad_to_fixed_length_fn, get_matched_datasets\n",
    "from luminar.utils.cuda import get_best_device\n",
    "from luminar.utils.training import ConvolutionalLayerSpec, LuminarSequenceTrainingConfig\n",
    "from luminar.encoder import LuminarEncoder\n",
    "\n",
    "import numpy as np\n",
    "import glob"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a9172b7cf1e56309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:32:29.231205Z",
     "start_time": "2025-08-01T11:32:29.145371Z"
    }
   },
   "source": [
    "class Config:\n",
    "    HF_TOKEN: Final[str] = (Path.home() / \".hf_token\").read_text().strip()\n",
    "    #DATASET_PATH: Final[str] = \"liberi-luminaris/PrismAI-encoded-gpt2\"\n",
    "    DATASET_ROOT_PATH: Final[str] = \"/storage/projects/stoeckel/prismai/encoded/fulltext/\"\n",
    "    #DATASET_ROOT_PATH: Final[str] = \"/mnt/c/home/projects/prismAI/data/encoded/fulltext/\"\n",
    "    NUM_INTERMEDIATE_LIKELIHOODS: Final[int] = 13 # gpt2=13, falcon=33\n",
    "    FEATURE_LEN = 512\n",
    "    BATCH_SIZE = 64\n",
    "    SEED = 42\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "7a319dd3c792f5ce",
   "metadata": {},
   "source": [
    "## Loading & Preprocessing the datasets\n",
    "\n",
    "Load the datasets for the training."
   ]
  },
  {
   "cell_type": "code",
   "id": "60326fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:22:44.902439Z",
     "start_time": "2025-08-01T11:22:43.037104Z"
    }
   },
   "source": [
    "domains = ['student_essays']#, 'student_essays']\n",
    "agents = ['gpt_4o_mini_gemma2_9b']\n",
    "feature_agents = ['gpt2_512'] #, 'falcon_7b_512']\n",
    "\n",
    "luminar_encoder = LuminarEncoder(max_len=Config.FEATURE_LEN)\n",
    "data_service = SequentialDataService(luminar_encoder, Config.BATCH_SIZE, Config.FEATURE_LEN)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/staff_homes/kboenisc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "d6f5d8ba7e620bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:25:08.975806Z",
     "start_time": "2025-08-01T11:22:45.014440Z"
    }
   },
   "source": [
    "datasets = {}\n",
    "\n",
    "for domain in domains:\n",
    "    for agent in agents:\n",
    "        for feature_agent in feature_agents:\n",
    "            dataset_path = Path(Config.DATASET_ROOT_PATH) / agent / feature_agent / domain\n",
    "\n",
    "            dataset_dict = data_service.load_dataset(dataset_path)\n",
    "\n",
    "            datasets.setdefault(domain, {}).setdefault(agent, {})[feature_agent] = dataset_dict\n",
    "            print(f\"Loaded dataset for domain '{domain}' with agent '{agent}' and feature agent '{feature_agent}'\")\n",
    "\n",
    "datasets"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /storage/projects/stoeckel/prismai/encoded/fulltext/gpt_4o_mini_gemma2_9b/gpt2_512/student_essays\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/50734 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "252099a10e154a7b943c64ef5de2eeb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/14496 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91e74177bb524417b4df184ef1b98b40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/7248 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2186a3c3c14249408d2db76315474cda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/50734 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95e7cf61f145447e99f7dfd9f9b1f511"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/14496 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38464529993f4614b2403d931e83d16c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/7248 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0818ce906dd465d8df22cac7688376c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset for domain 'student_essays' with agent 'gpt_4o_mini_gemma2_9b' and feature agent 'gpt2_512'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'student_essays': {'gpt_4o_mini_gemma2_9b': {'gpt2_512': DatasetDict({\n",
       "       train: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 50734\n",
       "       })\n",
       "       test: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 14496\n",
       "       })\n",
       "       eval: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 7248\n",
       "       })\n",
       "   })}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "98f45ae307339095",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:25:10.810576Z",
     "start_time": "2025-08-01T11:25:09.109221Z"
    }
   },
   "source": [
    "# Sanity check\n",
    "idx = 0\n",
    "for domain, agents_dict in datasets.items():\n",
    "    for agent, feature_agents_dict in agents_dict.items():\n",
    "        for feature_agent, dataset in feature_agents_dict.items():\n",
    "            md = f\"\"\"\n",
    "**Domain:** `{domain}`\n",
    "**Agent:** `{agent}`\n",
    "**Feature Agent:** `{feature_agent}`\n",
    "\n",
    "**Train:** {len(dataset['train'])}\n",
    "**Test:** {len(dataset['test'])}\n",
    "**Eval:** {len(dataset['eval'])}\n",
    "\n",
    "**Example-Features:**\n",
    "`{dataset['train'][idx]['features'][:2]}...`\n",
    "\n",
    "**Feature-Shape:**\n",
    "`{np.asarray(dataset['train'][idx]['features']).shape}`\n",
    "\n",
    "**Example text:**\n",
    "`{dataset['train'][idx]['text']}`\n",
    "\n",
    "**Example-Tokenized Text:**\n",
    "`{dataset['train'][idx]['tokenized_text'][:10]}...`\n",
    "\n",
    "**Tokenized Text Shape:**\n",
    "`{np.asarray(dataset['train'][idx]['tokenized_text']).shape}`\n",
    "\n",
    "**Sentence-Token-Spans:**\n",
    "`{dataset['train'][idx]['sentence_token_spans']}`\n",
    "\n",
    "**Example Sentence-Token Span decoded:**\n",
    "`{luminar_encoder.tokenizer.decode(np.asarray(dataset['train'][idx]['tokenized_text'])[dataset['train'][idx]['sentence_token_spans'][0][0]:dataset['train'][idx]['sentence_token_spans'][0][1]].flatten().tolist())}`\n",
    "\n",
    "**Example span labels:**\n",
    "`{dataset['train'][idx]['span_labels']}`\n",
    "\n",
    "**Example label:**\n",
    "`{dataset['train'][idx]['labels']}`\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "            display(Markdown(md))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n**Domain:** `student_essays`\n**Agent:** `gpt_4o_mini_gemma2_9b`\n**Feature Agent:** `gpt2_512`\n\n**Train:** 50734\n**Test:** 14496\n**Eval:** 7248\n\n**Example-Features:**\n`[[4.4132913899375126e-05, 4.203895392974451e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00015184479707386345], [9.98157247522613e-06, 2.268475629563227e-09, 2.4706817147723825e-10, 2.598772308459729e-10, 4.608947598572222e-11, 2.177385494128714e-10, 7.711160811448015e-13, 6.366168985201537e-13, 2.281021511211531e-17, 9.533206545892253e-25, 7.987888483894126e-31, 0.0, 0.029878340661525726]]...`\n\n**Feature-Shape:**\n`(512, 13)`\n\n**Example text:**\n`[Your Name] [Your Address] [City, State, Zip Code] [Email Address] [Date] The Honorable [Senator's Name] [Senator's Office Address] [City, State, Zip Code] Dear Senator [Senator's Last Name], I hope this letter finds you well. I’m writing to you today as a concerned citizen who’s really worried about the way our presidential elections work. Honestly, I can’t wrap my head around the Electoral College anymore. It feels like an outdated system that just doesn’t represent us, the people. I mean, how is it fair that a candidate can win the presidency without winning the popular vote? It’s like saying my vote doesn’t count just because I live in a state that leans one way or another. That doesn’t seem right, does it? We should all have an equal say in who leads our country, and the popular vote is the only way to truly reflect the will of the people. The Electoral College creates a situation where some votes are more valuable than others, and that’s just plain wrong. It leads to candidates ignoring vast swathes of the country because they know they won’t win those states. Isn’t it time we changed that? We need a system that encourages candidates to engage with all of us, not just the folks in swing states. I urge you to consider advocating for a popular vote system. It’s time for our elections to truly represent the voice of the people. Thank you for your time, and I hope you’ll take my concerns to heart.`\n\n**Example-Tokenized Text:**\n`[[58], [7120], [6530], [60], [685], [7120], [17917], [60], [685], [14941]]...`\n\n**Tokenized Text Shape:**\n`(512, 1)`\n\n**Sentence-Token-Spans:**\n`[[0, 61], [61, 87], [87, 103], [103, 121], [121, 141], [141, 168], [168, 179], [179, 209], [209, 233], [233, 256], [256, 266], [266, 288], [288, 300], [300, 317], [317, 336]]`\n\n**Example Sentence-Token Span decoded:**\n`[Your Name] [Your Address] [City, State, Zip Code] [Email Address] [Date] The Honorable [Senator's Name] [Senator's Office Address] [City, State, Zip Code] Dear Senator [Senator's Last Name], I hope this letter finds you well.`\n\n**Example span labels:**\n`[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]`\n\n**Example label:**\n`1`\n\n---\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "51b81189b4cd0958",
   "metadata": {},
   "source": [
    "# Data Loaders\n",
    "\n",
    "Create the data loaders for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "id": "ebcb886877bdde06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:29:11.472414Z",
     "start_time": "2025-08-01T11:25:10.886071Z"
    }
   },
   "source": [
    "train_datasets = []\n",
    "test_loaders = []\n",
    "\n",
    "all_train_parts = []\n",
    "all_test_parts = []\n",
    "\n",
    "for domain in domains:\n",
    "    for agent in feature_agents:\n",
    "        print(f\"Creating datasets for domain: {domain}\")\n",
    "\n",
    "        train_dataset, test_dataset, test_loader = data_service.dataset_to_luminar_sequence_dataset(\n",
    "            datasets[domain][\"gpt_4o_mini_gemma2_9b\"][agent]\n",
    "        )\n",
    "\n",
    "        print(f\"Train Dataset: {len(train_dataset)}\")\n",
    "        train_datasets.append((domain, agent, train_dataset))\n",
    "        all_train_parts.append(train_dataset)\n",
    "\n",
    "        print(f\"Test Dataset: {len(test_dataset)}\")\n",
    "        test_loaders.append((domain, agent, test_loader))\n",
    "        all_test_parts.append(test_dataset)\n",
    "\n",
    "# Now concat all\n",
    "all_domains_train_datasets = ConcatDataset(all_train_parts)\n",
    "all_domains_test_dataset = ConcatDataset(all_test_parts)\n",
    "all_domains_test_loader = DataLoader(\n",
    "    all_domains_test_dataset, batch_size=data_service.batch_size, shuffle=False, collate_fn=data_service._collate_fn\n",
    ")\n",
    "\n",
    "print(f\"All Train: {len(all_domains_train_datasets)}\")\n",
    "train_datasets.append((\"all\", \"all\", all_domains_train_datasets))\n",
    "\n",
    "print(f\"All Test: {len(all_domains_test_dataset)}\")\n",
    "test_loaders.append((\"all\", \"all\", all_domains_test_loader))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets for domain: student_essays\n",
      "Train Dataset: 57982\n",
      "Test Dataset: 14496\n",
      "All Train: 57982\n",
      "All Test: 14496\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "b29e02e97c1f008",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Train the model using K-Fold Cross Validation on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "2828fd3277e84424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:49:06.780801Z",
     "start_time": "2025-08-01T11:49:05.254335Z"
    }
   },
   "source": [
    "import gc\n",
    "from luminar.sequence_trainer import LuminarSequenceTrainer\n",
    "\n",
    "config = LuminarSequenceTrainingConfig(**{\n",
    "    \"feature_len\": Config.FEATURE_LEN,\n",
    "    \"num_intermediate_likelihoods\": Config.NUM_INTERMEDIATE_LIKELIHOODS,\n",
    "    \"conv_layer_shapes\": (\n",
    "        ConvolutionalLayerSpec(64, 5),\n",
    "        ConvolutionalLayerSpec(128, 5),\n",
    "        ConvolutionalLayerSpec(64, 3),\n",
    "    ),\n",
    "    \"projection_dim\": 64,\n",
    "    \"lstm_hidden_dim\": 64,\n",
    "    \"lstm_layers\": 1,\n",
    "    \"apply_delta_augmentation\": False,\n",
    "    \"apply_product_augmentation\": False,\n",
    "    \"max_epochs\": 120,\n",
    "    \"kfold\": 3,\n",
    "    \"early_stopping_patience\": 17,\n",
    "    \"learning_rate\": 2e-3,\n",
    "    \"batch_size\": Config.BATCH_SIZE,\n",
    "    \"seed\": Config.SEED,\n",
    "    \"rescale_features\": False,\n",
    "    \"stack_spans\": 2,\n",
    "    \"dataset_root_path\": Config.DATASET_ROOT_PATH\n",
    "})\n",
    "print(config)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LuminarSequenceTrainingConfig(feature_len=512, num_intermediate_likelihoods=13, apply_delta_augmentation=False, apply_product_augmentation=False, conv_layer_shapes=((64, 5, 1), (128, 5, 1), (64, 3, 1)), projection_dim=64, lstm_hidden_dim=64, lstm_layers=1, stack_spans=2, dataset_root_path='/storage/projects/stoeckel/prismai/encoded/fulltext/', models_root_path='/storage/projects/boenisch/PrismAI/models/luminar_sequence/', domain='student_essays', agent='gpt_4o_mini_gemma2_9b', feature_agent='gpt2_512', max_epochs=120, batch_size=64, early_stopping_patience=17, rescale_features=False, kfold=3, learning_rate=0.002, seed=42)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "08f7a50a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:49:07.566708Z",
     "start_time": "2025-08-01T11:49:06.928111Z"
    }
   },
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    with torch.cuda.device(torch.cuda.current_device()):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "device = get_best_device()\n",
    "print(device)\n",
    "\n",
    "log_to_wandb = False"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "93d3186525396281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T11:49:00.151832Z",
     "start_time": "2025-08-01T11:43:49.523971Z"
    }
   },
   "source": [
    "for train_dataset, test_loader in zip(train_datasets, test_loaders):\n",
    "    config.domain = train_dataset[0]\n",
    "    print(f\"Training on domain: {config.domain}\")\n",
    "    config.feature_agent = train_dataset[1]\n",
    "\n",
    "    # Reset the random seed for reproducibility\n",
    "    torch.manual_seed(Config.SEED)\n",
    "    np.random.seed(Config.SEED)\n",
    "\n",
    "    # Unpack the datasets\n",
    "    train_dataset = train_dataset[2]\n",
    "    test_loader = test_loader[2]\n",
    "\n",
    "    print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "    print(f\"Test Dataset Size: {len(test_loader.dataset)}\")\n",
    "\n",
    "    # Init wandb\n",
    "    if log_to_wandb:\n",
    "        run = wandb.init(\n",
    "            project=\"Luminar\",\n",
    "            config=config.__dict__,\n",
    "            reinit=True,\n",
    "            group=f\"notebook_runs\",\n",
    "            name=f\"notebook_{config.domain}_{config.feature_agent}_{config.feature_agent}\"\n",
    "        )\n",
    "\n",
    "    trainer = LuminarSequenceTrainer(train_dataset=train_dataset,\n",
    "                                     test_data_loader = test_loader,\n",
    "                                     collate_fn=data_service._collate_fn,\n",
    "                                     log_to_wandb=log_to_wandb,\n",
    "                                     config=config,\n",
    "                                     device=device,\n",
    "                                     use_experimental_attention=True)\n",
    "    \n",
    "    metrics, best_model = trainer.train()\n",
    "    avg_f1 = metrics.get(\"f1_score\", 0.0)\n",
    "    if log_to_wandb:\n",
    "        wandb.log({\"objective_f1\": avg_f1})\n",
    "\n",
    "    # Save model locally\n",
    "    model_store_path = Path(config.models_root_path) / config.domain / wandb.run.id\n",
    "    best_model.save(model_store_path)\n",
    "\n",
    "    # Upload to wandb as an artifact\n",
    "    if log_to_wandb:\n",
    "        artifact = wandb.Artifact(name=f\"luminar-sequence-{wandb.run.id}\", type=\"model\")\n",
    "        artifact.add_dir(str(model_store_path))\n",
    "        wandb.log_artifact(artifact)\n",
    "        wandb.finish()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on domain: student_essays\n",
      "Train Dataset Size: 57982\n",
      "Test Dataset Size: 14496\n",
      "\n",
      "========== Fold 1/3 ==========\n",
      "\n",
      "Epoch 1/120\n",
      "Train Loss: 0.6896 | Eval Loss: 0.6897\n",
      "\n",
      "Epoch 2/120\n",
      "Train Loss: 0.6895 | Eval Loss: 0.6899\n",
      "\n",
      "Epoch 3/120\n",
      "Train Loss: 0.6895 | Eval Loss: 0.6897\n",
      "\n",
      "Epoch 4/120\n",
      "Train Loss: 0.6894 | Eval Loss: 0.6897\n",
      "\n",
      "Epoch 5/120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[35]\u001B[39m\u001B[32m, line 35\u001B[39m\n\u001B[32m     19\u001B[39m     run = wandb.init(\n\u001B[32m     20\u001B[39m         project=\u001B[33m\"\u001B[39m\u001B[33mLuminar\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     21\u001B[39m         config=config.\u001B[34m__dict__\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     24\u001B[39m         name=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mnotebook_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.domain\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.feature_agent\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.feature_agent\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     25\u001B[39m     )\n\u001B[32m     27\u001B[39m trainer = LuminarSequenceTrainer(train_dataset=train_dataset,\n\u001B[32m     28\u001B[39m                                  test_data_loader = test_loader,\n\u001B[32m     29\u001B[39m                                  collate_fn=data_service._collate_fn,\n\u001B[32m   (...)\u001B[39m\u001B[32m     32\u001B[39m                                  device=device,\n\u001B[32m     33\u001B[39m                                  use_experimental_attention=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m metrics, best_model = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     36\u001B[39m avg_f1 = metrics.get(\u001B[33m\"\u001B[39m\u001B[33mf1_score\u001B[39m\u001B[33m\"\u001B[39m, \u001B[32m0.0\u001B[39m)\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m log_to_wandb:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/home/prismAI/PrismAI/src/luminar/sequence_trainer.py:66\u001B[39m, in \u001B[36mLuminarSequenceTrainer.train\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     62\u001B[39m     model = LuminarSequence(\u001B[38;5;28mself\u001B[39m.config).to(\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m     64\u001B[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001B[38;5;28mself\u001B[39m.config.learning_rate)\n\u001B[32m---> \u001B[39m\u001B[32m66\u001B[39m model = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_train_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmax_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mearly_stopping_patience\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[38;5;66;03m# Evaluate and collect fold metrics\u001B[39;00m\n\u001B[32m     71\u001B[39m metrics = \u001B[38;5;28mself\u001B[39m._evaluate_test(model)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/home/prismAI/PrismAI/src/luminar/sequence_trainer.py:142\u001B[39m, in \u001B[36mLuminarSequenceTrainer._train_and_evaluate\u001B[39m\u001B[34m(self, model, train_loader, eval_loader, optimizer, epochs, fold, patience)\u001B[39m\n\u001B[32m    139\u001B[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), \u001B[32m1.0\u001B[39m)\n\u001B[32m    140\u001B[39m     optimizer.step()\n\u001B[32m--> \u001B[39m\u001B[32m142\u001B[39m     total_train_loss += \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    143\u001B[39m avg_train_loss = total_train_loss / \u001B[38;5;28mlen\u001B[39m(train_loader)\n\u001B[32m    145\u001B[39m \u001B[38;5;66;03m# Evaluation\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ad46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# About\n",
    "\n",
    "Training of LuminarSequenceClassifier on the PrismAI dataset."
   ],
   "id": "f1cac2df18338774"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:28:26.762715Z",
     "start_time": "2025-07-29T08:28:25.576944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "66a5cfd8c7d2051e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T09:32:35.747918Z",
     "start_time": "2025-07-29T09:32:34.707343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "from typing import Final, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import display, Markdown\n",
    "from datasets import load_dataset\n",
    "from numpy._typing import NDArray\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import DatasetDict\n",
    "from src.luminar.utils.data import get_pad_to_fixed_length_fn, get_matched_datasets\n",
    "from src.luminar.utils.training import ConvolutionalLayerSpec\n",
    "from src.luminar.encoder import LuminarEncoder\n",
    "\n",
    "import numpy as np\n",
    "import glob"
   ],
   "id": "5a16e47050282fde",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:28:27.010928Z",
     "start_time": "2025-07-29T08:28:26.959082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    HF_TOKEN: Final[str] = (Path.home() / \".hf_token\").read_text().strip()\n",
    "    #DATASET_PATH: Final[str] = \"liberi-luminaris/PrismAI-encoded-gpt2\"\n",
    "    DATASET_ROOT_PATH: Final[str] = \"/storage/projects/stoeckel/prismai/encoded/fulltext/\"\n",
    "    #DATASET_ROOT_PATH: Final[str] = \"/mnt/c/home/projects/prismAI/data/encoded/fulltext/\"\n",
    "    NUM_INTERMEDIATE_LIKELIHOODS: Final[int] = 13\n",
    "    FEATURE_LEN = 512\n",
    "    SEED = 42\n"
   ],
   "id": "a9172b7cf1e56309",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading & Preprocessing the datasets\n",
    "\n",
    "Load the datasets for the training."
   ],
   "id": "7a319dd3c792f5ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:28:27.116580Z",
     "start_time": "2025-07-29T08:28:27.065977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We need that to sentence tokenize the text\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "id": "c8c1f57c27d96d42",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/staff_homes/kboenisc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:28:27.205208Z",
     "start_time": "2025-07-29T08:28:27.157601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sentence_to_token_spans(text: str) -> list[tuple[int, int]]:\n",
    "    \"\"\"Return a list of (start_token_idx, end_token_idx) for each sentence in the text.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    spans = []\n",
    "    current_token_idx = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        tokens = luminar_encoder.tokenize(sent)[\"input_ids\"]\n",
    "        token_count = len(tokens)\n",
    "\n",
    "        start = current_token_idx\n",
    "        end = min(current_token_idx + token_count, Config.FEATURE_LEN)\n",
    "\n",
    "        if start >= Config.FEATURE_LEN:\n",
    "            break  # Stop if padding region or overflow\n",
    "        spans.append((start, end))\n",
    "        current_token_idx = end\n",
    "\n",
    "    return spans\n"
   ],
   "id": "5bfc20f0f811508",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:28:28.266079Z",
     "start_time": "2025-07-29T08:28:27.247695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "domains = ['student_essays']\n",
    "agents = ['gpt_4o_mini_gemma2_9b']\n",
    "feature_agents = ['gpt2_512']\n",
    "\n",
    "luminar_encoder = LuminarEncoder(max_len=Config.FEATURE_LEN)"
   ],
   "id": "7ae3dbbad3613837",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:31:00.473642Z",
     "start_time": "2025-07-29T08:28:28.380408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = {}\n",
    "\n",
    "pad_to_fixed_length: Callable[[NDArray], NDArray] = get_pad_to_fixed_length_fn(Config.FEATURE_LEN)\n",
    "\n",
    "for domain in domains:\n",
    "    for agent in agents:\n",
    "        for feature_agent in feature_agents:\n",
    "            dataset_path = Path(Config.DATASET_ROOT_PATH) / agent / feature_agent / domain\n",
    "\n",
    "            if not dataset_path.exists():\n",
    "                raise FileNotFoundError(f\"Dataset path {dataset_path} does not exist.\")\n",
    "\n",
    "            data_files = {\n",
    "                \"train\": sorted(str(f) for f in dataset_path.glob(\"train/*.arrow\")),\n",
    "                \"test\": sorted(str(f) for f in dataset_path.glob(\"test/*.arrow\")),\n",
    "                \"eval\": sorted(str(f) for f in dataset_path.glob(\"eval/*.arrow\")),\n",
    "            }\n",
    "            data_files = {k: v for k, v in data_files.items() if v}\n",
    "\n",
    "            # These datasets are already matched, so we can load them directly\n",
    "            # We need the tokenized text since we label sequences based on sentences.\n",
    "            dataset_dict = (\n",
    "                load_dataset(\n",
    "                    \"arrow\",\n",
    "                    data_files=data_files,\n",
    "                )\n",
    "                .map(\n",
    "                    lambda batch: {\n",
    "                        \"tokenized_text\": [\n",
    "                            pad_to_fixed_length(\n",
    "                                np.array(luminar_encoder.tokenize(t)[\"input_ids\"]).reshape(-1, 1)\n",
    "                            )\n",
    "                            for t in batch[\"text\"]\n",
    "                        ],\n",
    "                        \"sentence_token_spans\": [\n",
    "                            sentence_to_token_spans(t)\n",
    "                            for t in batch[\"text\"]\n",
    "                        ]\n",
    "                    },\n",
    "                    batched=True,\n",
    "                    desc=\"Tokenizing, padding, and aligning sentences\"\n",
    "                )\n",
    "                .map(\n",
    "                    lambda batch: {\n",
    "                        \"span_labels\": [\n",
    "                            [label] * len(spans)\n",
    "                            for label, spans in zip(batch[\"labels\"], batch[\"sentence_token_spans\"])\n",
    "                        ]\n",
    "                    },\n",
    "                    batched=True,\n",
    "                    desc=\"Assigning labels to sentence spans\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            datasets.setdefault(domain, {}).setdefault(agent, {})[feature_agent] = dataset_dict\n",
    "            print(f\"Loaded dataset for domain '{domain}' with agent '{agent}' and feature agent '{feature_agent}'\")\n",
    "\n",
    "datasets"
   ],
   "id": "d6f5d8ba7e620bec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/50734 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6186c2b30a094ce998a5c3e7bcc76174"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/14496 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55de1fc691514323b6a44e3f478c3625"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing, padding, and aligning sentences:   0%|          | 0/7248 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd7a35e96b2d4833a55dfeb8b201ec4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/50734 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eeabe461185644119bd36edc2788d333"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/14496 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0a4d05c996b488f9a1585d500022e58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Assigning labels to sentence spans:   0%|          | 0/7248 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ed29c764aed40668c4be47f130fcf99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset for domain 'student_essays' with agent 'gpt_4o_mini_gemma2_9b' and feature agent 'gpt2_512'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'student_essays': {'gpt_4o_mini_gemma2_9b': {'gpt2_512': DatasetDict({\n",
       "       train: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 50734\n",
       "       })\n",
       "       test: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 14496\n",
       "       })\n",
       "       eval: Dataset({\n",
       "           features: ['agent', 'id_sample', 'id_source', 'labels', 'text', 'features', 'tokenized_text', 'sentence_token_spans', 'span_labels'],\n",
       "           num_rows: 7248\n",
       "       })\n",
       "   })}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T08:31:01.975578Z",
     "start_time": "2025-07-29T08:31:00.694132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sanity check\n",
    "idx = 0\n",
    "for domain, agents_dict in datasets.items():\n",
    "    for agent, feature_agents_dict in agents_dict.items():\n",
    "        for feature_agent, dataset in feature_agents_dict.items():\n",
    "            md = f\"\"\"\n",
    "**Domain:** `{domain}`\n",
    "**Agent:** `{agent}`\n",
    "**Feature Agent:** `{feature_agent}`\n",
    "\n",
    "**Train:** {len(dataset['train'])}\n",
    "**Test:** {len(dataset['test'])}\n",
    "**Eval:** {len(dataset['eval'])}\n",
    "\n",
    "**Example-Features:**\n",
    "`{dataset['train'][idx]['features'][:2]}...`\n",
    "\n",
    "**Feature-Shape:**\n",
    "`{np.asarray(dataset['train'][idx]['features']).shape}`\n",
    "\n",
    "**Example text:**\n",
    "`{dataset['train'][idx]['text']}`\n",
    "\n",
    "**Example-Tokenized Text:**\n",
    "`{dataset['train'][idx]['tokenized_text'][:10]}...`\n",
    "\n",
    "**Tokenized Text Shape:**\n",
    "`{np.asarray(dataset['train'][idx]['tokenized_text']).shape}`\n",
    "\n",
    "**Sentence-Token-Spans:**\n",
    "`{dataset['train'][idx]['sentence_token_spans']}`\n",
    "\n",
    "**Example Sentence-Token Span decoded:**\n",
    "`{luminar_encoder.tokenizer.decode(np.asarray(dataset['train'][idx]['tokenized_text'])[dataset['train'][idx]['sentence_token_spans'][0][0]:dataset['train'][idx]['sentence_token_spans'][0][1]].flatten().tolist())}`\n",
    "\n",
    "**Example span labels:**\n",
    "`{dataset['train'][idx]['span_labels']}`\n",
    "\n",
    "**Example label:**\n",
    "`{dataset['train'][idx]['labels']}`\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "            display(Markdown(md))"
   ],
   "id": "98f45ae307339095",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n**Domain:** `student_essays`\n**Agent:** `gpt_4o_mini_gemma2_9b`\n**Feature Agent:** `gpt2_512`\n\n**Train:** 50734\n**Test:** 14496\n**Eval:** 7248\n\n**Example-Features:**\n`[[4.4132913899375126e-05, 4.203895392974451e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00015184479707386345], [9.98157247522613e-06, 2.268475629563227e-09, 2.4706817147723825e-10, 2.598772308459729e-10, 4.608947598572222e-11, 2.177385494128714e-10, 7.711160811448015e-13, 6.366168985201537e-13, 2.281021511211531e-17, 9.533206545892253e-25, 7.987888483894126e-31, 0.0, 0.029878340661525726]]...`\n\n**Feature-Shape:**\n`(512, 13)`\n\n**Example text:**\n`[Your Name] [Your Address] [City, State, Zip Code] [Email Address] [Date] The Honorable [Senator's Name] [Senator's Office Address] [City, State, Zip Code] Dear Senator [Senator's Last Name], I hope this letter finds you well. I’m writing to you today as a concerned citizen who’s really worried about the way our presidential elections work. Honestly, I can’t wrap my head around the Electoral College anymore. It feels like an outdated system that just doesn’t represent us, the people. I mean, how is it fair that a candidate can win the presidency without winning the popular vote? It’s like saying my vote doesn’t count just because I live in a state that leans one way or another. That doesn’t seem right, does it? We should all have an equal say in who leads our country, and the popular vote is the only way to truly reflect the will of the people. The Electoral College creates a situation where some votes are more valuable than others, and that’s just plain wrong. It leads to candidates ignoring vast swathes of the country because they know they won’t win those states. Isn’t it time we changed that? We need a system that encourages candidates to engage with all of us, not just the folks in swing states. I urge you to consider advocating for a popular vote system. It’s time for our elections to truly represent the voice of the people. Thank you for your time, and I hope you’ll take my concerns to heart.`\n\n**Example-Tokenized Text:**\n`[[58], [7120], [6530], [60], [685], [7120], [17917], [60], [685], [14941]]...`\n\n**Tokenized Text Shape:**\n`(512, 1)`\n\n**Sentence-Token-Spans:**\n`[[0, 61], [61, 87], [87, 103], [103, 121], [121, 141], [141, 168], [168, 179], [179, 209], [209, 233], [233, 256], [256, 266], [266, 288], [288, 300], [300, 317], [317, 336]]`\n\n**Example Sentence-Token Span decoded:**\n`[Your Name] [Your Address] [City, State, Zip Code] [Email Address] [Date] The Honorable [Senator's Name] [Senator's Office Address] [City, State, Zip Code] Dear Senator [Senator's Last Name], I hope this letter finds you well.`\n\n**Example span labels:**\n`[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]`\n\n**Example label:**\n`1`\n\n---\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "Train the LuminarSequenceClassifier on the datasets."
   ],
   "id": "51b81189b4cd0958"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T10:05:12.096838Z",
     "start_time": "2025-07-29T10:05:12.054842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LuminarSequenceDataset(Dataset):\n",
    "    def __init__(self, dataset, feature_key=\"features\"):\n",
    "        self.samples = []\n",
    "        for example in dataset:\n",
    "            spans = example[\"sentence_token_spans\"]\n",
    "            if(len(spans)) < 10:\n",
    "                continue\n",
    "            features = torch.tensor(example[feature_key])  # (seq_len, feature_dim)\n",
    "            labels = example[\"span_labels\"]\n",
    "            self.samples.append({\n",
    "                \"features\": features,\n",
    "                \"sentence_spans\": spans,\n",
    "                \"span_labels\": labels\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features = [item[\"features\"] for item in batch]\n",
    "    sentence_spans = [item[\"sentence_spans\"] for item in batch]\n",
    "    span_labels = [item[\"span_labels\"] for item in batch]\n",
    "    features = torch.stack(features)\n",
    "    return {\n",
    "        \"features\": features,\n",
    "        \"sentence_spans\": sentence_spans,\n",
    "        \"span_labels\": span_labels\n",
    "    }"
   ],
   "id": "d50cd8ab23c00195",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T10:09:09.759016Z",
     "start_time": "2025-07-29T10:05:12.147274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = LuminarSequenceDataset(datasets[\"student_essays\"][\"gpt_4o_mini_gemma2_9b\"][\"gpt2_512\"][\"train\"])\n",
    "eval_dataset = LuminarSequenceDataset(datasets[\"student_essays\"][\"gpt_4o_mini_gemma2_9b\"][\"gpt2_512\"][\"eval\"])\n",
    "test_dataset = LuminarSequenceDataset(datasets[\"student_essays\"][\"gpt_4o_mini_gemma2_9b\"][\"gpt2_512\"][\"test\"])\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ],
   "id": "3a72c73e5d28490",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47578\n",
      "{'features': tensor([[4.4133e-05, 4.2039e-45, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.5184e-04],\n",
      "        [9.9816e-06, 2.2685e-09, 2.4707e-10,  ..., 7.9879e-31, 0.0000e+00,\n",
      "         2.9878e-02],\n",
      "        [1.5927e-05, 2.7541e-12, 5.1212e-10,  ..., 7.7882e-28, 2.9427e-44,\n",
      "         2.8827e-01],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]]), 'sentence_spans': [[0, 61], [61, 87], [87, 103], [103, 121], [121, 141], [141, 168], [168, 179], [179, 209], [209, 233], [233, 256], [256, 266], [266, 288], [288, 300], [300, 317], [317, 336]], 'span_labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T10:09:11.724762Z",
     "start_time": "2025-07-29T10:09:09.860608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch in list(enumerate(train_loader))[:1]:\n",
    "    print(batch)\n"
   ],
   "id": "2a99fbe789dcd0bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'features': tensor([[[8.2419e-07, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 5.7127e-05],\n",
      "         [1.0031e-06, 5.1468e-02, 1.3811e-02,  ..., 1.0000e+00,\n",
      "          1.0000e+00, 6.3995e-02],\n",
      "         [3.0254e-04, 3.5541e-04, 1.1336e-02,  ..., 1.1198e-09,\n",
      "          4.9380e-24, 5.7311e-02],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[2.4651e-05, 2.8026e-45, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 1.7961e-04],\n",
      "         [3.8299e-05, 6.5291e-14, 5.8796e-09,  ..., 3.2720e-42,\n",
      "          0.0000e+00, 9.2997e-03],\n",
      "         [8.6544e-06, 1.5348e-09, 1.7457e-06,  ..., 7.9098e-30,\n",
      "          0.0000e+00, 8.0821e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[4.2108e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 3.3240e-05],\n",
      "         [1.5047e-06, 1.0859e-02, 2.9112e-04,  ..., 2.1871e-16,\n",
      "          4.0307e-23, 2.6237e-02],\n",
      "         [4.0606e-05, 1.9634e-20, 1.3257e-17,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 4.7603e-03],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[6.6001e-06, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 5.6236e-06],\n",
      "         [1.0916e-05, 7.0633e-07, 6.1917e-05,  ..., 1.6707e-28,\n",
      "          0.0000e+00, 9.1336e-02],\n",
      "         [7.1785e-07, 8.7160e-13, 3.0740e-14,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 4.3805e-04],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[4.4133e-05, 4.2039e-45, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 1.5184e-04],\n",
      "         [9.9816e-06, 2.2685e-09, 2.4707e-10,  ..., 7.9879e-31,\n",
      "          0.0000e+00, 2.9878e-02],\n",
      "         [1.5927e-05, 2.7541e-12, 5.1212e-10,  ..., 7.7882e-28,\n",
      "          2.9427e-44, 2.8827e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[4.4133e-05, 4.2039e-45, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 1.5184e-04],\n",
      "         [9.9816e-06, 2.2685e-09, 2.4707e-10,  ..., 7.9879e-31,\n",
      "          0.0000e+00, 2.9878e-02],\n",
      "         [1.5927e-05, 2.7541e-12, 5.1212e-10,  ..., 7.7882e-28,\n",
      "          2.9427e-44, 2.8827e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]]), 'sentence_spans': [[[0, 11], [11, 33], [33, 50], [50, 66], [66, 85], [85, 107], [107, 118], [118, 142], [142, 156], [156, 178], [178, 193], [193, 209], [209, 219], [219, 239], [239, 267], [267, 280], [280, 298], [298, 311], [311, 327], [327, 334]], [[0, 28], [28, 42], [42, 49], [49, 65], [65, 96], [96, 112], [112, 149], [149, 175], [175, 201], [201, 223], [223, 244], [244, 281], [281, 350], [350, 388], [388, 402]], [[0, 13], [13, 39], [39, 66], [66, 77], [77, 97], [97, 107], [107, 112], [112, 126], [126, 133], [133, 164], [164, 182], [182, 201], [201, 217], [217, 236], [236, 260], [260, 287], [287, 297], [297, 312], [312, 327], [327, 341], [341, 357], [357, 365], [365, 386], [386, 414], [414, 438], [438, 469], [469, 489], [489, 504]], [[0, 18], [18, 68], [68, 100], [100, 117], [117, 133], [133, 172], [172, 209], [209, 248], [248, 285], [285, 299], [299, 320], [320, 363], [363, 387], [387, 408], [408, 445], [445, 461], [461, 510], [510, 512]], [[0, 22], [22, 54], [54, 80], [80, 93], [93, 98], [98, 116], [116, 133], [133, 154], [154, 177], [177, 202], [202, 231], [231, 260], [260, 284], [284, 298], [298, 310], [310, 327], [327, 344]], [[0, 11], [11, 20], [20, 59], [59, 77], [77, 96], [96, 119], [119, 141], [141, 166], [166, 197], [197, 210], [210, 236], [236, 273], [273, 297], [297, 311], [311, 320], [320, 369]], [[0, 13], [13, 23], [23, 32], [32, 49], [49, 62], [62, 81], [81, 94], [94, 100], [100, 113], [113, 131], [131, 161], [161, 172], [172, 184], [184, 203], [203, 222], [222, 233]], [[0, 15], [15, 40], [40, 53], [53, 87], [87, 101], [101, 128], [128, 153], [153, 202], [202, 235], [235, 251], [251, 263], [263, 278], [278, 293], [293, 320], [320, 362]], [[0, 3], [3, 25], [25, 48], [48, 62], [62, 87], [87, 119], [119, 141], [141, 154], [154, 185], [185, 200], [200, 228], [228, 248], [248, 272], [272, 317], [317, 354], [354, 359]], [[0, 23], [23, 44], [44, 65], [65, 78], [78, 94], [94, 114], [114, 126], [126, 146], [146, 160], [160, 169], [169, 186], [186, 197], [197, 232], [232, 240], [240, 247], [247, 259], [259, 271], [271, 284], [284, 304], [304, 318], [318, 345], [345, 356], [356, 375], [375, 405], [405, 416]], [[0, 21], [21, 41], [41, 108], [108, 141], [141, 189], [189, 251], [251, 310], [310, 359], [359, 380], [380, 424], [424, 468], [468, 512]], [[0, 19], [19, 35], [35, 53], [53, 64], [64, 83], [83, 110], [110, 124], [124, 163], [163, 184], [184, 211], [211, 226], [226, 242], [242, 264], [264, 275], [275, 284], [284, 299], [299, 325], [325, 349], [349, 361], [361, 370], [370, 402], [402, 417], [417, 435]], [[0, 8], [8, 19], [19, 27], [27, 36], [36, 44], [44, 53], [53, 59], [59, 91], [91, 105], [105, 126], [126, 136], [136, 144], [144, 155], [155, 171], [171, 184], [184, 194], [194, 204], [204, 214], [214, 228], [228, 251], [251, 261], [261, 271]], [[0, 24], [24, 46], [46, 71], [71, 94], [94, 114], [114, 128], [128, 153], [153, 172], [172, 185], [185, 214], [214, 251], [251, 264]], [[0, 12], [12, 30], [30, 38], [38, 45], [45, 66], [66, 83], [83, 97], [97, 122], [122, 138], [138, 153], [153, 163], [163, 175], [175, 193], [193, 215], [215, 229], [229, 252], [252, 259], [259, 305], [305, 324], [324, 343], [343, 383], [383, 409], [409, 426], [426, 448], [448, 474], [474, 502], [502, 512]], [[0, 41], [41, 53], [53, 76], [76, 97], [97, 121], [121, 137], [137, 154], [154, 180], [180, 201], [201, 217], [217, 230], [230, 270], [270, 285], [285, 302], [302, 325], [325, 345], [345, 365], [365, 382], [382, 413], [413, 436], [436, 449], [449, 487], [487, 510], [510, 512]], [[0, 17], [17, 52], [52, 81], [81, 114], [114, 135], [135, 151], [151, 155], [155, 174], [174, 207], [207, 212], [212, 219], [219, 266], [266, 303], [303, 392], [392, 441], [441, 465]], [[0, 41], [41, 65], [65, 93], [93, 126], [126, 142], [142, 170], [170, 199], [199, 222], [222, 250], [250, 279], [279, 318], [318, 352], [352, 360], [360, 380], [380, 396], [396, 418], [418, 442]], [[0, 16], [16, 39], [39, 78], [78, 103], [103, 118], [118, 157], [157, 163], [163, 222], [222, 241], [241, 282], [282, 308], [308, 334], [334, 356], [356, 387], [387, 401], [401, 417], [417, 444], [444, 458], [458, 468], [468, 479], [479, 500], [500, 512]], [[0, 33], [33, 77], [77, 80], [80, 112], [112, 138], [138, 160], [160, 177], [177, 209], [209, 226], [226, 241], [241, 259], [259, 275], [275, 314], [314, 345], [345, 359], [359, 380], [380, 409], [409, 431], [431, 443], [443, 471], [471, 487], [487, 510], [510, 512]], [[0, 23], [23, 36], [36, 56], [56, 58], [58, 63], [63, 92], [92, 96], [96, 111], [111, 122], [122, 125], [125, 143], [143, 167], [167, 172], [172, 212], [212, 241], [241, 255], [255, 263], [263, 280], [280, 299], [299, 306], [306, 316], [316, 320], [320, 328], [328, 346], [346, 353], [353, 371], [371, 385], [385, 420], [420, 439], [439, 448]], [[0, 29], [29, 54], [54, 75], [75, 112], [112, 152], [152, 231], [231, 269], [269, 293], [293, 314], [314, 384], [384, 434], [434, 512]], [[0, 36], [36, 59], [59, 89], [89, 106], [106, 134], [134, 156], [156, 186], [186, 225], [225, 238], [238, 261], [261, 290], [290, 321], [321, 334], [334, 368], [368, 401], [401, 437], [437, 459], [459, 486], [486, 512]], [[0, 19], [19, 34], [34, 53], [53, 80], [80, 85], [85, 108], [108, 131], [131, 148], [148, 172], [172, 180], [180, 214], [214, 238], [238, 254], [254, 262]], [[0, 18], [18, 51], [51, 68], [68, 86], [86, 104], [104, 122], [122, 151], [151, 185], [185, 203], [203, 222], [222, 247], [247, 265], [265, 280], [280, 306], [306, 316], [316, 341]], [[0, 24], [24, 35], [35, 59], [59, 64], [64, 87], [87, 99], [99, 125], [125, 159], [159, 191], [191, 210], [210, 237], [237, 259], [259, 287], [287, 313], [313, 328], [328, 344], [344, 369], [369, 388], [388, 400], [400, 413], [413, 424], [424, 448], [448, 474], [474, 492], [492, 504], [504, 512]], [[0, 13], [13, 34], [34, 47], [47, 74], [74, 94], [94, 107], [107, 119], [119, 134], [134, 148], [148, 161], [161, 180], [180, 219], [219, 231], [231, 250], [250, 263], [263, 277], [277, 308], [308, 332], [332, 352], [352, 365], [365, 373], [373, 400], [400, 412], [412, 428], [428, 445], [445, 471], [471, 476], [476, 503]], [[0, 30], [30, 41], [41, 70], [70, 114], [114, 161], [161, 233], [233, 263], [263, 292], [292, 312], [312, 334], [334, 348], [348, 390], [390, 412], [412, 442], [442, 468], [468, 512]], [[0, 19], [19, 29], [29, 54], [54, 63], [63, 84], [84, 107], [107, 140], [140, 159], [159, 180], [180, 202], [202, 208], [208, 219], [219, 246], [246, 285], [285, 313], [313, 323], [323, 355], [355, 384], [384, 395], [395, 410], [410, 422]], [[0, 32], [32, 47], [47, 74], [74, 101], [101, 119], [119, 154], [154, 172], [172, 187], [187, 200], [200, 232], [232, 245], [245, 266], [266, 279], [279, 308], [308, 324], [324, 342], [342, 366], [366, 402], [402, 440], [440, 468]], [[0, 40], [40, 61], [61, 79], [79, 94], [94, 106], [106, 127], [127, 142], [142, 155], [155, 176], [176, 190], [190, 199], [199, 217], [217, 223], [223, 240]], [[0, 62], [62, 83], [83, 110], [110, 133], [133, 147], [147, 164], [164, 189], [189, 208], [208, 214], [214, 222], [222, 237], [237, 249], [249, 269], [269, 293], [293, 300], [300, 313], [313, 333], [333, 360], [360, 387], [387, 406], [406, 425], [425, 431]]], 'span_labels': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]})\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T10:09:11.825524Z",
     "start_time": "2025-07-29T10:09:11.777706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#from luminar.sequence_classifier import LuminarSequence\n",
    "from src.luminar.sequence_classifier import LuminarSequence\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    \"feature_dim\": (Config.FEATURE_LEN, Config.NUM_INTERMEDIATE_LIKELIHOODS),\n",
    "    \"feature_type\": \"intermediate_likelihoods\",\n",
    "    \"feature_selection\": \"first\",\n",
    "    \"conv_layer_shapes\": (\n",
    "        ConvolutionalLayerSpec(32, 5),\n",
    "        ConvolutionalLayerSpec(64, 5),\n",
    "        ConvolutionalLayerSpec(32, 3),\n",
    "    ),\n",
    "    \"lstm_hidden_dim\": 128,\n",
    "    \"lstm_layers\": 1,\n",
    "    \"projection_dim\": 32,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"max_epochs\": 40,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"eval_batch_size\": 1024,\n",
    "    \"warmup_ratio\": 1.0,\n",
    "    \"seed\": Config.SEED,\n",
    "    \"rescale_features\": False\n",
    "}"
   ],
   "id": "49b953e16e810114",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T10:09:11.928205Z",
     "start_time": "2025-07-29T10:09:11.878295Z"
    }
   },
   "cell_type": "code",
   "source": "luminar = LuminarSequence(**config).to(device)",
   "id": "5f02483245f4a042",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LuminarTrainingConfig(feature_dim=(512, 13), feature_type='intermediate_likelihoods', feature_selection='first', conv_layer_shapes=((32, 5, 1), (64, 5, 1), (32, 3, 1)), lstm_hidden_dim=128, lstm_layers=1, projection_dim=32, early_stopping_patience=3, learning_rate=0.0005, max_epochs=40, gradient_clip_val=1.0, train_batch_size=32, eval_batch_size=1024, warmup_ratio=1.0, seed=42, rescale_features=False)\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T10:09:12.042969Z",
     "start_time": "2025-07-29T10:09:11.994438Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = torch.optim.Adam(luminar.parameters(), lr=config[\"learning_rate\"])",
   "id": "63ad01374bac7bef",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T10:09:12.175081Z",
     "start_time": "2025-07-29T10:09:12.121697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            features = batch[\"features\"].to(device)\n",
    "            sentence_spans = batch[\"sentence_spans\"]\n",
    "            span_labels = batch[\"span_labels\"]\n",
    "\n",
    "            output = model(features, sentence_spans)\n",
    "            probs = torch.sigmoid(output.logits).view(-1).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            labels = torch.cat([torch.tensor(lbl, dtype=torch.int) for lbl in span_labels]).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, f1\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            features = batch[\"features\"].to(device)\n",
    "            sentence_spans = batch[\"sentence_spans\"]\n",
    "            span_labels = batch[\"span_labels\"]\n",
    "\n",
    "            output = model(features, sentence_spans, span_labels=span_labels)\n",
    "            loss = output.loss\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def train_and_evaluate(model, train_loader, eval_loader, optimizer, device, epochs, patience=3):\n",
    "    best_eval_loss = float(\"inf\")\n",
    "    best_train_loss = None\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features = batch[\"features\"].to(device)\n",
    "            sentence_spans = batch[\"sentence_spans\"]\n",
    "            span_labels = batch[\"span_labels\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features, sentence_spans, span_labels=span_labels)\n",
    "            loss = output.loss\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Evaluation\n",
    "        avg_eval_loss = evaluate(model, eval_loader, device)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Eval Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "        # Every 5 epochs: evluate on test set\n",
    "        if test_loader is not None and (epoch + 1) % 5 == 0:\n",
    "            acc, f1 = evaluate_metrics(model, test_loader, device)\n",
    "            print(f\"Test Accuracy: {acc:.4f} | Test F1: {f1:.4f}\")\n",
    "\n",
    "        # Early Stopping & Checkpoint\n",
    "        if avg_eval_loss < best_eval_loss:\n",
    "            best_eval_loss = avg_eval_loss\n",
    "            best_train_loss = avg_train_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs.\")\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    print(f\"\\nBest Eval Loss: {best_eval_loss:.4f} | Best Train Loss: {best_train_loss:.4f}\")"
   ],
   "id": "91e1d2c24f9a1755",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T10:22:54.975933Z",
     "start_time": "2025-07-29T10:09:12.218229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_and_evaluate(luminar,\n",
    "                   train_loader,\n",
    "                   eval_loader,\n",
    "                   optimizer,\n",
    "                   device,\n",
    "                   epochs=config[\"max_epochs\"],\n",
    "                   patience=config[\"early_stopping_patience\"])"
   ],
   "id": "105a3383df0d35de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/40\n",
      "Train Loss: 0.5412 | Eval Loss: 0.5080\n",
      "\n",
      "Epoch 2/40\n",
      "Train Loss: 0.4657 | Eval Loss: 0.4499\n",
      "\n",
      "Epoch 3/40\n",
      "Train Loss: 0.4460 | Eval Loss: 0.4335\n",
      "\n",
      "Epoch 4/40\n",
      "Train Loss: 0.4332 | Eval Loss: 0.4296\n",
      "\n",
      "Epoch 5/40\n",
      "Train Loss: 0.4195 | Eval Loss: 0.4254\n",
      "Test Accuracy: 0.7941 | Test F1: 0.8017\n",
      "\n",
      "Epoch 6/40\n",
      "Train Loss: 0.4083 | Eval Loss: 0.4012\n",
      "\n",
      "Epoch 7/40\n",
      "Train Loss: 0.4013 | Eval Loss: 0.4153\n",
      "\n",
      "Epoch 8/40\n",
      "Train Loss: 0.3922 | Eval Loss: 0.4464\n",
      "\n",
      "Epoch 9/40\n",
      "Train Loss: 0.3870 | Eval Loss: 0.3980\n",
      "\n",
      "Epoch 10/40\n",
      "Train Loss: 0.3794 | Eval Loss: 0.4099\n",
      "Test Accuracy: 0.8159 | Test F1: 0.8451\n",
      "\n",
      "Epoch 11/40\n",
      "Train Loss: 0.3746 | Eval Loss: 0.3814\n",
      "\n",
      "Epoch 12/40\n",
      "Train Loss: 0.3675 | Eval Loss: 0.3757\n",
      "\n",
      "Epoch 13/40\n",
      "Train Loss: 0.3625 | Eval Loss: 0.3759\n",
      "\n",
      "Epoch 14/40\n",
      "Train Loss: 0.3585 | Eval Loss: 0.3608\n",
      "\n",
      "Epoch 15/40\n",
      "Train Loss: 0.3540 | Eval Loss: 0.3733\n",
      "Test Accuracy: 0.8249 | Test F1: 0.8304\n",
      "\n",
      "Epoch 16/40\n",
      "Train Loss: 0.3491 | Eval Loss: 0.3546\n",
      "\n",
      "Epoch 17/40\n",
      "Train Loss: 0.3448 | Eval Loss: 0.3642\n",
      "\n",
      "Epoch 18/40\n",
      "Train Loss: 0.3408 | Eval Loss: 0.3555\n",
      "\n",
      "Epoch 19/40\n",
      "Train Loss: 0.3373 | Eval Loss: 0.3472\n",
      "\n",
      "Epoch 20/40\n",
      "Train Loss: 0.3334 | Eval Loss: 0.3511\n",
      "Test Accuracy: 0.8433 | Test F1: 0.8626\n",
      "\n",
      "Epoch 21/40\n",
      "Train Loss: 0.3297 | Eval Loss: 0.3511\n",
      "\n",
      "Epoch 22/40\n",
      "Train Loss: 0.3265 | Eval Loss: 0.3439\n",
      "\n",
      "Epoch 23/40\n",
      "Train Loss: 0.3230 | Eval Loss: 0.3391\n",
      "\n",
      "Epoch 24/40\n",
      "Train Loss: 0.3200 | Eval Loss: 0.3468\n",
      "\n",
      "Epoch 25/40\n",
      "Train Loss: 0.3147 | Eval Loss: 0.3379\n",
      "Test Accuracy: 0.8506 | Test F1: 0.8679\n",
      "\n",
      "Epoch 26/40\n",
      "Train Loss: 0.3124 | Eval Loss: 0.3739\n",
      "\n",
      "Epoch 27/40\n",
      "Train Loss: 0.3085 | Eval Loss: 0.3324\n",
      "\n",
      "Epoch 28/40\n",
      "Train Loss: 0.3040 | Eval Loss: 0.3279\n",
      "\n",
      "Epoch 29/40\n",
      "Train Loss: 0.3012 | Eval Loss: 0.3407\n",
      "\n",
      "Epoch 30/40\n",
      "Train Loss: 0.2978 | Eval Loss: 0.3261\n",
      "Test Accuracy: 0.8561 | Test F1: 0.8679\n",
      "\n",
      "Epoch 31/40\n",
      "Train Loss: 0.2940 | Eval Loss: 0.3204\n",
      "\n",
      "Epoch 32/40\n",
      "Train Loss: 0.2898 | Eval Loss: 0.3158\n",
      "\n",
      "Epoch 33/40\n",
      "Train Loss: 0.2854 | Eval Loss: 0.3175\n",
      "\n",
      "Epoch 34/40\n",
      "Train Loss: 0.2817 | Eval Loss: 0.3081\n",
      "\n",
      "Epoch 35/40\n",
      "Train Loss: 0.2765 | Eval Loss: 0.3271\n",
      "Test Accuracy: 0.8601 | Test F1: 0.8786\n",
      "\n",
      "Epoch 36/40\n",
      "Train Loss: 0.2725 | Eval Loss: 0.3280\n",
      "\n",
      "Epoch 37/40\n",
      "Train Loss: 0.2681 | Eval Loss: 0.3282\n",
      "Early stopping after 37 epochs.\n",
      "\n",
      "Best Eval Loss: 0.3081 | Best Train Loss: 0.2817\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T10:22:55.104219Z",
     "start_time": "2025-07-29T10:22:55.102394Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "93d3186525396281",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
